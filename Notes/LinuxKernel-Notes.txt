

My Notes about the Linux Kernel Gathered from books, Linux code, 
my questions in LKML, Googling and several crazy months!.

2007-2008, Ahmed S. Darwish <darwish.07 at gmail.com>

Edit: Changed to represent the new changes in kernel version 2.6.25
      including the new x86 stuff (std regparm(3), x64 unification, ...)

      New!! Some glibc clone() goodies.

Topics Covered till now:
1- Segmentation and paging internals (+ boot initialization)
2- Processes creation, switch, deletion and LWP internals
3- Interrupt and Exception handling internals (+ boot initialization)
4- Softirqs, Tasklets and Workqueues internals
5- Kernel preemption and Synchronization primitives
6- A rough skim over MM, including the buddy allocator and slab.

Introducion:
------------

* Unix associates a current working directory with each process.

* A process executing a file normally keeps the UID of the process owner. If
  the suid flag is set, it gets the UID of the file owner. Same happens with
  the sgid flag.

* Kernel threads are execution contexts within the kernel. They have the 
  following charactarestics:
  1- They run in kernel mode, in kernel address space
  2- They do NOT interact with users, thus don't need terminal devices
  3- Not like user processes, they usually start at system startup and remain
     alive till system shut down.

* Kernel threads are execution contexts that can be independetly scheduled. 
  Context switchs between them are much less expensive than normal processes
  cause the former usually operate on a common address space.

* Files don't include any control information such as its length or an EOF
  delimiter. All information needed by the filesystem to handle a file is
  included in its inode structure. Each file has its own inode which the
  filesystem uses to identify the file.

* Dirty page/buffers are pages/buffers whose contents differ from that of the
  corresponding disk blocks.

* Three of the six Segmentation registers have specific purposes:
  cs: Code Segment Register
  ss: Stack Segment Register
  ds: Data Segment Register
  The cs register includes a 2bit value representing the current privilege level
  (CPL). Linux uses two values only of them, 0 for kernel mode and 3 for user
  mode.

* There're several types of segments, thus several types of descriptros too.
  + Code Segment Descriptor: refers to a code segment, in GDT or LDT, S flag set.
  + Data Segment Descriptor: refers to a data segment, in GDT or LDT, S flag set.
  + Task State Segment Descriptor (TSSD): 
      represents a Task State segment, in GDT only, S flag unset.
      
      ``Task state segments are used to save the contents of the processor
      	registers (like in context switching).''

  + Local Descriptor Table Descriptor(LDTD): 
      indicates that the segment descriptor refers to a segment containing an
      LDT. ofcourse it can appears only in a GDT.

* To address the kernel code segment, the kernel loads the value __KERNEL_CS into
  the cs segmentation register. 

  Segment	      Base		G	Limit	S	Type	DPL	P
  user code	      0x00000000	1	0xfffff	1	10	3	1
  user data	      0x00000000	1	0xfffff	1	2	3	1
  kernel code	      0x00000000	1	0xfffff	1	10	0	1
  kernel data	      0x00000000	1	0xfffff	1	2	0	1

  --> include/asm-x86/segment.h:
  /*
   * Layout of per-cpu GDT:
   *  ------- start of kernel segments:
   *
   *  12 - kernel code segment		<==== new cacheline
   *  13 - kernel data segment
   */
  #define GDT_ENTRY_KERNEL_BASE	12

  #define GDT_ENTRY_KERNEL_CS		(GDT_ENTRY_KERNEL_BASE + 0)
  #define __KERNEL_CS (GDT_ENTRY_KERNEL_CS * 8)

* Since the base of all segments is 0x00000000, the value of the offset field of
  a logical address = the value of the linear address. So several processes in
  kernel or user mode can have the same _logical_ address.

* When CPL (Current Privelege level) is equal to 3 (user mode), the ds register
  must contain the segment selector of the user data segment. When it's equal to
  zero, the ds register must contain the segment selector of the kernel data
  segment. A similar situation occurs for the ss register responsible for the
  stack segment.

* For fast access to segment descriptors(without refering to GDT or LDT), From
  80386 reference:
  ``Program loads the visible part of the segment register with a 16bit
    selector. The processor _automatically_ fetches the base address, limit, type
    and others from a descriptor table and loads them to the invisible part.''
  The _invisible part_ is the nonprogrammable register mentioned in the
  book. Cause of it's nonprogrammable, it load automatically by the processor.

           16-BIT VISIBLE
                SELECTOR                 HIDDEN DESCRIPTOR
           +---------------------------------------------------------+
        CS |                |        Nonprogrammable Register        |
           |----------------+----------------------------------------|
        SS |                |        Nonprogrammable Register        |
           |----------------+----------------------------------------|
        DS |                |        Nonprogrammable Register        |
           |----------------+----------------------------------------|
        ES |                |        Nonprogrammable Register        |
           |----------------+----------------------------------------|
        FS |                |        Nonprogrammable Register        |
           |----------------+----------------------------------------|
        GS |                |        Nonprogrammable Register        |
           +---------------------------------------------------------+


	   DESCRIPTOR TABLE			SEGMENT
	   +--------------+                     +------+
	   |              |           +-------->|      |<-----+
	   |   Segment    |           |         |      |      |
	   |  Descriptor  | ..........|         |      |      |
           |              |                     +------+      |
	   |     ...      |                                   |
           +--------------+                                   |
	   						      |
	              (Nonprogrammable Register) -------------+

* Segmentation and paging are somewhat redundant; both can separate the physical
  address spaces of the processes. Segmentation can assign a different linear
  address space for each processor, while paging can map the same linear address
  space into different physical address spaces.
  Linux 2.6 uses segmentation only when required by the 80x86 architecture.
  

The Linux GDT:
--------------

From the 80386 reference:
``A descriptor table is simply a memory array of _8bytes_ entries. They're variable
  in length and may contain up to 2^13 descriptor.''

  --> include/asm-x86/desc_defs.h:
  /*
   * FIXME: Acessing the desc_struct through its fields is more elegant,
   * and should be the one valid thing to do. However, a lot of open code
   * still touches the a and b acessors, and doing this allow us to do it
   * incrementally. We keep the signature as a struct, rather than an union,
   * so we can get rid of it transparently in the future -- glommer
   */

   31                23                15                7               0
  +-----------------+-----------------+-----------------+-----------------+
  |                 | | | |A|         | |     | |       |                 |
  |   BASE 31..24   |G|X|O|V| LIMIT   |P| DPL |0|  TYPE |  BASE 23..16    | 4
  |                 | | | |L| 19..16  | |     | |       |                 |
  |-----------------------------------+-----------------------------------|
  |                                   |                                   |
  |        SEGMENT BASE 15..0         |       SEGMENT LIMIT 15..0         | 0
  |                                   |                                   |
  +-----------------+-----------------+-----------------+-----------------+

  // 8 byte segment descriptor

  struct desc_struct {
	union {
		struct { unsigned int a, b; };
		struct {
			u16 limit0;
			u16 base0;
			unsigned base1: 8, type: 4, s: 1, dpl: 2, p: 1;
			unsigned limit: 4, avl: 1, l: 1, d: 1, g: 1, base2: 8;
		};

	};
  } __attribute__((packed));


All GDTs are stored in the cpu_gdt_table array. Each GDT has 18 segment
descriptors and 14null, unused or reserved entries.  

  --> include/asm-x86/segment.h:
  /* A full desccription of the linux GDT table */

  /*
   * The GDT has 32 entries
   */
  #define GDT_ENTRIES 32
  #define GDT_SIZE (GDT_ENTRIES * 8)

  #define GDT_ENTRY_DEFAULT_USER_CS	14
  #define __USER_CS (GDT_ENTRY_DEFAULT_USER_CS * 8 + 3)

  #define GDT_ENTRY_DEFAULT_USER_DS	15
  #define __USER_DS (GDT_ENTRY_DEFAULT_USER_DS * 8 + 3)

  #define GDT_ENTRY_KERNEL_BASE	12

  #define GDT_ENTRY_KERNEL_CS		(GDT_ENTRY_KERNEL_BASE + 0)
  #define __KERNEL_CS (GDT_ENTRY_KERNEL_CS * 8)

  #define GDT_ENTRY_KERNEL_DS		(GDT_ENTRY_KERNEL_BASE + 1)
  #define __KERNEL_DS (GDT_ENTRY_KERNEL_DS * 8)

  --> include/asm-x86/desc.h:
  struct gdt_page
  {
	struct desc_struct gdt[GDT_ENTRIES];
  } __attribute__((aligned(PAGE_SIZE)));
  DECLARE_PER_CPU(struct gdt_page, gdt_page);

The 18 segment descriptors are: 

+ Four user, kernel code and data segments

  Segment	 	 Base		G	Limit	S	Type	DPL	P
  -------------------------------------------------------------------------------
  user code	      0x00000000	1	0xfffff	1	10	3	1
  user data	      0x00000000	1	0xfffff	1	2	3	1
  kernel code	      0x00000000	1	0xfffff	1	10	0	1
  kernel data	      0x00000000	1	0xfffff	1	2	0	1


    31                23                15                7               0
   +-----------------+-----------------+-----------------+-----------------+
   |                 | | | |A|         | |     | |       |                 |
   |   BASE 31..24   |G|X|O|V| LIMIT   |P| DPL |0|  TYPE |  BASE 23..16    | 4
   |                 | | | |L| 19..16  | |     | |       |                 |
   |-----------------------------------+-----------------------------------|
   |                                   |                                   |
   |        SEGMENT BASE 15..0         |       SEGMENT LIMIT 15..0         | 0
   |                                   |                                   |
   +-----------------+-----------------+-----------------+-----------------+

  --> arch/x86/kernel/cpu/common.c:
  DEFINE_PER_CPU(struct gdt_page, gdt_page) = { .gdt = {
	[GDT_ENTRY_KERNEL_CS] = { 0x0000ffff, 0x00cf9a00 },
	[GDT_ENTRY_KERNEL_DS] = { 0x0000ffff, 0x00cf9200 },
	[GDT_ENTRY_DEFAULT_USER_CS] = { 0x0000ffff, 0x00cffa00 },
	[GDT_ENTRY_DEFAULT_USER_DS] = { 0x0000ffff, 0x00cff200 },

	...

	[GDT_ENTRY_ESPFIX_SS] = { 0x00000000, 0x00c09200 },
	[GDT_ENTRY_PERCPU] = { 0x00000000, 0x00000000 },
  } };
  EXPORT_PER_CPU_SYMBOL_GPL(gdt_page);

+ A segment including the default Local Descriptor Table (LDT), usually shared
  by all processes.

	In old 2.6.20 code:
	.quad 0x0000000000000000	/* 0x88 LDT descriptor */

	In 2.6.22 ++:
	zeroed by the default behaviour of the C99 initialization

+ Three Thread-Local-Storage (TLS) segments. Allowing multithreaded apps to
  contain data local to each thread.

  	In old 2.6.20 code:
	.quad 0x0000000000000000	/* 0x33 TLS entry 1 */
	.quad 0x0000000000000000	/* 0x3b TLS entry 2 */
	.quad 0x0000000000000000	/* 0x43 TLS entry 3 */

	In 2.6.22 ++:
	zeroed by the default behaviour of the C99 initialization

+ Three segments related to Advanced Power Management (APM). BIOS code make use
  of this segments. When Linux APM driver invokes BIOS functions to set or get
  APM devices status, it may use custom code and data segments.

	/*
	 * The APM segments have byte granularity and their bases
	 * are set at run time.  All have 64k limits.
	 */
	[GDT_ENTRY_APMBIOS_BASE] = { 0x0000ffff, 0x00409a00 },/* 32-bit code */
	/* 16-bit code */
	[GDT_ENTRY_APMBIOS_BASE+1] = { 0x0000ffff, 0x00009a00 },
	[GDT_ENTRY_APMBIOS_BASE+2] = { 0x0000ffff, 0x00409200 }, /* data */

+ Five segments related PnP BIOS services. BIOS code makes use of this segments
  similar to previous point situation.

	/*
	 * Segments used for calling PnP BIOS have byte granularity.
	 * They code segments and data segments have fixed 64k limits,
	 * the transfer segment sizes are set at run time.
	 */
	[GDT_ENTRY_PNPBIOS_CS32] = { 0x0000ffff, 0x00409a00 },/* 32-bit code */
	[GDT_ENTRY_PNPBIOS_CS16] = { 0x0000ffff, 0x00009a00 },/* 16-bit code */
	[GDT_ENTRY_PNPBIOS_DS] = { 0x0000ffff, 0x00009200 }, /* 16-bit data */
	[GDT_ENTRY_PNPBIOS_TS1] = { 0x00000000, 0x00009200 },/* 16-bit data */
	[GDT_ENTRY_PNPBIOS_TS2] = { 0x00000000, 0x00009200 },/* 16-bit data */

+ A Task State Segment (TSS) different for each processor in the system.

    	In old 2.6.20 code:
	.quad 0x0000000000000000	/* 0x80 TSS descriptor */

	In 2.6.22:
	zeroed by the default behaviour of C99 initialization

  NOTE: Reread this paragraph about TSS after finishing Chapter 7 on 80386
  	reference.

+ A special TSS segment to handle Double fault exceptions
  
	.quad 0x0000000000000000	/* 0xf8 -GDT entry 31: double-fault TSS*/


* Call gates are a mechanism provided by 80 x 86 microprocessors to change the
  privilege level of the CPU while invoking a predefined function. Linux make
  no use of the LDT. Wine apps use it since they executed segment-oriented MS
  windows applications.


********************************************************************************
Paging:
*******

In Hardware:
------------

* Page tables are stored in main memory and must be properly initialized by the
  kernel before enabling the paging unit.

* Each active process must have a Page Directory assigned to it (using the CR3 in
  the TSS segment). However, there's no need to allocate all the pages at once,
  it's only provided when effictively needed.

* For extended paging, PSE flag in cr4 must be set. Extended paging makes the page
  size 4MB so it's efficient in terms of TLB usage.

* Format of a page table entry:

				<-- 4bytes -->
    31                                  12 11    9 8 7 6 5 4 3 2 1 0
      +----------------------------------------------------------------+
      |                                      |       |   | | |   |U|R| |
      |      PAGE FRAME ADDRESS 31..12       | AVAIL |0 0|D|A|0 0|/|/|P|
      |                                      |       |   | | |   |S|W| |
      +----------------------------------------------------------------+

  Architecture         Page size   Address bits   paging levels	Linear(Address Splitting)
  x86    4KB (12bits)      20         2		  10 + 10 + 12
  x86-64 4KB (12bits)      48	      4		  9 + 9 + 9 + 9 + 12
  
* When a process attempts to access a linear address outside of its interval,
  Page unit issues a page fault exception. This happens cause page table entries
  not assigned to the process are filled with zero, leading to a zero `Present'
  bit.

* A new unit called the cache line was introduced into the 80 x 86 architecture.
  It consists of a few dozen contiguous bytes that are transferred in _burst mode_
  between the slow DRAM and the fast on-chip static RAM (SRAM) used to implement
  caches.

* The cache unit is inserted between the paging unit and the main memory. Cach
  unit includes a hardware cache memory and a cache controller. 
  Cache controller stores an array of entries, one entry for each line of cache
  memory.
  Entry Format:
  tag - subset index - offset within the line.

* When a cache hit occurs and a write operation desired. Two basic strategies:
  a - Write through: Controller always writes into both RAM and cache line. 
  b - Write Back: more immediate efficiency, only the cache is updated. RAM is
      	    	  updated after the instruction ends.

* Linux always clear the PCD (Page cache disable) and PWT (Page Write Through)
  flags in page specifies. Leading to always-on cache policy and write-back policy
  for all pages. (remember that linux also zero all pages not used by a user-process,
  which leads to clearing present bit, PCD, PWT and others).
  
* CPU flags to control cache controllers:
  In Control register cr0:
  CD Flag: enable or disable the cache circuity
  NW: specifies wether write-through or write-back strategy is used.

  CR0 Register:

      31 30  29                      18  17  16         5  4  3  2 1 0
     +----------------------------------------------------------------+
     |P   C   N                       A      W          N  E  T  E M P|
     |G   D   W                       M      P          E  T  S  M P E|
     +----------------------------------------------------------------+
     
     PG: Control Paging
     CD: Cache Disabling
     NW: Not write-through

* Hardware automatically invalidates (implicitly flushing) TLB entries when the
  contents of the cr3 register changes. Since all the TLB stored entries become
  invalid.

* We use multi-level paging for several reasons:

  - If we're going to use a single level page table, this means we'll need 2^20
    entries to cover the 4GB/2^32 virtual space. At 4bytes/entry, we'll need
    4MB for each process page table. By the nature of a single table, we'll
    have to maintain it even if the process is not using the entire space

  - Also because it's a single table, it has to be put on contiguous space. Using
    contiguous 4MB for each process Page Table is not simply acceptable.

In Linux:
---------

* Starting with version 2.6.11, a four-level paging model has been adopted. For
  types are:
  Page Global Directory
  Page Upper Directory
  Page Middle Directory
  Page Table

* Common macros to deal with pages of memory:
  Assuming a 80x86 processor with no PAE extension

+ First Level: (PAGE)
  PAGE_SHIFT = (in x86: 12):
    used to get the page address by right shifting the page size from a linear
    address. so PAGE_SHIFT = log (base 2) area mapped by a page (page size).
  ==> PAGE_SIZE = (1UL << PAGE_SHIFT) = (in x86: 0x00001000)

  PAGE MASK:
    using the value 0xfffff000 to mask all the bits of the offset field
    PAGE_MASK = ~(PAGE_SIZE - 1)

+ Second Level: (PMD)
  PMD_SHIFT: 
    Page Middle Directory shift. As in previous macros, used to get the address
    of PMD. PMD_SHIFT = log (base 2) area mapped by a PMD entry.
                      = 12(offset) + 10(PGTable) = 22
  ==> PMD_SIZE = (1UL << PMD_SHIFT) = 4MB (note: large page size)

  PMD_MASK:
    Using the value 0xffc00000 to mask all the bits of the offset and Table
    fields.
    PMD_MASK = ~(PMD_SIZE - 1)
    
+ Third Level and Fourth Level (PUD, PGDIR):
  All of _SHIFT and _SIZE macros are equal to their PMD values

  
* In x86 processors:
  no PAE:
  PAGE_SHIFT = 12, PAGE_SIZE = 4KB, PAGE_MASK = 0xfffff000
  PMD_SHIFT = 22, PMD_SIZE = 4MB, PMD_MASK = 0xffc00000
  PAGE_LARGE_SHIFT = PMD_SHIFT, PAGE_LARGE_SIZE = PMD_SIZE <-- Extended paging
  PUD_SHIFT = PMD_SHIFT, PUD_SIZE = PMD_SIZE
  PGDIR_SHIFT = PMD_SHIFT, PGDIR_SIZE = PMD_SIZE

  PAE:
  PAGE_SHIFT = 12, PAGE_SIZE = 4KB, PAGE_MASK = 0xfffff000
  PMD_SHIFT = 12 + 9 = 21, PMD_SIZE = 2MB, PMD_MASK = 0xffc00000
  PUD_SHIFT = PMD_SHIFT, PUD_SIZE = PMD_SIZE
  PGDIR_SHIFT = 12 + 9 + 9 = 30, PGIDR_SIZE = 1GB, PGDIR_MASK = 0xc000000

* In x86-64 processors:
  PAGE_SHIFT = 12, PAGE_SIZE = 4KB, PAGE_MASK = 0xfffffffffffff000
  PMD_SHIFT = 12 + 9, PMD_SIZE = 2^21, PMD_MASK = 0xfffffffffffe00000
  PUD_SHIFT = 21 + 9, PUD_SIZE = 2^30, PUD_MASK = 0xfffffffffc0000000
  PGDIR_SHIFT = 30 + 9, PGDIR_SIZE = 2^39, PGDIR_MASK = 0xffffffc000000000

Page Table handling:
--------------------

* pte_t, pmd_t, pud_t, pgd_t respectively describes the format of a Page Table, a
  Page Middle directory, a Page Upper Directory and a Page Global directory entry.

  --> include/asm-x86/page_32.h:
  #else  /* !CONFIG_X86_PAE (No middle or upper directories used)*/
  typedef unsigned long	pteval_t;
  typedef unsigned long	pmdval_t;
  typedef unsigned long	pudval_t;
  typedef unsigned long	pgdval_t;
  typedef union { pteval_t pte, pte_low; } pte_t;
  typedef struct { unsigned long pgd; } pgd_t;
  typedef struct { unsigned long pgprot; } pgprot_t;
  #define boot_pte_t pte_t /* or would you rather have a typedef */

  --> include/asm-x86/pgtable_32.h:
  #define pte_present(x)	((x).pte_low & (_PAGE_PRESENT | _PAGE_PROTNONE))	

  --> include/asm-x86/pgtable.h
  #define _PAGE_BIT_PRESENT	0
  #define _PAGE_BIT_RW		1
  #define _PAGE_BIT_USER	2
  #define _PAGE_BIT_PWT		3
  ...

  /*
   * Note: we use _AC(1, L) instead of _AC(1, UL) so that we get a
   * sign-extended value on 32-bit with all 1's in the upper word,
   * which preserves the upper pte values on 64-bit ptes:
   */
  #define _PAGE_PRESENT	(_AC(1, L)<<_PAGE_BIT_PRESENT)
  #define _PAGE_RW	(_AC(1, L)<<_PAGE_BIT_RW)
  #define _PAGE_USER	(_AC(1, L)<<_PAGE_BIT_USER)
  #define _PAGE_PWT	(_AC(1, L)<<_PAGE_BIT_PWT)

  /*
   * The following only work if pte_present() is true.
   * Undefined behaviour if not..
   */
  static inline int pte_dirty(pte_t pte)	{ return pte_val(pte) & _PAGE_DIRTY; }
  static inline int pte_young(pte_t pte)	{ return pte_val(pte) & _PAGE_ACCESSED; }
  static inline int pte_write(pte_t pte)	{ return pte_val(pte) & _PAGE_RW; }
  static inline int pte_file(pte_t pte)		{ return pte_val(pte) & _PAGE_FILE; }
  static inline int pte_huge(pte_t pte)		{ return pte_val(pte) & _PAGE_PSE; }
  static inline int pte_global(pte_t pte) 	{ return pte_val(pte) & _PAGE_GLOBAL; }

  static inline pte_t pte_mkclean(pte_t pte)	{ return __pte(pte_val(pte) & ~(pteval_t)_PAGE_DIRTY); }
  static inline pte_t pte_mkold(pte_t pte)	{ return __pte(pte_val(pte) & ~(pteval_t)_PAGE_ACCESSED); }
  static inline pte_t pte_wrprotect(pte_t pte)	{ return __pte(pte_val(pte) & ~(pteval_t)_PAGE_RW); }
  static inline pte_t pte_mkexec(pte_t pte)	{ return __pte(pte_val(pte) & ~(pteval_t)_PAGE_NX); }
  static inline pte_t pte_mkdirty(pte_t pte)	{ return __pte(pte_val(pte) | _PAGE_DIRTY); }
  static inline pte_t pte_mkyoung(pte_t pte)	{ return __pte(pte_val(pte) | _PAGE_ACCESSED); }


Physical Memory Layout:
-----------------------

* During the initializatin phase the kernel must build a physical address map
  that specifies which physical address ranges are usable by the kernel and which
  are unavailable. 

* Linux kernel in installed in RAM starting from the physical address
  0x00100000 (from the second MB) since the PC architecture has several uses of 
  the first MB in RAM.

* setup_memory() method is invoked after the machine_specific_memory_setup(). It
  analyzes the table of physical memory and initializes a few important variables
  to describe the kernel's physical memory layout.
  
  num_physpages   - pfn of the highest usable page frame
  totalram_pages  - Total number of _usable_ page frames
  
  min_low_pfn     - first usable pfn after the kernel image in RAM
  max_pfn         - pfn of the last usable page frame
  max_low_pfn     - pfn of the last frame directly mapped by the kernel (low memory)
  
  totalhigh_pages - Total num of frames not directly kernel mapped (high memory)
  highstart_pfn   - First high memory pfn
  highend_pfn     - last high memory pfn

* Linear address space of a process is divided to two parts:
  - from 0x0 to 0xbfffffff (2^28 * 12 = 2^30 * 3 = 3GB) can be addressed in user 
    or kernel mode
  - from 0xc0000000 to 0xffffffff (1GB) can be addressed only in kernel mode

* Entires in the page table from 0xc0000000 should be the same for all processes
  and equal to the corresponding entries of the master kernel Global Page
  Directory.

  So the first 786 entries ((2^30 * 3) / 2^22 = 2^8 * 3 = 786) depend on the specific 
  process, while the remaining 256 entries (2^30 / 2^22) are the kernel ones.

* Kernel maintains a set of page tables for its own use, rooted at the master
  kernel Page Global directory. The highest entries of the master kernel Page
  Global Directory are the reference model for the corresponding entries of the
  Page Global Directories of every regular process in the system.
  
Initialization phases:
----------------------

* Right after the kernel image is loaded, the CPU is still running in real mode.
  first phase: Kernel creates a limited address space including the kernel's code
  	       and data segment, the initial Page Tables, and a 128KB for some
  	       dynamic data structures.
  Second phase: kernel takes advantages of all the existing RAM and sets up the
  	 	page tables properly. 

* Imp: The provisional _global page directory_ is contained in swapper_pg_dir
  variable. while the provisional _page tables_ are stored starting from pg0, 
  right after _end.

* Objective of the first phase is to allow 8MB of RAM (as example) to be easily
  addressed in both real and protected mode. So
  the _physical_ address from 0x00000000 to 0x007fffff are mapped in both linear
  ranges:
  0x00000000 -> 0x007fffff (Identity mapping) and to 0xc0000000 -> 0xc07fffff

  Why this is done ?
  As said by Joseph:

  `` 
  + All pointers in the compiled kernel refer to addresses >
  PAGE_OFFSET. That is, the kernel is linked under the assumption that its base
  address will be start_text (I think; I don't have the code on hand at the
  moment), which is defined to be PAGE_OFFSET+(some small constant, call it C).

  + All the kernel bootstrap code is linked assuming that its base address is 0+C.

  head.S is part of the bootstrap code. It's running in protected mode with
  paging turned off, so all addresses are physical. In particular, the
  instruction pointer is fetching instructions based on physical address. The
  instruction that turns on paging (movl %eax, %cr0) is located, say, at some
  physical address A. 

  As soon as we set the paging bit in cr0, paging is enabled, and starting at
  the very next instruction, all addressing, including instruction fetches, pass
  through the address translation mechanism (page tables). IOW, all address are
  henceforth virtual. That means that

   1. We must have valid page tables, and
   2. Those tables must properly map the instruction pointer to the next
      instruction to be executed.

   That next instruction is physically located at address A+4 (the address
   immediately after the "movl %eax, %cr0" instruction), but from the point of view
   of all the kernel code - which has been linked at PAGE_OFFSET - that instruction
   is located at virtual address PAGE_OFFSET+(A+4). Turning on paging, however, does
   not magically change the value of EIP. The CPU fetches the next instruction from
   ***virtual*** address A+4; that instruction is the beginning of a short sequence
   that effectively relocates the instruction pointer to point to the code at
   PAGE_OFFSET+A+(something). 

   But since the CPU is, for those few instructions, fetching instructions based
   on physical addresses ***but having those instructions pass through address
   translation***, we must ensure that both the physical addresses and the
   virtual addresses are :

   1. Valid virtual addresses,and
   2. Point to the same code. 

   That means that at the very least,the initial page tables must map virtual
   address PAGE_OFFSET+(A+4) to physical address (A+4), and must map virtual
   address A+4 to physical address A+4. This dual mapping for the first 8MB of
   physical RAM is exactly what the initial page tables accomplish. The 8MB
   initally mapped is moreor less arbitrary. It's certain that no bootable kernel
   will be greater than 8MB in size. The identity mapping is discarded when the
   MM system gets initialized. ''
  
  --> include/asm-x86/page_32.h:
  /*
   * This handles the memory map.
   *
   * A __PAGE_OFFSET of 0xC0000000 means that the kernel has
   * a virtual address space of one gigabyte, which limits the
   * amount of physical memory you can use to about 950MB.
   *
   * If you want more physical memory than this then see the CONFIG_HIGHMEM4G
   * and CONFIG_HIGHMEM64G options in the kernel configuration.
   */
  #define __PAGE_OFFSET		_AC(CONFIG_PAGE_OFFSET, UL)

  --> arch/i386/kernel/head.S:
  /* Physical address */
  #define pa(X) ((X) - __PAGE_OFFSET)

  /*
   * Initialize page tables.  This creates a PDE and a set of page
   * tables, which are located immediately beyond _end.  The variable
   * init_pg_tables_end is set up to point to the first "safe" location.
   * Mappings are created both at virtual address 0 (identity mapping)
   * and PAGE_OFFSET for up to _end+sizeof(page tables)+INIT_MAP_BEYOND_END.
   *
   * Note that the stack is not yet set up!
   */
  #define PTE_ATTR	0x007		/* PRESENT+RW+USER */
  #define PDE_ATTR	0x067		/* PRESENT+RW+USER+DIRTY+ACCESSED */
  #define PGD_ATTR	0x001		/* PRESENT (no other attributes) */

  /*
   * **: >> 20 and not >> 22: Because Page Frame Address are aligned on 4KB
   * chunks. loworder 12bits are always zero. so we can represent it by only 
   * 20bytes as noted in the graph.
   */
  page_pde_offset = (__PAGE_OFFSET >> 20);

  /* **: After alot of questions in LKML, here's what I deduced from code below:
   * We want to store pg0 pte addresses to high 20bits swapper_pg_dir entries,
   * we also want the PRESENT,RW,USER flags be set in those entries. This is
   * done by  getting the address of each pg0 entry and adding 7 to it (Brian's
   * note). Since pg0 entires address are page aligned(Andreas, Jeremy notes),
   * adding 7 will assure that the first 3 bits are set without affecting the
   * high 20 bits.
   */

   movl $pa(pg0), %edi
   movl $pa(swapper_pg_dir), %edx
   movl $PTE_ATTR, %eax

10:
   leal PDE_ATTR(%edi),%ecx		/* Create PDE entry */
   movl %ecx,(%edx)			/* Store identity PDE entry */
   movl %ecx,page_pde_offset(%edx)	/* Store kernel PDE entry */
   
   /* Move to the next swapper_pg_dir_cell */
   addl $4,%edx
   movl $1024, %ecx

11:
   stosl
   addl $0x1000,%eax
   loop 11b
   /*
    * End condition: we must map up to and including INIT_MAP_BEYOND_END
    * bytes beyond the end of our own page tables; the +0x007 is
    * the attribute bits
    */
   leal (INIT_MAP_BEYOND_END+PTE_ATTR)(%edi),%ebp
   cmpl %ebp,%eax
   jb 10b
   movl %edi,pa(init_pg_tables_end)

   jmp 3f


 /*
  * Enable paging
  */
 3:
      movl $pa(swapper_pg_dir),%eax
      movl %eax,%cr3		/* set the page table pointer.. */
      movl %cr0,%eax
      orl  $X86_CR0_PG,%eax
      movl %eax,%cr0		/* ..and set paging (PG) bit */
      ljmp $__BOOT_CS,$1f	/* Clear prefetch and normalize %eip */


* When the identity mapping done in startup_32() is no longer necessary, kernel
  clears those pages using the zap_low_mappings() method as follows:

  --> arch/x86/mm/init_32.c:
  /* __pgd: tranfsorms passed value to a pgd suitable entry */
  void zap_low_mappings (void)
  {
	int i;

	/*
	 * Zap initial low-memory mappings.
	 *
	 * Note that "pgd_clear()" doesn't do it for
	 * us, because pgd_clear() is a no-op on i386.
	 */
	for (i = 0; i < USER_PTRS_PER_PGD; i++)
  #ifdef CONFIG_X86_PAE
		set_pgd(swapper_pg_dir+i, __pgd(1 + __pa(empty_zero_page)));
  #else
		set_pgd(swapper_pg_dir+i, __pgd(0));
  #endif
	flush_tlb_all();
  }
  

Caches and TLB:
---------------

* Hardware caches are addressed by cache lines. Linux tries to optimize the cache
  hit rate - respecting architecture differences -, it tries to:
  + put the most frequently used structure elements at a low offset to be cached
    in the same line.
  + When allocating large set of data structures, kernel tries to put them in
    memory in such way that all cache lines are used uniformly.

* Processors can't synchronize their own TLB automatically. kernel, not hardware,
  is the one that decides when mapping between a linear and a physical address is
  no longer valid.

* Kernel tries (and succeedes) to avoid TLB flushes in the following cases:
  + Performing a process switch between two regular processes that use the same
    set of page tables.
  + Performing a process switch between a regular process and a kernel thread.
    This is preferred cause kernel threads don't use low-memory (user-level) page
    mappings, so it's more efficient no to flush the TLB.

  This was pure assembly code in the past, but in newer kernels, more assembly 
  code is replaced by C code:

  --> include/asm-x86/system.h:
  static inline unsigned long native_read_cr3(void)
  {
	unsigned long val;
	asm volatile("mov %%cr3,%0\n\t" :"=r" (val), "=m" (__force_order));
	return val;
  }

  static inline void native_write_cr3(unsigned long val)
  {
	asm volatile("mov %0,%%cr3": :"r" (val), "m" (__force_order));
  }

  --> include/asm-x86/tlbflush.h:
  /* Flush tlb by writing cr3 to itself */
  static inline void __native_flush_tlb(void)
  {
	write_cr3(read_cr3());
  }

  /* Flush single entry, supported in Pentium Pro and above */
  static inline void __native_flush_tlb_single(unsigned long addr)
  {
	__asm__ __volatile__("invlpg (%0)" ::"r" (addr) : "memory");
  }

* Lazy TLB mode useful for deferring the TLB invalidation till really needed. this
  is represented by the tlb_state structure as follows:

  #ifdef CONFIG_X86_32
  
  #define TLBSTATE_OK	1
  #define TLBSTATE_LAZY	2

  struct tlb_state
  {
	struct mm_struct *active_mm;
	int state;
	char __cacheline_padding[L1_CACHE_BYTES-8];
  };
  DECLARE_PER_CPU(struct tlb_state, cpu_tlbstate);
  #endif	/* _X86_32 */

  
********************************************************************************
Processes:
**********

* From Kernel point of view, the purpose of a process is to act as an entity to
  which system resources are allocated.

* Older versions of the linux kernel offered no support for multithreaded
  applications. From the kernel view, multithreaded apps was just a normal
  process. Multiple execution flows were created, handled and scheduled entirely
  in user mode.

* Two LWP may share the same resources, like address space and open
  files. Whenever one of them modifies a shared resource, the other see the
  change immediately.

* Available Process States:

  --> include/linux/sched.h:
  /*
   * Task state bitmask. NOTE! These bits are also
   * encoded in fs/proc/array.c: get_task_state().
   *
   * We have two separate sets of flags: task->state
   * is about runnability, while task->exit_state are
   * about the task exiting. Confusing, but this way
   * modifying one set can't modify the other one by
   * mistake.
   */
  #define TASK_RUNNING		0
  #define TASK_INTERRUPTIBLE	1
  #define TASK_UNINTERRUPTIBLE	2
  #define __TASK_STOPPED	4
  #define __TASK_TRACED		8
  /* in tsk->exit_state */
  #define EXIT_ZOMBIE		16
  #define EXIT_DEAD		32
  /* in tsk->state again */
  #define TASK_DEAD		64
  #define TASK_WAKEKILL		128

  #define __set_task_state(tsk, state_value)		\
	do { (tsk)->state = (state_value); } while (0)
  #define set_task_state(tsk, state_value)		\
	set_mb((tsk)->state, (state_value))

  /* Convenience macros for the sake of set_task_state */
  #define TASK_KILLABLE		(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)
  #define TASK_STOPPED		(TASK_WAKEKILL | __TASK_STOPPED)
  #define TASK_TRACED		(TASK_WAKEKILL | __TASK_TRACED)

  ** In  task_struct->state:

  TASK_RUNNING        : Executing or on a CPU waiting to be executed
  TASK_INTERRUPTIBLE  : sleeping till some condition becomes true. can be waken up by
		        a hardware interrupt or a signal. (letting it TASK_RUNNING)
  TASK_UNINTERRUPTIBLE: Like above but can't be waken up by a hardware interrupt or
		        a signal. Seldom used, used in some valuable spaces (ex:
		        exiting may leave the hardware in an upredictable state).
  TASK_STOPPED        : Process stopped as a result of recieving a SIGSTOP, SIGSTP 
  			or others.
  TASK_TRACED         : Process execution has been stopped by a debugger. 

  ** In task_struct->exit_state:

  EXIT_ZOMBIE         : Process execution is terminated, but parent has not yet issued 
  		        a wait()-like system call to return information about the dead
	     		process.


  EXIT_DEAD           : Final state, process is being removed by the system cause the 
  		      	parent process has just issued a wait()-like system call.

* It's better to use set_current_state(TASK_STATE) that sets a compiler barrier
  to insure the right instruction order.

Identifying Processes:
----------------------

* As a general rule, each execution context that can be independently scheduled
  has its own process descriptor.

* Strict one-to-one correspondonce between the the process and its descriptor
  makes the 32bit address of the task struct a useful way by the kernel to
  identify a process (like in scheduling).

--- IDs --- :
*************

task_struct->pid:
.................
* Users can identify processes by means of PID in task_struct->pid. 
  PID of new porcess = PID of last created process + 1.
  So when we reach the maximum PID value 32767 (PID_MAX_DEFAULT -1), it must
  start recycling the lower unused PIDs.

  PIDs are marked as used or not in a bitmap array that takes a whole page.
  Since a page fram is 4Kb (2 ^ 12), then our upper limit is the number of
  bits in a page. Number of a bits in a page = (2 ^ 12) * 8 = 32767.

  --> include/linux/threads.h:
  #define PID_MAX_DEFAULT (CONFIG_BASE_SMALL ? 0x1000 : 0x8000)
  
  /*
   * A maximum of 4 million PIDs should be enough for a while.
   * [NOTE: PID/TIDs are limited to 2^29 ~= 500+ million, see futex.h.]
   * **: This limit can be reached only in 64bit archs (long > 4)
   */
  #define PID_MAX_LIMIT (CONFIG_BASE_SMALL ? PAGE_SIZE * 8 : \
	(sizeof(long) > 4 ? 4 * 1024 * 1024 : PID_MAX_DEFAULT))

  CONFIG_BASE_SMALL is used to reduce the size of miscellinious kernel core
  structures and letting it be suitable for little-memory embedded devices.

task_struct->tgid:
..................
* Linux associates a different PID with each process or LWP. This allows
  maximum flexibility cause each execution context can be _uniquely_ identified.
  BUT Unix programmers expect threads in the same group to have a common PID.
  To deal with the kernl/user space contradiction, Linux uses a thread groups
  technique described below.

* Using thread groups technique, the identifier shared by the threads is the PID
  of the thread group leader. Group leader is the first LWP in the group. this
  id is stored in the tgid field. so getpid() system call returns the value of
  tgid relative to the current process. 

* Even in single-thread apps, getpid() returns the tgid field. As thread group
  leaders (and only member), tgid = pid. So getpid() works as usual here

  --> kernel/timer.c:
  /**
   * sys_getpid - return the thread group id of the current process
   *
   * Note, despite the name, this returns the tgid not the pid.  The tgid and
   * the pid are identical unless CLONE_THREAD was specified on clone() in
   * which case the tgid is the same in all threads of the same group.
   *
   * This is SMP safe as current->tgid does not change.
   */
   asmlinkage long sys_getpid(void)
   {
	return current->tgid;	
   }

--- Processes Lists --- :
*************************

struct list_head tasks:
.......................
* represent the current process node in the current proccesses list.
  tasks->next and prev represent the previous and the next task_struct element.

* The head of the processes list is the the process descriptor of the so-called
  process 0 (swapper or idle process). tasks->prev of this process represents the
  last process added to the system.

  --> include/linux/sched.h:
  #define next_task(p)	list_entry(rcu_dereference((p)->tasks.next), struct task_struct, tasks)

  #define for_each_process(p) \
	for (p = &init_task ; (p = next_task(p)) != &init_task ; )


struct list_head run_list:
..........................
* links the process descriptor to into the list of runnable processes having
  priority k (one of 140 lists). Done to achieve constant time scheduling.

int prio, static_prio, normal_prio:
...................................
integers to specify process priority. prio is the element used to specify the
process runqueue

	/* Add process to the tail of its equivalent priority runqueue */
	list_add_tail(&p->run_list, array->queue + p->prio);

struct prio_array *array:
.........................
The priority array structure current process belongs to.

--- Relationships among processes ---:
**************************************

struct task_struct *real_parent:
................................
points to the process descriptor that created current process (when being
debugged).

struct task_struct *parent:
...........................
Points to the current parent of P. It may differ with real_parent when debugging.
Ex: When a process issues a ptrace system call to be allowed to monitor P.

struct list_head children:
..........................
Head of list containing all the childrean created by P

struct list_head sibling:
.........................
List node of processes which have same parent as P. The head of this list is 
P->parent->children.

struct task_struct *group_leader:
.................................
Process descriptor of the group leader of P.
Process group is a way to present a job abstraction. Like in $ls | sort | more.
Shell creates a group for the three and treates them as one entity.

/*    
 * ptrace_list/ptrace_children forms the list of my children
 * that were stolen by a ptracer.
 */
struct list_head ptrace_children:
.................................
Head of a list containing all children of P traced by a debugger

struct ptrace_list:
...................
Node of real parent's list of traced processes (used when P is being traced).
The head of that list is P->parent->ptrace_children.

--- ptrace --- :
****************

unsigned long ptrace_message:
.............................
used when the parent process is traced. when it creates a child using the
do_fork() method, it put its ID in the ptrace_message field and send a SIGCHLD
for the parent to notify the debugger with its existence and its ID.


TO DO: talk about usage, did_exec, flags(PF_SUPERPRIV, PF_FORKNOEXEC, ...)

Process Descriptor Handling:
----------------------------

* Kernel must be able to handle many processes at the same time but Processes
  descriptors are stored in dynamic memory rather than a memory area directly
  permanently assigned to the kernel. 

* Linux packs two different data structures in a single per process memory area
  a - A small data structure (thread_info structure)
  b - Kernel mode process stack
  Length of this mem area is usually two page frames (8KB).

* A process in kernel mode accesses a stack contained in the kernel data segment
  which is different from the user-mode stack. the %esp register is the CPU stack
  pointer, used to address the stack top location. Stack creation: 
  
  a - Right after switching from user to kernel mode, kernel stack of a
      process is always empty.

  b - Value of esp is decreased as soon as data is written to the stack. Writing
      the thread_info structure (52 bytes) leaves the stack with 8,140 bytes.

* Kernel allocates and releases thread info using this macros:

  --> include/asm-x86/thread_info_32.h:
  #define PREEMPT_ACTIVE	0x10000000
  #ifdef CONFIG_4KSTACKS
  #define THREAD_SIZE           (4096)
  #else
  #define THREAD_SIZE		(8192)
  #endif

  /* thread information allocation */
  #ifdef CONFIG_DEBUG_STACK_USAGE
  #define alloc_thread_info(tsk) ((struct thread_info *) \
	__get_free_pages(GFP_KERNEL| __GFP_ZERO, get_order(THREAD_SIZE)))
  #else
  #define alloc_thread_info(tsk) ((struct thread_info *) \
	__get_free_pages(GFP_KERNEL, get_order(THREAD_SIZE)))
  #endif

  #define free_thread_info(info)  free_pages((unsigned long)(info), get_order(THREAD_SIZE))

  /* thread information allocation */
  #ifdef CONFIG_DEBUG_STACK_USAGE
  #define alloc_thread_info(tsk) kzalloc(THREAD_SIZE, GFP_KERNEL)
  #else
  #define alloc_thread_info(tsk) kmalloc(THREAD_SIZE, GFP_KERNEL)
  #endif

* Kernel can easily get the current process descriptor. Since the kmode stack is
  8kb (2 ^ 13bits). Masking out the least significant 13bits of %esp leads to the
  base address of the thread_info structure. 

  Example:
  
  --> %esp				0xffff bfff
  --> end first page + 1		0xffff b000       
  --> begin_first_page/thread_info	0xffff a000

  Masking %esp first 13 bits will lead to thread_info address.

  This is done in the following code:

  --> include/linux/sched.h:
  union thread_union {
	struct thread_info thread_info;
	unsigned long stack[THREAD_SIZE/sizeof(long)];
  };

  --> include/asm-x86/thread_info_32.h:
  /* how to get the current stack pointer from C */
  register unsigned long current_stack_pointer asm("esp") __attribute_used__;

  /* how to get the thread information struct from C */
  static inline struct thread_info *current_thread_info(void)
  {
	return (struct thread_info *)(current_stack_pointer & ~(THREAD_SIZE - 1));
  }

  Above C code translates to the following asm code:
  movl $0xffffe000,%ecx /* or 0xfffff000 for 4KB stacks */
  andl %esp,%ecx 
  movl %ecx,p
  # OR
  movl (%ecx),p		# To directly get the address of the process descriptor
       			# since it's already on offset 0

  --> This happens cause the thread_union always begin at the beginning of a
      page and takes two pages. So masking the first 12 bits leads to the
      beginning of the second page, masking the 13th bit leads to the address
      of the first page.

* Storing process descriptor with the stack is useful on SMP systems. Instead of
  using a global `current' variable or current[numOfCPUs], The correct current
  processor for each hardware processor can be derived by checking the stack. 

* Is above point collide with following code ?
  
  --> include/asm-x86/current_32.h:
  #include <linux/compiler.h>
  #include <asm/percpu.h>

  struct task_struct;

  DECLARE_PER_CPU(struct task_struct *, current_task);
  static __always_inline struct task_struct *get_current(void)
  {
	return x86_read_percpu(current_task);
  }
 
  #define current get_current()

Lists of TASK_RUNNING processes:
--------------------------------

* Please read the kernel doubly linked list discussion at ULK section 3.2.2.3
  and also see the code at include/linux/list.h.
  Important: to use it, we don't put data/structure in the list_head structure.
  	     Otherwise, we include the list_head structure in each node of our
	     custom-structure list.
	     ie. to make a list of running processes, we put a list_head in each
	     task_struct.

* Processes descriptors are maintained in a doubly linked list. The head of the
  process list is the init_task descriptor. It plays the role of the process list
  header as follows:

  --> include/linux/list.h:
  /**
   * list_entry - get the struct for this entry
   * @ptr:	the &struct list_head pointer.
   * @type:	the type of the struct this is embedded in.
   * @member:	the name of the list_struct within the struct.
   */
  #define list_entry(ptr, type, member) \
	 container_of(ptr, type, member)

  --> include/linux/sched.h:
  #define next_task(p)	list_entry((p)->tasks.next, struct task_struct, tasks)

  #define for_each_process(p) \
	for (p = &init_task ; (p = next_task(p)) != &init_task ; )

* Earlier linux versions (pre 2.5) put all runnable processes in a list called
  runqueue. cause it'll be too costly to order the list according to to process
  priorities, earlier scheduler were compelled to scan the whole list to select
  the best runnable process.

* In linux 2.6, to provide a constant time scheduler. the runqueue is split to 
  many lists of runnable processes, one list per each priority. run_list
  represents the `runqueue' the process belongs to..

* The main data structures of a runqueue are the lists or process descriptors
  belonging to the runqueue. All these are implemented by a single prio_array_t
  structure as follows:

  --> kernel/sched.c:
  struct rt_prio_array {
	/* 
	 * ** A priority bitmap: each flag is set iff the corresponding priority
	 * list is not empty 
	 * Include 1bit for delimiter
         */
	DECLARE_BITMAP(bitmap, MAX_RT_PRIO+1); 
	/* ** The 140 (MAX_PRIO) heads of the priority list */
	struct list_head queue[MAX_RT_PRIO];
  };

* Code to insert a new process in the system:
      
  In pre 2.6.22:

  --> kernel/sched.c:
  static void enqueue_task(struct task_struct *p, struct prio_array *array)
  {
	/* Collect some info for the scheduler (such as time) */
	sched_info_queued(p);

	/* Add process to the tail of its equivalent priority runqueue */
	list_add_tail(&p->run_list, array->queue + p->prio);

	/* maybe we're the only process in the current list */
	__set_bit(p->prio, array->bitmap);

	array->nr_active++;
	p->array = array;
  }

  In post 2.6.23:

  --> include/linux/sched.h:
  struct sched_class {
	struct sched_class *next;

	void (*enqueue_task) (struct rq *rq, struct task_struct *p, int wakeup);
	void (*dequeue_task) (struct rq *rq, struct task_struct *p, int sleep);
	void (*yield_task) (struct rq *rq, struct task_struct *p);
	
	...

  --> kernel/sched.c:
  static void enqueue_task(struct rq *rq, struct task_struct *p, int wakeup)
  {
	sched_info_queued(p);
	p->sched_class->enqueue_task(rq, p, wakeup);
	p->se.on_rq = 1;
  }

  Every scheduler has it's instance of above structure as found in files
  kernel/sched_rt.c, kerenl/sched_fair.c, kerenl/sched_idletask.c

PID to Process descriptor efficient transfromation:
---------------------------------------------------

* It can be done by bruteforcing the processes list. It's not efficient since we
  need this kind of transformation alot (espicially in signalling another
  processes). Multiple hash table exists for each kind of PIDs.

  Hash Table Type           task_struct field_name    Description
  PIDTYPE_PID		    pid			      PID of the process
  PIDTYPE_TGID 		    tgid		      PID of thread group leader process
  PIDTYPE_PGID		    pgrp		      PID of the group leader process
  PIDTYPE_SID		    session		      PID of the session leader process

* The PID is transformed to a _table index_ using the pid_hashfn macro.
  --> kernel/pid.c:
  #define pid_hashfn(nr) hash_long((unsigned long)nr, pidhash_shift)
  --> include/linux/hash.h:
  static inline unsigned long hash_long(unsigned long val, unsigned int bits)
  {
	unsigned long hash = val;

	/* On some cpus multiply is faster, on others gcc will do shifts */
	hash *= GOLDEN_RATIO_PRIME;

	/* High bits are more random, so use them. */
	return hash >> (BITS_PER_LONG - bits);
}

* Hashing with chaining is preferable to linear transformation from PIDs to table
  indexes. Hashing with linear transformation needs an array with 2^16 elements (MAXPID). 
  At any given time, number of processors in the system is usually far below 32,768 (2^16). 
  So doing linear hashing is a waste of memory.

How Processes are organized:
----------------------------

* TASK_RUNNING Processes are grouped in different runqueues as mentioned
  earlier. Other states are organized as:
  + TASK_STOPPED, EXIT_ZOMBIE or EXIT_DEAD: no need to group those proecsses in
    any lists. They're only accessed through their parents or PIDs.
  + TASK_INTERRUPTIBLE , TASK_UNINTERRUPTIBLE: handled using waitqueues.

* Wait queues represents a set of sleeping processes, which are woken up by the
  kernel when some condition becomes true.

* Wait queues are implemented as doubly linked lists whose elements include
  pointers to process descriptors. Each wait queue is defined by a wait queue
  head:

  --> include/linux/wait.h:
  struct __wait_queue_head {
	spinlock_t lock;		/* should be thread/interrupts safe */
	struct list_head task_list;
  };
  typedef struct __wait_queue_head wait_queue_head_t;
  
* Each element in the wait queue represents a sleeping process, which is waiting
  for some event to occur. 

  --> include/linux/wait.h:
  struct __wait_queue {
  	/* Exclusive (1) or nonexclusive (0) processes waiting */
	unsigned int flags;
  #define WQ_FLAG_EXCLUSIVE	0x01
	void *private;
	/* How the sleeping processes should be waking up */
	wait_queue_func_t func;
 	/* Node of the list of the sleeping processes waiting for the same event */
	struct list_head task_list;	
  };
  typedef struct __wait_queue wait_queue_t;

* To avoid the `thundering herd' problem. the flags element is used to denote if
  the processes is waiting for an exclusive event (only one of the them can use
  the resource and then will block the others) or nonexclusive event.

  --> include/linux/wait.h:
  /* Static method */
  #define __WAIT_QUEUE_HEAD_INITIALIZER(name) {				\
	.lock		= __SPIN_LOCK_UNLOCKED(name.lock),		\
	/* Head next and prev pointers point to the head itself */
	.task_list	= { &(name).task_list, &(name).task_list } }

  #define DECLARE_WAIT_QUEUE_HEAD(name) \
	wait_queue_head_t name = __WAIT_QUEUE_HEAD_INITIALIZER(name)

  extern void init_waitqueue_head(wait_queue_head_t *q);

  /* Dynamic method (Parameter is allocated dynamically) */
  void init_waitqueue_head(wait_queue_head_t *q)
  {
	spin_lock_init(&q->lock);
	INIT_LIST_HEAD(&q->task_list);
  }
  EXPORT_SYMBOL(init_waitqueue_head);

  static inline void init_waitqueue_entry(wait_queue_t *q, struct task_struct *p)
  {
	q->flags = 0;		/* Non exclusive wakeup */
	q->private = p;
	q->func = default_wake_function;
  }

  /* _add_wait_queue_tail is the same but using list_add_tail */
  static inline void __add_wait_queue(wait_queue_head_t *head, wait_queue_t *new)
  {
	list_add(&new->task_list, &head->task_list);
  }

  /*
   * Used for wake-one threads (Exclusive wakups, see prepare_to_wait_eclusive)
   * I think the idea is that the nodes on the left wait_queue_head_t list node
   * are exclusive wakeup ones  while the ones in the right are nonexclusive
   * wakeup processes (won't be affected by the thundering herd problem).
   */
  static inline void __add_wait_queue_tail(wait_queue_head_t *head,
						wait_queue_t *new)
  {
	list_add_tail(&new->task_list, &head->task_list);
  }

  --> kernel/wait.c:
  void fastcall add_wait_queue(wait_queue_head_t *q, wait_queue_t *wait)
  {
	unsigned long flags;

	wait->flags &= ~WQ_FLAG_EXCLUSIVE;
	spin_lock_irqsave(&q->lock, flags);
	__add_wait_queue(q, wait);
	spin_unlock_irqrestore(&q->lock, flags);
  }
  EXPORT_SYMBOL(add_wait_queue);

  /* Called to add our wait_queue to a wait_queue_head and set our state
   * to INTERRUPTIBLE or UNINTERRUPTIBLE. The state change won't take 
   * effect except after a call to schedule */
  void fastcall
  prepare_to_wait(wait_queue_head_t *q, wait_queue_t *wait, int state)
  {
	unsigned long flags;

	wait->flags &= ~WQ_FLAG_EXCLUSIVE;
	spin_lock_irqsave(&q->lock, flags);

	/* If the waitqueue_t list node isn't attached to any list, then it is not
	 * attached yet to any waitqueue */
	if (list_empty(&wait->task_list))
		__add_wait_queue(q, wait);

	/*
	 * don't alter the task state if this is just going to
	 * queue an async wait queue callback
	 * ** See is_sync_wait defeinition from wait.h
	 */
	if (is_sync_wait(wait))
		set_current_state(state);
	spin_unlock_irqrestore(&q->lock, flags);
  }

  /* Only different parts from the above nonexclusive wait function are written */
  void fastcall
  prepare_to_wait_exclusive(wait_queue_head_t *q, wait_queue_t *wait, int state)
  {
	wait->flags |= WQ_FLAG_EXCLUSIVE;
	[...]
	/* Note here, it's different from the above by using __add_wait_queue_tail */
	if (list_empty(&wait->task_list))
		__add_wait_queue_tail(q, wait);
  }
  EXPORT_SYMBOL(prepare_to_wait_exclusive);

  void fastcall finish_wait(wait_queue_head_t *q, wait_queue_t *wait)
  {
	unsigned long flags;

	/*
	 * Set to TASK_RUNNING, just in case the awakening condition became
	 * true before invoking schedule 
	 * ** see below usage example.
	 */
	__set_current_state(TASK_RUNNING);

	/*
	 * We can check for list emptiness outside the lock
	 * IFF:
	 *  - we use the "careful" check that verifies both
	 *    the next and prev pointers, so that there cannot
	 *    be any half-pending updates in progress on other
	 *    CPU's that we haven't seen yet (and that might
	 *    still change the stack area.
	 * and
	 *  - all other users take the lock (ie we can only
	 *    have _one_ other CPU that looks at or modifies
	 *    the list).
	 */
	if (!list_empty_careful(&wait->task_list)) {
		spin_lock_irqsave(&q->lock, flags);
		/*
		 * _del_init variant is used so waitqueue can be 
		 * reinitialized with prepare_to_wait
		 * Again, see usage example!
		 */
		list_del_init(&wait->task_list);
		spin_unlock_irqrestore(&q->lock, flags);
	}
  }
  EXPORT_SYMBOL(finish_wait);

* Now how the above functions (prepare_to_wait, finish_wait, ..) are used ?. Here's
  a very good example from LDD3:
  
  /* Wait for space for writing; caller must hold device semaphore.  On
   * error the semaphore will be released before returning. */
  static int scull_getwritespace(struct scull_pipe *dev, struct file *filp)
  {
	while (spacefree(dev) == 0) { /* full */
		DEFINE_WAIT(wait);
		
		up(&dev->sem);
		if (filp->f_flags & O_NONBLOCK)
			return -EAGAIN;
		PDEBUG("\"%s\" writing: going to sleep\n",current->comm);
		prepare_to_wait(&dev->outq, &wait, TASK_INTERRUPTIBLE);
		if (spacefree(dev) == 0)
			schedule();
		finish_wait(&dev->outq, &wait);
		if (signal_pending(current))
			/* signal: tell the fs layer to handle it */
			return -ERESTARTSYS; 
		if (down_interruptible(&dev->sem))
			return -ERESTARTSYS;
	}
	return 0;
  }	

  and it's used like:

  static ssize_t scull_p_write(struct file *filp, const char __user *buf, size_t count,
                loff_t *f_pos)
  {
	struct scull_pipe *dev = filp->private_data;
	int result;

	if (down_interruptible(&dev->sem))
		return -ERESTARTSYS;

	/* Make sure there's space to write */
	result = scull_getwritespace(dev, filp);
	if (result)
		return result; /* scull_getwritespace called up(&dev->sem) */

	/* ok, space is there, accept something */

* Kernel developers created macros to ease up the sleeping process and hide the
  above point and LDD3 code details. Macros are called wait_event_*.
  
  --> include/linux/wait.h:
  #define __wait_event(wq, condition) 					\
  do {									\
	DEFINE_WAIT(__wait);						\
									\
	for (;;) {							\
		prepare_to_wait(&wq, &__wait, TASK_UNINTERRUPTIBLE);	\
		if (condition)						\
			break;						\
		schedule();						\
	}								\
	finish_wait(&wq, &__wait);					\
  } while (0)

  #define wait_event(wq, condition) 					\
  do {									\
	if (condition)	 						\
		break;							\
	__wait_event(wq, condition);					\
  } while (0)
  
* The kernel wakes up processes in waitqueues by means of wake_up_* macros which
  invokes the default_wake_up method (or other custom method) in its code as
  follows:

  --> include/linux/wait.h:
  /*
   * No "interruptible" means that macro also awakens TASK_UNINTERRUPTIBLE waitque processes.
   * "nr" means the needed number of _exclusive_ processes to be woken up.
   * "all" wakes all the processes having the _required_ state.
   * No "sync" means check whether the priority of any woken process is higher than the
   * current running process and invokes schedule().
   * "locked" assume the workqueue lock is already held.
   */
  #define wake_up(x)			  __wake_up(x, TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE, 1, NULL)
  #define wake_up_nr(x, nr)		  __wake_up(x, TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE, nr, NULL)
  #define wake_up_all(x)		  __wake_up(x, TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE, 0, NULL)
  #define wake_up_interruptible(x)	  __wake_up(x, TASK_INTERRUPTIBLE, 1, NULL)
  #define wake_up_interruptible_nr(x, nr) __wake_up(x, TASK_INTERRUPTIBLE, nr, NULL)
  #define wake_up_interruptible_all(x)	  __wake_up(x, TASK_INTERRUPTIBLE, 0, NULL)
  #define wake_up_locked(x)		  __wake_up_locked((x), TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE)
  #define wake_up_interruptible_sync(x)   __wake_up_sync((x),TASK_INTERRUPTIBLE, 1)

  --> kernel/sched.c:
  /**
   * __wake_up - wake up threads blocked on a waitqueue.
   * @q: the waitqueue
   * @mode: which threads
   * @nr_exclusive: how many wake-one or wake-many threads to wake up
   * @key: is directly passed to the wakeup function
   */
  void fastcall __wake_up(wait_queue_head_t *q, unsigned int mode,
			int nr_exclusive, void *key)
  {
	unsigned long flags;

	spin_lock_irqsave(&q->lock, flags);
	__wake_up_common(q, mode, nr_exclusive, 0, key);
	spin_unlock_irqrestore(&q->lock, flags);
  }
  EXPORT_SYMBOL(__wake_up);

  /*
   * Same as __wake_up but called with the spinlock in wait_queue_head_t held.
   */
  void fastcall __wake_up_locked(wait_queue_head_t *q, unsigned int mode)
  {
	__wake_up_common(q, mode, 1, 0, NULL);
  }

  /*
   * The core wakeup function.  Non-exclusive wakeups (nr_exclusive == 0) just
   * wake everything up.  If it's an exclusive wakeup (nr_exclusive == small +ve
   * number) then we wake all the non-exclusive tasks and one exclusive task.
   *
   * There are circumstances in which we can try to wake a task which has already
   * started to run but is not in state TASK_RUNNING.  try_to_wake_up() returns
   * zero in this (rare) case, and we handle it by continuing to scan the queue.
   */
  static void __wake_up_common(wait_queue_head_t *q, unsigned int mode,
	          	       int nr_exclusive, int sync, void *key)
  {
	struct list_head *tmp, *next;

	list_for_each_safe(tmp, next, &q->task_list) {
		wait_queue_t *curr = list_entry(tmp, wait_queue_t, task_list);
		unsigned flags = curr->flags;

		/* Non-exclusive processes are always the first ones in waitqueue
		 * list, then exclusive ones. Wakeup the process using it's func()
		 * wakeup function. 
		 * In most cases, func() is the default_wake_function() 
		 * Note: Exclusive and non-exclusive processes rarely exist in a 
		 * unique wait-queue */
		if (curr->func(curr, mode, sync, key) &&
				(flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
			break;
	}
  }

* default_wake_function(), the most used method in wait_queue->func(), is just 
  a wrapper of try_to_wake_up discussed later.
  
  --> kernel/sched.c:
  int default_wake_function(wait_queue_t *curr, unsigned mode, int sync,
			  void *key)
  {
	return try_to_wake_up(curr->private, mode, sync);
  }
  EXPORT_SYMBOL(default_wake_function);

Process Resource Limits:
------------------------

********************************************************************************
Process Switches:
*****************

Hardware Context:
-----------------

* Old versions of linux took advantage of the 80x86 support and performed a
  process switch using a far jump to the TSS descriptor of the next process
  (modifying the %cs and %eip registers).

* Linux 2.6 uses software to perform process switch for better control over the
  context mechanism.

* Process switching occurs only in kernel mode. The contents of all registers
  used by a process in user mode have already been saved in the kernel mode stack
  (in `thread_struct thread' particularly) before performing process switching.

* Though 2.6 do the context in software, it's forced to use the TSS for the
  following cases:
  a- when 80x86 CPU switches from usermode to kernel mode, it fetches the address
     of the kernel mode stack from the TSS.

     |---------------+---------------+---------------+---------------|
     |                              ESP                              |38
     |---------------+---------------+---------------+---------------|

  b- when a Usermode process attempts to access an I/O port using in() and out(),
     CPU may need to acces an I/O permission bitmap stored in the TSS to verify
     whethere the processor is allowed or not.

    31              23              15              7             0
     +---------------+---------------+---------------+---------------+
     |          I/O MAP BASE         | 0 0 0 0 0 0 0   0 0 0 0 0 0 |T|64
     |---------------+---------------+---------------+---------------|
     |0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0|              LDT              |60
     |---------------+---------------+---------------+---------------|
     |0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0|              GS               |5C
     |---------------+---------------+---------------+---------------|
     |0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0|              FS               |58
     |---------------+---------------+---------------+---------------|
     |0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0|              DS               |54
     |---------------+---------------+---------------+---------------|
     |0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0|              SS               |50
     |---------------+---------------+---------------+---------------|
     |0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0|              CS               |4C
     |---------------+---------------+---------------+---------------|
     |0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0|              ES               |48
     |---------------+---------------+---------------+---------------|
  
* In Intel's original design, each process has its own TSS. In Linux design,
  there's only one TSS for each CPU. The init_tss array stores one TSS for each
  CPU on the system. At each process switch, the kernel updates some fields of
  the TSS so that the corresponding CPU's control unit may safely retrieve the
  information it needs. Thus, the TSS reflects the privilege of the current
  process on the CPU, but there is no need to maintain TSSs for processes when
  they're not running.

* Since linux does not use hardware to store the hardware context in the
  per-process (intel's design) TSS segment, Each processor descriptor includes a
  `struct thread_struct thread' to save the hardware context of each process.  

  /*
   * The general purpose registers eax, ebx, .. are not stored here but
   * in the kernel mode stack
   */

  --> include/asm-x86/processor.h:
  struct thread_struct {
  	/* cached TLS descriptors. */
	struct desc_struct tls_array[GDT_ENTRY_TLS_ENTRIES];
	unsigned long	sp0;
	unsigned long	sp;
  #ifdef CONFIG_X86_32
	unsigned long	sysenter_cs;
  #else
	unsigned long 	usersp;	/* Copy from PDA */
	unsigned short	es, ds, fsindex, gsindex;
  #endif
	unsigned long	ip;
	unsigned long	fs;
	unsigned long	gs;
	/* Hardware debugging registers */
	unsigned long	debugreg0;
	unsigned long	debugreg1;
	unsigned long	debugreg2;
	unsigned long	debugreg3;
	unsigned long	debugreg6;
	unsigned long	debugreg7;
	/* fault info */
	unsigned long	cr2, trap_no, error_code;
	/* floating point info */
	union i387_union	i387 __attribute__((aligned(16)));;
  #ifdef CONFIG_X86_32
  	/* virtual 86 mode info */
	struct vm86_struct __user *vm86_info;
	unsigned long		screen_bitmap;
	unsigned long		v86flags, v86mask, saved_sp0;
	unsigned int		saved_fs, saved_gs;
  #endif
	/* IO permissions */
	unsigned long	*io_bitmap_ptr;
	unsigned long	iopl;
	/* max allowed port in the bitmap, in bytes: */
	unsigned io_bitmap_max;
	/* MSR_IA32_DEBUGCTLMSR value to switch in if TIF_DEBUGCTLMSR is set.  */
	unsigned long	debugctlmsr;
	/* Debug Store - if not 0 points to a DS Save Area configuration;
 	 *               goes into MSR_IA32_DS_AREA */
	unsigned long	ds_area_msr;
  };

* Every process switch consits of two steps:
  1- Switching the page global directory to the new address space.
  2- Switching the kernel mode stack AND the new hardware context.

* In any process switch three process are involved, not just two!. 
  Why using three parameters instead of two ?
  
  Suppose a process wishes to stop process A and activates process B. wrt A, prev
  points to A process descriptor while next points to B's one. 
  
  Now assume process C is goin to stop and give control back to A. After
  transforming the stack to A, prev has A while next has B. The schedule() code
  lost any reference to C! (which it needs in later lines of code).

  The last parameter in the switch macro is an output parameter that specifies
  a memory location in which the macro writes the descriptor address of process
  C. Before the process switching, the macro saves in the %eax register the
  content identified by the prev variable. 

  After resuming execution, the macro writes the content of the eax register in
  the memory location of A identified by the third parameter `last'. 

  Equivalent Assembly code:

  /* Save prev and next addresses in %eax and %edx */
  movl prev, %eax
  movl next, %edx

  /* Save the flags and %ebp, they'll be stored again when returning
   * back (beginning executing from $1) */
  pushfl
  pushl %ebp 

  /* save the current stack pointer in prev->thread.esp */
  movl %esp, 484(%eax)

  /* from this moment, we're the new porcess */
  movl 484(%edx), %esp

  /* save address labeled 1 in prev->thread.eip. When proecess being replaced
   * returns to execution again, it'll begin from 1 */  
  movl $1f, 480(%eax)

  /* On the kernel mode stack of next, push next->thread.eip value in the stack.
   * This is done cause after __switch_to is finished, it executes the "ret"
   * instruction. The `ret' instruction puts in EIP the value on the top of the
   * stack (Our original %eip [next->thread->eip]). */
  pushl 480(%edx)
  jmp __switch_to

  1:
     popl %ebp
     popfl

  /* Copy the address of the process descriptor that has just been 
   * replaced in last. last is the new prev (instead of the new process's
   * prev) cause in the whole kernel code, switch_to is called once as:
   * switch_to(prev, next, prev) */
  mov %eax, last

* Tha above code is found as extended assembly in the linux sources as follows:
  --> include/asm-x86/system.h:
  /*
   * Saving eflags is important. It switches not only IOPL between tasks,
   * it also protects other tasks from NT leaking through sysenter etc.
   */
  #define switch_to(prev,next,last) do {				\
	unsigned long esi,edi;						\
	asm volatile("pushfl\n\t"		/* Save flags */	\
		     "pushl %%ebp\n\t"					\
		     "movl %%esp,%0\n\t"	/* save ESP */		\
		     "movl %5,%%esp\n\t"	/* restore ESP */	\
		     "movl $1f,%1\n\t"		/* save EIP */		\
		     "pushl %6\n\t"		/* restore EIP */	\
		     "jmp __switch_to\n"				\
		     "1:\t"						\
		     "popl %%ebp\n\t"					\
		     "popfl"						\
		     :"=m" (prev->thread.esp),"=m" (prev->thread.eip),	\
		      "=a" (last),"=S" (esi),"=D" (edi)			\
		     :"m" (next->thread.esp),"m" (next->thread.eip),	\
		      "2" (prev), "d" (next));				\
  } while (0)
  
* As noted by above assembly code, __switch_to do the rest of the process switch
  started by the switch_to() macro. 

  --> incclude/asm-x86/system.h:
  struct task_struct; /* one of the stranger aspects of C forward declarations */
  struct task_struct *__switch_to(struct task_struct *prev,
	 	                  struct task_struct *next);

  Kernel 2.6.25+ note:
  X86_32 now uses regparm(3) instead of __stdcall calling convention by 
  default, FASTCALL() is no longer required.

  reparm(3) puts the parameters in registers eax, edx ... registers instead
  of the stack

  Pre 2.6.25 kernels:
  --> include/asm-x86/linkage.h:
  #define asmlinkage CPP_ASMLINKAGE __attribute__((regparm(0)))
  /* Let %eax and %edx be the parameters of the function */
  #define FASTCALL(x)	x __attribute__((regparm(3)))
  #define fastcall	__attribute__((regparm(3)))

  --> arch/x86/kernel/process_32.c:
  /*
   *	switch_to(x,yn) should switch tasks from x to y.
   *
   * We fsave/fwait so that an exception goes off at the right time
   * (as a call from the fsave or fwait in effect) rather than to
   * the wrong process. Lazy FP saving no longer makes any sense
   * with modern CPU's, and this simplifies a lot of things (SMP
   * and UP become the same).
   *
   * NOTE! We used to use the x86 hardware context switching. The
   * reason for not using it any more becomes apparent when you
   * try to recover gracefully from saved state that is no longer
   * valid (stale segment register values in particular). With the
   * hardware task-switch, there is no way to fix up bad state in
   * a reasonable manner.
   *
   * The fact that Intel documents the hardware task-switching to
   * be slow is a fairly red herring - this code is not noticeably
   * faster. However, there _is_ some room for improvement here,
   * so the performance issues may eventually be a valid point.
   * More important, however, is the fact that this allows us much
   * more flexibility.
   *
   * The return value (in %ax) will be the "prev" task after
   * the task-switch, and shows up in ret_from_fork in entry.S,
   * for example.
   */
  struct task_struct * __switch_to(struct task_struct *prev_p, struct task_struct *next_p)
  {
	struct thread_struct *prev = &prev_p->thread,
				 *next = &next_p->thread;
	int cpu = smp_processor_id();
	struct tss_struct *tss = &per_cpu(init_tss, cpu);

	/* never put a printk in __switch_to... printk() calls wake_up*() indirectly */

	__unlazy_fpu(prev_p);
	/* we're going to use this soon, after a few expensive things */
	if (next_p->fpu_counter > 5)
		prefetch(&next->i387.fxsave);

	/*
	 * Reload esp0.
	 */
	load_sp0(tss, next);

	[...]

	/*
	 * Now maybe handle debug registers and/or IO bitmaps
	 */
	if (unlikely((task_thread_info(next_p)->flags & _TIF_WORK_CTXSW)
	    || test_tsk_thread_flag(prev_p, TIF_IO_BITMAP)))
		__switch_to_xtra(next_p, tss);

	disable_tsc(prev_p, next_p);

	/* If the task has used fpu the last 5 timeslices, just do a full
	 * restore of the math state immediately to avoid the trap; the
	 * chances of needing FPU soon are obviously high now
	 */
	if (next_p->fpu_counter > 5)
		math_state_restore();

	[...]

	x86_write_percpu(current_task, next_p);

	return prev_p;
  }

* The last line "return prev_p" is very important. If you have noticed, switch_to
  macro was called using FASTCALL and not a normal C procedure call. This line is
  equivalent to the assembly code:
  
   mov  %edi, %eax
   ret

  The ret instruction loads %eip with the top value of the stack. from the
  __switch_to() extended assembly macro:

  "pushl %6\n\t"		/* restore EIP */	\
  [...]
  :"m" (next->thread.esp),"m" (next->thread.eip),	\

  Thus execution returns where "next" stopped, i.e. next->thread.eip

* From above code, if the new process was previously suspeneded, its 
  thread.eip will point to the label 1: in the switch_to() macro. which
  will restore the eflags and the %ebp state. After that, code path 
  normally continue with the function that called the switch_to() macro, 
  i.e. the context_switch() method.

Creating Processes:
-------------------

* The clone() glibc library call:

  GNU Libc code:
  --> sysdeps/unix/sysv/linux/i386/clone.S:
  clone(int (*fn)(void *arg), void *child_stack, int flags, void *arg,
	     pid_t *ptid, struct user_desc *tls, pid_t *ctid);

  --> sysdeps/i386/bp-asm.h
  /*
   * pointer sizes
   * Use of bounded pointers makes it possible for a compiler to generate 
   * code that tests if the pointer's value lies within the bounds prior to 
   * dereferencing the pointer or modifying the value of the pointer.
   */
  #if __BOUNDED_POINTERS__
  /* Bounded pointers occupy three words.  */
  #define PTR_SIZE 12
  #else
  /* Unbounded pointers occupy one word.  */
  #define PTR_SIZE 4 
  #endif

  /* Stack space overhead of procedure-call linkage: return address and
   * frame pointer.  */
  #define LINKAGE 8

  --> sysdeps/unix/sysv/linux/i386/clone.S:
  /*
   * Parameters offsets relative to the current stack (%esp)
   * +4: For ints
   * +PTR_SIZE: for pointers
   */
  #define PARMS	LINKAGE		/* no space for saved regs */
  #define FUNC	PARMS
  #define STACK	FUNC+4
  #define FLAGS	STACK+PTR_SIZE
  #define ARG	FLAGS+4
  #define PTID	ARG+PTR_SIZE
  #define TLS	PTID+PTR_SIZE
  #define CTID	TLS+PTR_SIZE

  /* The clone system call number */
  #define SYS_clone 120

  /*
   * Save the the function and its argument on the child stack
   */
  movl    STACK(%esp),%ecx
  /* Insert the argument onto the new stack.  Make sure the new
   * thread is started with an alignment of (mod 16). 
   * (This operation does no harm, since the passed address is the
   *  top, not the bottom of the stack) */
  andl	   $0xfffffff0, %ecx

  /* Make enough room in the stack */
  subl	   $28,%ecx			

  /* Store the argument in the stack */
  movl	   ARG(%esp),%eax		
  movl	   %eax,12(%ecx)

  /* Save the function pointer as the first argument.
     It will be popped off in the child in the ebx frobbing below.  */
  movl	   FUNC(%esp),%eax
  movl	   %eax,8(%ecx)

  /* Do the system call */
  pushl	   %ebx
  pushl	   %esi
  pushl	   %edi

  /* save remaining sys call paramerters. %ecx is already set 
   * +12 due to the three pushes above */
  movl	   FLAGS+12(%esp),%ebx
  movl	   PTID+12(%esp),%edx
  movl	   CTID+12(%esp),%edi
  movl	   TLS+12(%esp),%esi
  movl	   $SYS_ify(clone),%eax
  
  int	   $0x80

  /*
   * NOTE: Remember that from now, the child stack is different from
   * 	   the parent's one
   * Parent: those are the original values stored in the above 3 
   * 	     push statements.
   * Child: values are popped from the passed stack (FUNC(%esp)). 
   * 	    So %esi is 0 and %ebx holds `fn'
   *
   * IMPORTANT: The `arg' parameter follows `fn' in the child's stack,
   * 		So when the function get called, it pops its argument
   *		directly from the stack.
   */
  popl	   %edi
  popl	   %esi
  popl	   %ebx

  /* If system call returned 0 (child's thread), execute passed function
   * by jumping to thread_start. 
   * Otherwise, we are the parent, so return */
  test	   %eax,%eax
  jz	   L(thread_start)
L(pseudo_end):
  ret

L(thread_start):
  /* Note: %esi is zero. 
   * terminate the stack frame */
  movl	   %esi,%ebp	 
  
  call	   *%ebx

  /* Terminate the current child process by calling exit() giving
   * the return value of passed function as the exit signal */
  movl	   %eax, %ebx
  movl	   $SYS_ify(exit), %eax
  ENTER_KERNEL

  < Woooo, That was nice ! >

* Last point about glibc, is how to bypass the Glibc library call, which
  is done as follows (assuming calling getpid(2) syscall):

  #include <syscall.h>
  pid_t mypid;
  mypid = syscall(SYS_getpid);

* Clone Flags encountered later:

CLONE_VM		Shares memory descriptor and all page tables
CLONE_PTRACE		if traced, the parent wants the child to be traced too
CLONE_VFORK		Set when a vfork() system call is issued
CLONE_THREAD		Insert the child into the same thread group of the parent

--> arch/x86/kernel/process_32.c:
/*
 * when switching from user-mode to kernel mode stack, the general purpose
 * registers 
 * as if sys_clone(clone_flags, newsp, parent_tidptr, child_tidptr).
 */
asmlinkage int sys_clone(struct pt_regs regs)
{
	unsigned long clone_flags;
	unsigned long newsp;
	int __user *parent_tidptr, *child_tidptr;

	/* 
	 * Linux system calls number is passed in %eax
	 * Arguments are passed in order as %ebx, %ecx, %edx, %edi, ..
	 */
	clone_flags = regs.bx;
	newsp = regs.cx;

	/* tidptr: parent user space pointer to a variable that will hold child's
	   	   PID */
	parent_tidptr = (int __user *)regs.dx;
	child_tidptr = (int __user *)regs.di;

	/* From clone(2):
	 * the child_stack argument may be zero, in which case copy-on-write
	 * semantic ensure that the child gets separate copies of stack pages when
	 * either process modifies the stack.  In this case, for correct operation,
	 * the CLONE_VM option should not be specified.
	 * 
	 * If the child_stack argument is not zero, the invoking process (the
	 * parent) should always allocate a new stack for the child
	 */

	if (!newsp)
		/* Same stack pages */
		newsp = regs.sp;

	return do_fork(clone_flags, newsp, &regs, 0, parent_tidptr, child_tidptr);
}

* do_fork jobs:

  --> kernel/fork.c:

	long do_fork(unsigned long clone_flags,
	             unsigned long stack_start,
	      	     struct pt_regs *regs,
	      	     unsigned long stack_size,
	      	     int __user *parent_tidptr,
	      	     int __user *child_tidptr)
	{
	     struct task_struct *p;
	     int trace = 0;

  1- Checks the ptrace field of the parent, if not zero, parent process is traced
     by another process. 

	if (unlikely(current->ptrace)) {
		trace = fork_traceflag (clone_flags);
		if (trace)
			clone_flags |= CLONE_PTRACE;
	}

  2- invokes copy_process to make copy of the process descriptor. if all
     resources are available, method returns task_struct just created

	p = copy_process(clone_flags, stack_start, regs, stack_size, 
	    		 parent_tidptr, child_tidptr, nr);

  3- if CLONE_PARENT_SETTID is set, it copies the child's PID to the usermode
     variable addressed by parent_tipdir parameter.

     	#define CLONE_PARENT_SETTID	0x00100000	/* set the TID in the parent */

	if (clone_flags & CLONE_PARENT_SETTID)
		if (put_user(p->pid, parent_tidptr))
			goto bad_fork_cleanup_delays_binfmt;

  4- If CLONE_STOPPED flag is set or PT_PTRACED, it sets the state of the child
     as TASK_STOPPED and adds a SIGSTOP signal to it. It will reamins STOPPED
     till another process revert it to RUNNING using a SIGCONT signal. 
     This is done to allow the debugger to trace the new child process.

        if ((p->ptrace & PT_PTRACED) || (clone_flags & CLONE_STOPPED)) {
		/*
		 * We'll start up with an immediate SIGSTOP.
		 */
		sigaddset(&p->pending.signal, SIGSTOP);
		set_tsk_thread_flag(p, TIF_SIGPENDING);
	}

   5- if CLONE_STOPPED is not set, it invokes the wake_up_new_task() method which
   do:
   
      a- Adjust scheduling parameters of the child and parent
      b- If parent and child on the same CPU and they don't share the same page
      	 tables, it then forces the child to run before the parent by putting it
   	 before the parent in the runqueue (better performace using cp-on-write)
      c- otherwiese (not the same CPU or same pages), child is put at the end of
      	 the runqueue.
	 
      --> kerenl/sched.c:
      /*
       * wake_up_new_task - wake up a newly created task for the first time.
       *
       * This function will do some initial scheduler statistics housekeeping
       * that must be done for every newly created context, then puts the task
       * on the runqueue and wakes it.
       */
       
       Getting back to our do_fork() method:
	if (!(clone_flags & CLONE_STOPPED))
		wake_up_new_task(p, clone_flags);
	else
		p->state = TASK_STOPPED;

   6- If the parent process is traced, it stores the child's PID in the ptrace
      message and invokes a ptrace_notify() which stops the current process using
      ptrace_stop() and sends a SIGCHLD.
      SIGCHLD notifies the debugger that current has forked a child which PID can
      be retrieved in current->ptrace_message field.

	if (unlikely (trace)) {
		current->ptrace_message = nr;
		ptrace_notify ((trace << 8) | SIGTRAP);
	}

   7- If the CLONE_VFORK is set, it inserts the parent in a wait queue until the
      child releases its memory addresses space (terminates or executes a new
      program)

	if (clone_flags & CLONE_VFORK) {
		p->vfork_done = &vfork;
		init_completion(&vfork);
	}
	[...]
	if (clone_flags & CLONE_VFORK) {
		wait_for_completion(&vfork);
		if (unlikely (current->ptrace & PT_TRACE_VFORK_DONE)) {
			current->ptrace_message = nr;
			ptrace_notify ((PTRACE_EVENT_VFORK_DONE << 8) | SIGTRAP);
		}
	}

   8- Terminates by returning the PID of the child

        return nr;

* The copy_process() method creates a new process (task_struct) as a copy of the
  old one. It don't actually _run_ it yet. It takes parameters as do_fork() + the
  pid allocated from alloc_pid() in the do_fork() code.

  --> kernel/fork.c:

  /*
   * This creates a new process as a copy of the old one,
   * but does not actually start it yet.
   *
   * It copies the registers, and all the appropriate
   * parts of the process environment (as per the clone
   * flags). The actual kick-off is left to the caller.
   */
  static struct task_struct *copy_process(unsigned long clone_flags,
					unsigned long stack_start,
					struct pt_regs *regs,
					unsigned long stack_size,
					int __user *parent_tidptr,
					int __user *child_tidptr,
					int pid)
  {

  1- Checks whether the flags in clone_flags parameter are compatible

        /* Error if both CLONE_NEWNS and CLONE_FS are set */
  	if ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))
		return ERR_PTR(-EINVAL);

	/*
	 * Thread groups must share signals as well, and detached threads
	 * can only be started up within the thread group.
	 */
	if ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))
		return ERR_PTR(-EINVAL);

	/*
	 * Shared signal handlers imply shared VM. By way of the above,
	 * thread groups also imply shared VM. Blocking this case allows
	 * for various simplifications in other code.
	 */
	if ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))
		return ERR_PTR(-EINVAL);

   2- Performs any additional security checks (LSM hooks for security extensions)

	retval = security_task_create(clone_flags);
	if (retval)
		goto fork_out;

   3- Invokes dup_task_struct() which copies the parent task_struct and performs
      the following actions:
      a- __unlazy_fpu on the current process to save (if necessary) contents of
      FPU, MMX and SSE/SSE2 registers in the thread_info parent's structure.

            	struct task_struct *tsk;
		struct thread_info *ti;
		prepare_to_copy(orig);

      b- Executes alloc_task_struct() macro to get a process descriptor for the
      	 new process

	 	tsk = alloc_task_struct();
		if (!tsk)
		   return NULL;

     c- Executes alloc_thread_info() macro to get a free mem area to store
     	thread_info and the kernel mode stack

		ti = alloc_thread_info(tsk);
		if (!ti) {
		   free_task_struct(tsk);
		   return NULL;
		}

     d- copies content of parent process descriptor to the one pointed by
     	tsk. also set tsk->thread_info to its own just created stack:

		*tsk = *orig;
		tsk->thread_info = ti;
		setup_thread_stack(tsk, orig);

     e- copies the content of the current's thread_info descriptor then sets its
     	task element to tsk

	     	setup_thread_stack(tsk, orig);
		static inline void setup_thread_stack(struct task_struct *p, struct task_struct *org)
		{
			*task_thread_info(p) = *task_thread_info(org);
			task_thread_info(p)->task = p;
		}

     f- Sets the new process usage counter (tsk->usage) to 2. 2 spcifies that the
     	process descriptor is in use and that the corresponding process is alive
     	(not EXIT_ZOMBIE OR EXIT_DEAD). 
	
		/* One for us, one for whoever does the "release_task()" (usually parent) */
		atomic_set(&tsk->usage,2);


     g- return the created and initialized task_struct tsk.

	       return tsk;

  4- Increase the usage counter of the user_struct structure and the counter of the
     process owned by the user.
          	atomic_inc(&p->user->__count);
		atomic_inc(&p->user->processes);

  5- Check the number of processes in the system don't exceed the limit
     max_threads variable.
     The general rule that the space taken by all threads_info and kernel mode
     stacks don't exceed 1/8 of available physical memory.

     	if (nr_threads >= max_threads)
	    goto bad_fork_cleanup_count;
     
  6- .If the kernel functions implementing the execution domain and the
     executable format of the new process are included in kernel modules, it
     increases their usage counter

        if (!try_module_get(task_thread_info(p)->exec_domain->module))
		goto bad_fork_cleanup_count;

	if (p->binfmt && !try_module_get(p->binfmt->module))
	   	goto bad_fork_cleanup_putdomain

  7- Initialaize some fields related to the process state:
     	initializes the did_exec field to zero - indicates the number of execve()
	system calls issued by the process.
	Update some tsk->flags. clear the PF_SUPERPRIV flag and set the
	PF_FORKNOEXEC flag.

	--> include/linux/sched.h:
	#define PF_FORKNOEXEC	0x00000040	/* forked but didn't exec */
	#define PF_SUPERPRIV	0x00000100	/* used super-user privileges */
	#define PF_NOFREEZE	0x00008000	/* this thread should not be frozen */	

	static inline void copy_flags(unsigned long clone_flags, struct task_struct *p)
	{
		unsigned long new_flags = p->flags;

		new_flags &= ~(PF_SUPERPRIV | PF_NOFREEZE);
		new_flags |= PF_FORKNOEXEC;
		if (!(clone_flags & CLONE_PTRACE))
		      p->ptrace = 0;
		p->flags = new_flags;
	}

	p->did_exec = 0;
	copy_flags(clone_flags, p);

  8- initializes the list_head data structures and the spin locks in the child's
     process descriptor. setup other fields related to signals, timers and time
     statistics.

      	INIT_LIST_HEAD(&p->children);
	INIT_LIST_HEAD(&p->sibling);
	p->vfork_done = NULL;
	spin_lock_init(&p->alloc_lock);

	clear_tsk_thread_flag(p, TIF_SIGPENDING);
	init_sigpending(&p->pending);

	p->utime = cputime_zero;
	p->stime = cputime_zero;
 	p->sched_time = 0;

	task_io_accounting_init(p);
	acct_clear_integrals(p);

 	p->it_virt_expires = cputime_zero;
	p->it_prof_expires = cputime_zero;
 	p->it_sched_expires = 0;
 	INIT_LIST_HEAD(&p->cpu_timers[0]);
 	INIT_LIST_HEAD(&p->cpu_timers[1]);
 	INIT_LIST_HEAD(&p->cpu_timers[2]);

	p->lock_depth = -1;		/* -1 = no lock */
	do_posix_clock_monotonic_gettime(&p->start_time);
	p->security = NULL;
	p->io_context = NULL;
	p->io_wait = NULL;
	p->audit_context = NULL;
	cpuset_fork(p);

	p->tgid = p->pid;
	if (clone_flags & CLONE_THREAD)
		p->tgid = current->tgid;

  9- Invokes sched_fork() to complete the initialization of the scheduler data
     structure of the new process. 
     Functions sets the state of the process to TASK_RUNNING and disables kernel
     preemption. To keep process scheduling fair, function shares the remaining
     time slice of the parent between the parent and the child.

	/* Perform scheduler related setup. Assign this task to a CPU. */
	sched_fork(p, clone_flags);

      Some related lines from sched_fork():

      --> kernel/sched.c:

	/*
	 * We mark the process as running here, but have not actually
	 * inserted it onto the runqueue yet. This guarantees that
	 * nobody will actually run it, and a signal or other external
	 * event cannot wake it up and insert it on the runqueue either.
	 */
	p->state = TASK_RUNNING;      

        [...]

	/*
	 * Share the timeslice between parent and child, thus the
	 * total amount of pending timeslices in the system doesn't change,
	 * resulting in more scheduling fairness.
	 */
	local_irq_disable();
	p->time_slice = (current->time_slice + 1) >> 1;
	/*
	 * The remainder of the first timeslice might be recovered by
	 * the parent if the child exits early enough.
	 */
	p->first_time_slice = 1;
	current->time_slice >>= 1;
	p->timestamp = sched_clock();
	if (unlikely(!current->time_slice)) {
		/*
		 * This case is rare, it happens when the parent has only
		 * a single jiffy left from its timeslice. Taking the
		 * runqueue lock is not a problem.
		 */
		current->time_slice = 1;
		task_running_tick(cpu_rq(cpu), current);
	}
	local_irq_enable();


  10- invokes copy_semundo, copy_files, copy_fs, copy_sighand, copy_signal,
      ... unless specified differently by clone_flags parameter.

	/* copy all the process information */
	if ((retval = copy_semundo(clone_flags, p)))
		goto bad_fork_cleanup_audit;
	if ((retval = copy_files(clone_flags, p)))
		goto bad_fork_cleanup_semundo;
	if ((retval = copy_fs(clone_flags, p)))
		goto bad_fork_cleanup_files;
	if ((retval = copy_sighand(clone_flags, p)))
		goto bad_fork_cleanup_fs;
	if ((retval = copy_signal(clone_flags, p)))
		goto bad_fork_cleanup_sighand;
	if ((retval = copy_mm(clone_flags, p)))
		goto bad_fork_cleanup_signal;
	if ((retval = copy_keys(clone_flags, p)))
		goto bad_fork_cleanup_mm;
	if ((retval = copy_namespaces(clone_flags, p)))
		goto bad_fork_cleanup_keys;


  11- Invokes copy_thread() to initialize the kernel mode stack of the child with
      the values of the registers when the clone system call was issued (those
      values have been saved in the kernel mode stack of the parent process).

      	retval = copy_thread(0, clone_flags, stack_start, stack_size, p, regs);
	if (retval)
		goto bad_fork_cleanup_namespaces;


      so copy_thread jobs is:

      --> arch/x86/kernel/process_32.c:

      int copy_thread(int nr, unsigned long clone_flags, unsigned long esp,
	unsigned long unused,
	struct task_struct * p, struct pt_regs * regs)
	{
		struct pt_regs * childregs;
		struct task_struct *tsk;
		int err;

      a- initializes the kernel mode stack of the child with the values of the
      	 registers when the clone() system call was issued. 
	 *regs is passed from copy_process which is passed from do_fork wich is
      	 passed from sys_clone().

	 	childregs = task_pt_regs(p);
		*childregs = *regs;

		/*
		 * Note: eax represents the system call return value
		 * it's set to 0 as the convention of returning 0 to the child
		 * and 1 to the parent in the traditional fork() syscall
		 */
		childregs->eax = 0;
		childregs->esp = esp;

      b- the thread.esp field in the descriptor of the child process is
      	 initialized with the base address of the child's kernel mode stack. 
	 An address of an assembly function (ret_from_fork) is stored in
      	 thread.eip.

	 	p->thread.esp = (unsigned long) childregs;
		p->thread.esp0 = (unsigned long) (childregs+1);

		p->thread.eip = (unsigned long) ret_from_fork;


      c- If the parent process makes use of an I/O permission bitmap, the child
      	 gets a copy (I7na mabn7rmshy 7add min 7aga :D)

	 	tsk = current;
		if (unlikely(test_tsk_thread_flag(tsk, TIF_IO_BITMAP))) {
		   p->thread.io_bitmap_ptr = kmemdup(tsk->thread.io_bitmap_ptr,
					   		IO_BITMAP_BYTES, GFP_KERNEL);
		if (!p->thread.io_bitmap_ptr) {
			p->thread.io_bitmap_max = 0;
			return -ENOMEM;
		}
		set_tsk_thread_flag(p, TIF_IO_BITMAP);
		}

      d- Finally, if the CLONE_SETTLS flag is set, child gets the TLS segment
      	 specified by the user mode data structure pointed by the tls parameter
      	 of clone. 


  12- If CLONE_CHILD_SETTID or CLONE_CHILD_CLERTID is set, ..

      	p->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;
	/*
	 * Clear TID on mm_release()?
	 */
	p->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? child_tidptr: NULL;

  13- Turns TIF_SYSCALL_TRACE flag in thread_info of the child so that
      ret_from_fork() function will not notify the debugging process about the
      system call termination. 

	/*
	 * Syscall tracing should be turned off in the child regardless
	 * of CLONE_PTRACE.
	 */
	clear_tsk_thread_flag(p, TIF_SYSCALL_TRACE);

  14- Initialize the exit_signal element with the signal element encoded in
      clone_flags. exit_signal is the signal we'll sent to our parent when we
      die, exit_signal = -1 (invalid signal) means we don't want to notify our
      parent. If CLONE_THREAD is set, set it to -1.

      From the man page:

      `` A new thread created with CLONE_THREAD has the same parent  pro‐
      cess as the caller of clone() (i.e., like CLONE_PARENT), so that
      calls to getppid(2) return the same value for all of the threads
      in  a  thread group.  When a CLONE_THREAD thread terminates, the
      thread that created it using clone() is not sent a  SIGCHLD  (or
      other  termination)  signal; nor can the status of such a thread
      be obtained using wait(2).  (The thread is said to be detached.)

      After all of the threads in a thread group terminate the parent 
      process of  the  thread  group  is  sent  a SIGCHLD (or other 
      termination) signal. ''

	/* ok, now we should be set up.. */
	p->exit_signal = (clone_flags & CLONE_THREAD) ? -1 : (clone_flags & CSIGNAL);
	p->pdeath_signal = 0;
	p->exit_state = 0;

	/*
	 * Ok, make it visible to the rest of the system.
	 * We dont wake it up yet.
	 */
	p->group_leader = p;
	INIT_LIST_HEAD(&p->thread_group);
	INIT_LIST_HEAD(&p->ptrace_children);
	INIT_LIST_HEAD(&p->ptrace_list);

  14- Allocate a new PID:

  	if (pid != &init_struct_pid) {
		retval = -ENOMEM;
		pid = alloc_pid(task_active_pid_ns(p));
		if (!pid)
			goto bad_fork_cleanup_io;

		if (clone_flags & CLONE_NEWPID) {
			retval = pid_ns_prepare_proc(task_active_pid_ns(p));
			if (retval < 0)
				goto bad_fork_free_pid;
		}
	}

	p->pid = pid_nr(pid)
	p->tgid = p->pid;
	if (clone_flags & CLONE_THREAD)
		p->tgid = current->tgid;

	p->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? child_tidptr : NULL;


  15- Sets the cpu field in the thread info structure to the number of the local
      CPU returned by smp_processor_id()

      	/*
	 * The task hasn't been attached yet, so its cpus_allowed mask will
	 * not be changed, nor will its assigned CPU.
	 *
	 * The cpus_allowed mask of the parent may have changed after it was
	 * copied first time - so re-copy it here, then check the child's CPU
	 * to ensure it is on a valid CPU (and if not, just force it back to
	 * parent's CPU). This avoids alot of nasty races.
	 */
	p->cpus_allowed = current->cpus_allowed;
	if (unlikely(!cpu_isset(task_cpu(p), p->cpus_allowed) ||
			!cpu_online(task_cpu(p))))
		set_task_cpu(p, smp_processor_id());


  16- Initializes fields that specifies parenthood relationships:

  	/* CLONE_PARENT re-uses the old parent */
	if (clone_flags & (CLONE_PARENT|CLONE_THREAD))
		p->real_parent = current->real_parent;
	else
		p->real_parent = current;
	p->parent = p->real_parent;

  [...]

  17- Invokes attach_pid() to insert the PID of the new process descriptor in the
      pid_hash[PIDTYPE_PID] hash table.

      	if (likely(p->pid)) {
		add_parent(p);
		if (unlikely(p->ptrace & PT_PTRACED))
			__ptrace_link(p, current->parent);

		if (thread_group_leader(p)) {
			p->signal->tty = current->signal->tty;
			p->signal->pgrp = process_group(current);
			set_signal_session(p->signal, process_session(current));
			attach_pid(p, PIDTYPE_PGID, process_group(p));
			attach_pid(p, PIDTYPE_SID, process_session(p));

			list_add_tail_rcu(&p->tasks, &init_task.tasks);
			__get_cpu_var(process_counts)++;
		}
		attach_pid(p, PIDTYPE_PID, p->pid);

		/*
		 * A new process has now been added to the set of processes. 
		 * Increase the number of threads in the system
		 */
		nr_threads++;
	}

  18- Increase the total_forks variable to keep track of the number of forked
      processes. 

 	total_forks++;

  19- Terminates by returning the child's process descriptor.


* After the do_fork() terminates, We have a complete child process in the
  runnable state But it _isn't_ actually running. It's up to the scheduler to
  decide when to give the CPU to the child. 


Kernel Threads:
---------------

* Unix systems traditionally delegate some critical tasks intermittently running
  processes including flushing disk caches, swapping out unused pages, servicing
  network connections and so on.

* Because some of the processes runs only in kernel mode, linux puts them in
  kernel threads which differs from normal processes in the following:
  1- kernel threads runs only in kernel mode
  2- Cause they are all in kernel mode, they use addresses > PAGE_OFFSET only.

* A kernel thread is created using the kernel_thread method. It just pass the
  the function where the thread will execute with help by kernel_thread_helper
  completely in kernel-mode.

  --> arch/x86/kernel/process_32.c:

  /*
   * Create a kernel thread
   */
   int kernel_thread(int (*fn)(void *), void * arg, unsigned long flags)
   {
	struct pt_regs regs;

	memset(&regs, 0, sizeof(regs));

	regs.ebx = (unsigned long) fn;
	regs.edx = (unsigned long) arg;

	regs.xds = __USER_DS;
	regs.xes = __USER_DS;
	regs.xfs = __KERNEL_PDA;
	regs.orig_eax = -1;
	regs.eip = (unsigned long) kernel_thread_helper;
	regs.xcs = __KERNEL_CS | get_kernel_rpl();
	regs.eflags = X86_EFLAGS_IF | X86_EFLAGS_SF | X86_EFLAGS_PF | 0x2;

	/* Ok, create the new process..
	 * Don't copy the userspace VM since we won't touch it anyway */
	return do_fork(flags | CLONE_VM | CLONE_UNTRACED, 0, &regs, 0, NULL, NULL);
   }
EXPORT_SYMBOL(kernel_thread);

   --> arch/x86/kernel/entry_32.S:

   /* 
    * ebx and edx points to fn and arg
    */
   ENTRY(kernel_thread_helper)
	pushl $0		# fake return address for unwinder
	movl %edx,%eax
	/* push the arg for the fn function */
	push %edx
	call *%ebx
	/* push fn return value for the _exit system call */
	push %eax
	call do_exit
   ENDPROC(kernel_thread_helper)

Processes 0 (idle/swapper) and 1(init):
---------------------------------------

* The idle/swapper process and the ansector of all processes. Its data structures
  is statically initialized. The most important data structure there is The
  master kernel page Global Directory.

* The swapper process descriptor is named init_task. Its values are initialized
  by the INIT_TASK macro and others found in include/linux/init_task.h.
  
  --> include/linux/init_task.h:
  [...]
  #define INIT_MM(name) \
  {			 					\
	.mm_rb		= RB_ROOT,				\
	/** Remember the previous discussin about swapper_pg_dir ? */
	.pgd		= swapper_pg_dir, 			\
	.mm_users	= ATOMIC_INIT(2), 			\
	.mm_count	= ATOMIC_INIT(1), 			\
	.mmap_sem	= __RWSEM_INITIALIZER(name.mmap_sem),	\
	.page_table_lock =  __SPIN_LOCK_UNLOCKED(name.page_table_lock),	\
	.mmlist		= LIST_HEAD_INIT(name.mmlist),		\
	.cpu_vm_mask	= CPU_MASK_ALL,				\
  }
  [...]

* the start_kernel() method initializes all data structures needed by the
  kernel, enables interrupts and creates another kernel thread, process 1 
  i.e. init as follows:

  --> init/main.c:

  [some lengthy and easy initialization code (interrupts, rcu, timers, ...) ]

  /*
   * We need to finalize in a non-__init function or else race conditions
   * between the root thread and the init thread may cause start_kernel to
   * be reaped by free_initmem before the root thread has proceeded to
   * cpu_idle.
   *
   * gcc-3.4 accidentally inlines this function, so use noinline.
   */

   static void noinline rest_init(void)
	  __releases(kernel_lock)
   {
	/** Let the tread execute the kernel_init function */
	kernel_thread(kernel_init, NULL, CLONE_FS | CLONE_SIGHAND);

	numa_default_policy();
	unlock_kernel();
	/*
	 * The boot idle thread must execute schedule()
	 * at least one to get things moving:
	 */
	preempt_enable_no_resched();
	schedule();
	preempt_disable();
	/* Call into cpu_idle with preempt disabled */
	cpu_idle();
   } 

* Process 1, created by the idle process executes the init() function which in
  turn completes the initialization of the kernel. The init stays alive till the
  system shuts down, because it creates and monitors the activity of all
  processes in the system.

Destroying Processes:
---------------------

* The usual way of a process to terminate is to invoke the exit() _library_
  function. The C compiler always inserts an exit() function call right after the
  last statement of main().

* There are two system calls to terminate a user mode application:
  exit_group() which terminates a whole multithreaded application. invoked by the
  	       GlibC.
  _exit() which terminates only a single process, invoked by pthread_exit.

* exit_group():
  
  --> kernel/exit.c:

  /*
   * this kills every thread in the thread group. Note that any externally
   * wait4()-ing process will get the correct exit code - even if this
   * thread is not the thread group leader.
   */
   asmlinkage void sys_exit_group(int error_code)
   {
	do_group_exit((error_code & 0xff) << 8);
   }

   
* do_group_exit():

   kills each process belonging to the thread group of current. It recieves the
   process termination code, which is either a value specified in
   sys_exit_group() (normal termination) or error code by the kernel.

   --> kernel/exit.c:
   /*
    * Take down every thread in the group.  This is called by fatal signals
    * as well as by sys_exit_group (below).
    */
    NORET_TYPE void
    do_group_exit(int exit_code)
    {
	BUG_ON(exit_code & 0x80); /* core dumps don't get here */

  1- Checks if SIGNAL_GROUP_EXIT is set. 

   --> include/linux/sched.h:

   /*
    * Bits in flags field of signal_struct.
    */
    #define SIGNAL_STOP_STOPPED	  0x00000001 /* job control stop in effect */
    #define SIGNAL_STOP_DEQUEUED  0x00000002 /* stop signal dequeued */
    #define SIGNAL_STOP_CONTINUED 0x00000004 /* SIGCONT since WCONTINUED reap */
    #define SIGNAL_GROUP_EXIT	  0x00000008 /* group exit in progress */


   2- If set (which means that the kernel already started an exit procedure) it
      considers the current->signal->group_exit_code the exit code and invokes
      the do_exit() method. 

      if (current->signal->flags & SIGNAL_GROUP_EXIT)
	 	exit_code = current->signal->group_exit_code;
    
     
   3- If it's not set, it stores the termination code in
      current->signal->group_exit_code. then it calls zap_other_threads().
      

      else if (!thread_group_empty(current)) {
		struct signal_struct *const sig = current->signal;
		struct sighand_struct *const sighand = current->sighand;

		/* Avoid RACE CONDITIONS as said in the NOTE below */
		spin_lock_irq(&sighand->siglock);

		/* Equivalent to: if (sig->flags & SIGNAL_GROUP_EXIT) */
		if (signal_group_exit(sig))	
			/* Another thread got here before we took the lock.*/
			exit_code = sig->group_exit_code;
		else {
			sig->group_exit_code = exit_code;
			sig->flags = SIGNAL_GROUP_EXIT;
			/* Called with sighand->siglock held */
			zap_other_threads(current);
		}
		spin_unlock_irq(&sighand->siglock);
	}

      zap_other_threads sets the SIGNAL_GROUP_EXIT flag. It then sends a SIGKILL
      signal to all other threads in `current' threads group by scanning the
      `thread_group' list:

      --> kernel/signal.c:

      /*
       * Nuke all other threads in the group.
       */
       void zap_other_threads(struct task_struct *p)
       {
		struct task_struct *t;

		p->signal->group_stop_count = 0;

		for (t = next_thread(p); t != p; t = next_thread(t)) {
		    /*
		     * Don't bother with already dead threads (exit_state != 0)
		     */		    
		    if (t->exit_state)
			continue;

		    /*
		     * We don't want to notify the parent, since we are
		     * killed as part of a thread group due to another
		     * thread doing an execve() or similar. So set the
		     * exit signal to -1 to allow immediate reaping of
		     * the process.  But don't detach the thread group
		     * leader.
		     *
		     * i.e: only notify the parent for the kill of the group
		     * leader, not every thread
		     */
		     if (t != p->group_leader)
			   t->exit_signal = -1;

		     /* SIGKILL will be handled before any pending SIGSTOP */
		     sigaddset(&t->pending.signal, SIGKILL);
		     signal_wake_up(t, 1);
		}
       }

       --> include/linux/sched.h:

       static inline struct task_struct *next_thread(const struct task_struct *p)
       {
	return list_entry(rcu_dereference(p->thread_group.next),
			  struct task_struct, thread_group);
       }
       
   4- Invokes the do_exit() at the end

            do_exit(exit_code);

 
* All process terminations are handled by the do_exit() method which removes most
  references of the terminating process from the kernel data structures. 

  --> kernel/exit.c:

  fastcall NORET_TYPE void do_exit(long code)
  {
	struct task_struct *tsk = current;
	int group_dead;

	/* Warn if the process is holding fs execlusive resources */
	WARN_ON(atomic_read(&tsk->fs_excl));

	if (unlikely(in_interrupt()))
		panic("Aiee, killing interrupt handler!");
	if (unlikely(!tsk->pid))
		panic("Attempted to kill the idle task!");

	/* We are init ? ;) */
	if (unlikely(tsk == child_reaper(tsk))) {
		if (tsk->nsproxy->pid_ns != &init_pid_ns)
			tsk->nsproxy->pid_ns->child_reaper = init_pid_ns.child_reaper;
		else
			panic("Attempted to kill init!");
	}

	if (unlikely(current->ptrace & PT_TRACE_EXIT)) {
		current->ptrace_message = code;
		ptrace_notify((PTRACE_EVENT_EXIT << 8) | SIGTRAP);
	}

  2- Sets the PF_EXITING flag in process descriptor flag field to indicate
     process is being eleminated

     	/*
	 * We're taking recursive faults here in do_exit. Safest is to just
	 * leave this task alone and wait for reboot.
	 */
	if (unlikely(tsk->flags & PF_EXITING)) {
		printk(KERN_ALERT
			"Fixing recursive fault but reboot is needed!\n");
		if (tsk->io_context)
			exit_io_context();
		set_current_state(TASK_UNINTERRUPTIBLE);
		schedule();
	}

	tsk->flags |= PF_EXITING;

  3- Detatches from the process descriptor other data structures related to
     paging, semaphores, file system, open FS descriptors, namespaces and I/O
     permission bitmap. 
     
     	if (unlikely(in_atomic()))
		printk(KERN_INFO "note: %s[%d] exited with preempt_count %d\n",
				current->comm, current->pid,
				preempt_count());
        [...]				
	taskstats_exit(tsk, group_dead);

	exit_mm(tsk);

	if (group_dead)
		acct_process();
	exit_sem(tsk);
	__exit_files(tsk);
	__exit_fs(tsk);
	exit_thread();
	cpuset_exit(tsk);
	exit_keys(tsk);

  4- If the kernel functions implementing the execution domain and the executable
     format of the process being killed are included in kernel modules, the
     function decreases their usage counters.

     	module_put(task_thread_info(tsk)->exec_domain->module);
	if (tsk->binfmt)
		module_put(tsk->binfmt->module);


  5- Sets the exit_code field of process descriptor to the process termination
     code. It's the one passed from _exit() or exit_group() system calls. 

     	tsk->exit_code = code;

  6- Invokes exit_notify() which notifies other processes related to us that we
     are goin to die.

     /*
      * Send signals to all our closest relatives so that they know
      * to properly mourn us..
      */
      static void exit_notify(struct task_struct *tsk)
      {
	int state;
	struct task_struct *t;
	struct list_head ptrace_dead, *_p, *_n;
	struct pid *pgrp;

     a- Updates the parenthood relationships of both the parent process and the
     	child process. Children become children of another process in the same
     	thread group or otherwise of init process.

	/*
	 * This does two things:
	 *
  	 * A.  Make init inherit all the child processes
	 * B.  Check to see if any process groups have become orphaned
	 *	as a result of our exiting, and if they have any stopped
	 *	jobs, send them a SIGHUP and then a SIGCONT.  (POSIX 3.2.2.2)
	 */
	forget_original_parent(tsk);
	exit_task_namespaces(tsk);

	write_lock_irq(&tasklist_lock);
	/*
	 * Check to see if any process groups have become orphaned
	 * as a result of our exiting, and if they have any stopped
	 * jobs, send them a SIGHUP and then a SIGCONT.  (POSIX 3.2.2.2)
	 *
	 * Case i: Our father is in a different pgrp than we are
	 * and we were the only connection outside, so our pgrp
	 * is about to become orphaned.
	 */
	t = tsk->real_parent;


     b- If the exit_signal field is a valid signal (not equal to -1) and
     	the process is the last member of its thread group and It's not being
     	traced, signal_exit signal is sent. Otherwise, . In this case,
     	function sends a signal SIGCHLD to the parent to notify it about our
     	death.

	In otherwords, notify the parent only if this is the last process in
	group and the parent _do want_ to be signalled. Or simply cause we are
	traced. (** If the parent process intialized the child process with
	exist_signal = -1, then it does NOT want to be signalled by the exit of
	that process **).

	
	/* If something other than our normal parent is ptracing us, then
	 * send it a SIGCHLD instead of honoring exit_signal.  exit_signal
	 * only has special meaning to our real parent.
	 */
	if (tsk->exit_signal != -1 && thread_group_empty(tsk)) {
		int signal = tsk->parent == tsk->real_parent ? tsk->exit_signal : SIGCHLD;
		do_notify_parent(tsk, signal);
	} else if (tsk->ptrace) {
		do_notify_parent(tsk, SIGCHLD);
	}

     c- If exit_signal is -1 and the process is not traced or the parent is going
     	to die very soon, sets the exit field to EXIT_DEAD. otherwise, set it to EXIT_ZOMBIE.

	Explanation: If the exist_signal = -1, then the parent don't care about
	the child exit. i.e., it does not need to know its exit_value or any
	other data, so release it. If the father don't care about child's exist,
	no body does.

	state = EXIT_ZOMBIE;
	if (tsk->exit_signal == -1 &&
	    (likely(tsk->ptrace == 0) ||
	     unlikely(tsk->parent->signal->flags & SIGNAL_GROUP_EXIT)))
		state = EXIT_DEAD;
	tsk->exit_state = state;

	Note: In 2.6.22 and bove, kernel devs removed the last SIGNAL_GROUP_EXIT
	check, Here's the explanation:

	" the "->parent->signal->flags & SIGNAL_GROUP_EXIT" check in
	exit_notify() is not right. SIGNAL_GROUP_EXIT can mean exec(), not
	exit_group(). This means ptracer can lose a ptraced zombie on
	exec(). Minor problem, but still the bug. "

     d- If the exit state field set by previous code is EXIT_DEAD, its released
     	by release_task() as follows:

	/* If the process is dead, release it - nobody will wait for it */
	if (state == EXIT_DEAD)
		release_task(tsk);

  7- Invoke a scedule function to select a new process to run cause a process in
     EXIT_ZOMBIE state is ignored by the scheduler.

     	schedule();
	BUG();
	/* Avoid "noreturn function does return".  */
	for (;;)
		cpu_relax();	/* For when BUG is null */
}

* As you noted in above code, release_task() is only invoked if the process is in
  EXIT_DEAD only and not in EXIT_ZOMBIE. This is a Unix design. A process may
  create a child to perform a specific task and then invoke some wait()-like
  library function to check if the child has terminated. If it'd terminated, it
  checks the return code to know if the task is completed successfuly.

  If the process is EXIT_ZOMBIE, it'll be released by a sys_wait4 call

* To comply with above design, Unix kernels are not allowed to delete process
  descriptors after they terminate. They're only allowed to do so if the parent
  issued a wait() function for them. That's the reason of EXIT_ZOMBIE. 
  
  Note: init always release ZOMBIE processes by waiting() for them. That's the
  	way system get out of zombie processes if the parent is dead (handling
  	them to init to killing them).

* release_task() detatches the last data structures from the descriptor. It's
  applied to zombie processes only by do_exit if:
  1- Parent isn't interested (exit_signal = -1) (in do_exit step 6.d)
  2- wait4() or waitpid() syscall from the parent after a signal been sent.

  Function do the following:

  void release_task(struct task_struct * p)
  {
	struct task_struct *leader;
	int zap_leader;
repeat:

  1- Decrease the number of processes belonging to the user owner of the
     terminated process. (Increased in copy_process()).
     
	atomic_dec(&p->user->processes);

  2- If the process is being traced, it removes it from the debugger's
     `ptrace_child' and reassigns it to its original parent.

	write_lock_irq(&tasklist_lock);
     	ptrace_unlink(p);
	BUG_ON(!list_empty(&p->ptrace_list) || !list_empty(&p->ptrace_children));

	--> include/linux/ptrace.h:

	static inline void ptrace_unlink(struct task_struct *child)
	{
		if (unlikely(child->ptrace))
		   __ptrace_unlink(child);
	}


	--> kernel/ptrace.c:

	/*
	 * unptrace a task: move it back to its original parent and
 	 * remove it from the ptrace list.
 	 *
 	 * Must be called with the tasklist lock write-held.
 	 */
	 void __ptrace_unlink(struct task_struct *child)
	 {
		BUG_ON(!child->ptrace);

		child->ptrace = 0;
		if (!list_empty(&child->ptrace_list)) {
		   list_del_init(&child->ptrace_list);
		   remove_parent(child);
		   child->parent = child->real_parent;
		   add_parent(child);
		}

		if (child->state == TASK_TRACED)
		   ptrace_untrace(child);
	 }

  3,4- Invokes __exit_signal() to cancel any pending signal and release the
     signal_struct descriptor. __exit_signal along with other things also invokes
     __cleanup_sighand to get rid of signal handlers.

	 __exit_signal(p);

	 static void __exit_signal(struct task_struct *tsk)
	 {
		struct signal_struct *sig = tsk->signal;
		struct sighand_struct *sighand;

	 [...]

	__unhash_process(tsk);

	tsk->signal = NULL;
	tsk->sighand = NULL;
	spin_unlock(&sighand->siglock);
	rcu_read_unlock();

	__cleanup_sighand(sighand);
	clear_tsk_thread_flag(tsk,TIF_SIGPENDING);
	flush_sigqueue(&tsk->pending);
	if (sig) {
		flush_sigqueue(&sig->shared_pending);
		taskstats_tgid_free(sig);
		__cleanup_signal(sig);
	}

  5- __unhash_process called above :
     
     static void __unhash_process(struct task_struct *p)
     {
	nr_threads--;
	/* Detatch it from PIDTYPE_PID hash table */
	detach_pid(p, PIDTYPE_PID);
	if (thread_group_leader(p)) {
		detach_pid(p, PIDTYPE_PGID);
		detach_pid(p, PIDTYPE_SID);

		list_del_rcu(&p->tasks);
		__get_cpu_var(process_counts)--;
	}
	list_del_rcu(&p->thread_group);
	remove_parent(p);
      }

  6- If the process is not a thread group leader, the leader is a zombie and the
     process is the last member of the thread group, function sends a signal to
     the parent of the leader to notify it about its death.

     /*
      * If we are the last non-leader member of the thread
      * group, and the leader is zombie, then notify the
      * group leader's parent process. (if it wants notification.)
      */
      zap_leader = 0;
      leader = p->group_leader;
      if (leader != p && thread_group_empty(leader) && leader->exit_state == EXIT_ZOMBIE) {
		BUG_ON(leader->exit_signal == -1);
		do_notify_parent(leader, leader->exit_signal);
		/*
		 * If we were the last child thread and the leader has
		 * exited already, and the leader's parent ignores SIGCHLD,
		 * then we are the one who should release the leader.
		 *
		 * do_notify_parent() will have marked it self-reaping in
		 * that case.
		 */
		zap_leader = (leader->exit_signal == -1);
	}

   7- Invoke exit_schedule() to adjust the time slice of the parent process.
   
	sched_exit(p);
	write_unlock_irq(&tasklist_lock);
	proc_flush_task(p);
	release_thread(p);

   8- Invokes put_task_struct() to decrease the process descriptor usage
      counter. 

	call_rcu(&p->rcu, delayed_put_task_struct);

	p = leader;
	if (unlikely(zap_leader))
		goto repeat;
      }


********************************************************************************
Interrupts:
***********

* Interrupt signals provide a way for the processor to code outside the normal
  flow of control. Code executed by an interrupt handling or by an exception is
  not a process, it is a kernel control path that runs at the expense of the same
  process that was running. Interrupt handling is one of the most sensitive tasks
  run by the kernel.

* Exceptions are divided to:
  1- Processor detected:
     a- Faults: can be corrected and cause the restart of the instruction.
     b- Traps: reported following the execution of the trapped instruction. Its
     	       main use is debugging purposes.
     c- Aborts: Used for severe errors, handler of such interrupt has no choice
     		but to force the affected process to terminate.

* Each hardware device controller capable of issuing interrupt requests usually
  has a single output line called IRQ (Interuupt ReQuest Line). All IRQs are
  connected by `input pins' to the Programmable Interrupt Controller. 

* Each interrupt or exception identified by a number ranged from 0 to 255. Intel
  calls this 8bit unsigned number a vector.

* The interrupt controller’s goal is to provide interrupt capabilities to the
  main processor (CPU) through a single line. Intels default vector associated
  with IRQ n is n+32. Mapping between IRQs and vectors can be modified by issuing
  suitable I/O instructions to PIC ports. 

* IMPORTANT:  Disabled interrupts are not lost; the PIC sends them to the CPU as
  soon as they are enabled again. This feature is used by most interrupt
  handlers, because it allows them to process IRQs of the same type serially (by
  masking the IRQ value while its handler is executing).

* Traditional PICs are implemented by connecting "in cascade" two 8259A-style
  external chips.
  PIC job is to:
  1- Monitor IRQ lines, checking for raised signals. If two or more IRQ lines are
     raised, selects the one with the lower PIN number.
  2- Convert the raised signal to a corresponding vector.
  3- Sends a raised signal to processor INTR pin. [ Issues an interrupt ].
  4- Waits till the CPU acknowledges the interrupt by writing to one of PIC I/O
     ports. When this occurs, it clears the INTR pin.

* In APIC (Advanced Programmable Interrupt Controllers), Interrupt signals coming
  from external hardware devices can be distributed among available CPUs in two
  ways:
  1- Static Distribution: Delivered to local APICs listed in the corresponding
     	    		  Redirection Table entry. Delivered to one CPU, subset
  			  CPUs or to all CPUs (broadcast).

  2- Dynamic Distribution: Delivered to the local APIC that's executing a process
     	     		   with the lowest priority (read below point).

* Every local APIC has a Task Priority register (TPR) which is used to compute
  the priority of the currently running process. Intel expects this register to
  be modified in each process switch.

* Multi APIC system is also used by processors to generate interrupts to another
  processors. IPI (Interprocessor interrupts) are cruical component of SMP
  architecture. They're used by Linux to exchange messages among CPUs.

* Interrupt Descriptor Table (IDT) associates each interrupt or exception vector
  with the address of corresponding interrupt or exception handler. It must be
  properly intialized before the kernel enables interrupts.

* Three types of descriptros exist, two are mainly used while Task gate is only
  used in serving double faults:
  Interrupt gate: includes the segment selector and the offset inside the segment
  	    	  of an interrupt or exception handler. clears the IF flag
  Trap gate: similar to interrupt gate except while transfering control to the
       	     proper segment, processor does not modify the IF flag.


Hardware handling of interrupts and exceptions:
-----------------------------------------------

* After finishing an instruction, %cs and %eip registers contain the logical address
  of the next instruction. Before executing this instruction, processor checks
  wheter an interrupt occured, If one occured, control does the following:
  1- Determines the vector (0 <= i <= 255) associated with the interrupt.
  2- Reads the ith entry of IDT using %idtr. 
  3- Gets GDT base address from %gdtr, reads the Segment Descriptor identified by
     the IDT ith entry


                IDT                                    EXECUTABLE SEGMENT
           +---------------+                             +---------------+
           |               |                       OFFSET|               |
           |---------------|  +------------------------->| ENTRY POINT   |
           |               |  |         GDT              |               |
           |---------------|  |   +---------------+      |               |
           |               |  |   |               |      |               |
INTERRUPT  |---------------|  |   |---------------|      |               |
   ID----->| TRAP GATE OR  |--+   |               |      |               |
           |INTERRUPT GATE |--+   |---------------|      |               |
           |---------------|  |   |               |      |               |
           |               |  |   |---------------|      |               |
           |---------------|  +-->|   SEGMENT     |-+    |               |
           |               |      |  DESCRIPTOR   | |    |               |
           |---------------|      |---------------| |    |               |
           |               |      |               | |    |               |
           |---------------|      |---------------| |    |               |
           |               |      |               | |BASE|               |
           +---------------+      |---------------| +--->+---------------+
                                  |               |
                                  |               |
                                  |               |
                                  +---------------+

  4- Checks authorization. compares CPL (stored in %cs) with GDT's entry DPL
     field. will complete iff CPL >= DPL otherwise issuing a "General Protection"
     Exception. 

  5- Checks if CPL != DPL , If so CU (Control Unit) must use the stack associated
     with the new privilege level. it does so by:
     a- reads %tr to access TSS segment of running process.
     b- loads %ss, %esp with proper values (from TSS) for the new privilege level.
     c- In the new stack, it saves old %ss and %esp values. 

     NOTE: I don't think Linux passes above situation since it only works under
     	   Ring 0 and Ring 3.

  6- If a fault occured, loads %cs and %eip with logical addresses of the
     instruction that caused the exception to be reexecuted. 

  -->IMPORTANT NOTE: It does not actually load them in the registers themsevles,
     	       	     instead it puts this values in the stack to be loaded _after_
		     finishing the interrupt.

  7- Saves %eflags, %cs and %eip in the stack.
  8- If the exception carries a hardware code, save it in the stack.

                           WITHOUT PRIVILEGE TRANSITION

      D  O      31          0                     31          0
      I  F    |---------------|                 |---------------|
      R       |       |       |    OLD          |       |       |    OLD
      E  E    |-------+-------|   SS:ESP        |-------+-------|   SS:ESP
      C  X    |       |       |     |           |       |       |     |
      T  P    |---------------|<----+           |---------------|<----+
      I  A    |  OLD EFLAGS   |                 |  OLD EFLAGS   |
      O  N    |---------------|                 |---------------|
      N  S    |       |OLD CS |    NEW          |       |OLD CS |
         I    |---------------|   SS:ESP        |---------------|
       | O    |    OLD EIP    |     |           |    OLD EIP    |    NEW
       | N    |---------------|<----+           |---------------|   SS:ESP
       |      |               |                 |  ERROR CODE   |     |
       v      .               .                 |---------------|<----+
              .               .                 |               |
              .               .
              WITHOUT ERROR CODE                 WITH ERROR CODE

                             WITH PRIVILEGE TRANSITION

      D  O     31            0                     31          0
      I  F    +---------------+<----+           +---------------+<----+
      R       |       |OLD SS |     |           |       |OLD SS |     |
      E  E    |---------------|   SS:ESP        |---------------|   SS:ESP
      C  X    |    OLD ESP    |  FROM TSS       |    OLD ESP    |  FROM TSS
      T  P    |---------------|                 |---------------|
      I  A    |  OLD EFLAGS   |                 |  OLD EFLAGS   |
      O  N    |---------------|                 |---------------|
      N  S    |       |OLD CS |    NEW          |       |OLD CS |
         I    |---------------|   SS:EIP        |---------------|
       | O    |    OLD EIP    |     |           |    OLD EIP    |    NEW
       | N    |---------------|<----+           |---------------|   SS:ESP
       |      |               |                 |  ERROR CODE   |     |
       v      .               .                 |---------------|<----+
              .               .                 |               |
              .               .
              WITHOUT ERROR CODE                 WITH ERROR CODE


  9- Loads cs and eip, respectively, with the Segment Selector and the Offset
     fields of the Gate Descriptor stored in the i th entry of the ID. From now
     begins the execution of the interrupt handler (See first graph). 

* After finishing the interrupt, a `iret' instruction is issued which do exactly
  reverse of the above steps happens as follows:
  
  1- Loads %cs, %eip, %flags with values saved in stack.
  2- If current CPL is eqaul to the last two %cs bits, resumes old execution,
     otherwise go to 3
  3- Load %ss, %esp with values saved in stack to return the same old privilege
     level state.
  4- Security: Clear contents of %ds, %es, %fs and %gs segment registers if they
     have a DPL < CPL. Done to prevent malicious programs from accessing kernel
     space.

--------------------------------

* Assuming the kernel is bug free, most exceptions can only occur while the CPU
  is in user mode. However the "Page Fault" exception may occur in kernel
  mode. The kernel may suspend the current process and replace it with another
  one until the requested page is available. 
  
* Becaue the Page Fault handler never gives rise to further exceptions, at most
  two kernel control paths associated with exceptions maybe stacked. One caused
  by the system call, the other by the Page Fault. 

* An interrupt handler may premept both other interrupt handlers and exception
  handlers. An exception handler never premepts an interrupt handler since
  interrupt handlers never induce page faults (thus potentially process switch).

* From above points, Interrupt handlers can never sleep or introduce page faults.

* Linux interleaves kernel paths for:
  1- To improve the througput of PIC and device controllers. Thanks to
  interleaving, kernel is able to send an acknowledgment even when it's handling
  a previous interrupt.
  2- To implement an interrupt model without priority levels. 

* Initialization of IDT entries is done with the help of below macros:

  2.6.25 +:

  --> include/asm-x86/desc_defs.h:
  /*
   * FIXME: Acessing the desc_struct through its fields is more elegant,
   * and should be the one valid thing to do. However, a lot of open code
   * still touches the a and b acessors, and doing this allow us to do it
   * incrementally. We keep the signature as a struct, rather than an union,
   * so we can get rid of it transparently in the future -- glommer
   */
  // 8 byte segment descriptor
  struct desc_struct {
	union {
		struct { unsigned int a, b; };
		struct {
			u16 limit0;
			u16 base0;
			unsigned base1: 8, type: 4, s: 1, dpl: 2, p: 1;
			unsigned limit: 4, avl: 1, l: 1, d: 1, g: 1, base2: 8;
		};

	};
  } __attribute__((packed));

  typedef struct desc_struct gate_desc;
  typedef struct desc_struct ldt_desc;
  typedef struct desc_struct tss_desc;

  --> include/asm-x86/desc.h:
  /*
   * Note the flags parameter is not used. just used instead of ist
   * field in the 64bit version
   */
  static inline void pack_gate(gate_desc *gate, unsigned char type,
       unsigned long base, unsigned dpl, unsigned flags, unsigned short seg)

  {
	/*|-----------------------------------+-----------------------------------|
	  |             SELECTOR              |           OFFSET 15..0            |0
	  +-----------------+-----------------+-----------------+-----------------+*/	  

	gate->a = (seg << 16) | (base & 0xffff);

	/* 31                23                15                7                0
	  +-----------------+-----------------+-----------------+-----+-----------+
	  |           OFFSET 31..16           | P |DPL|0 1 1 1 0|0 0 0|(NOT USED) |4
	  |-----------------------------------+-----------------------------------|*/


	gate->b = (base & 0xffff0000) |
		  (((0x80 | type | (dpl << 5)) & 0xff) << 8);
  }

  static inline void _set_gate(int gate, unsigned type, void *addr,
			      unsigned dpl, unsigned ist, unsigned seg)
  {
	gate_desc s;
	pack_gate(&s, type, (unsigned long)addr, dpl, ist, seg);
	/*
	 * does not need to be atomic because it is only done once at
	 * setup time
	 */
	write_idt_entry(idt_table, gate, &s);
  }


* Linux IDT entries are divided to:

    --> include/asm-x86/desc_defs.h:
    enum {
	GATE_INTERRUPT = 0xE,
	GATE_TRAP = 0xF,
	GATE_CALL = 0xC,
	GATE_TASK = 0x5,
   };

  + Interrupt Gate: 
    An intel interrupt gate that Can't be accessed by a user mode process (DPL =
    0), All Linux interrupted handlers are activaed by means of interrupt
    gates. It's initialized by code below:

    --> include/asm-x86/desc.h:
    static inline void set_intr_gate(unsigned int n, void *addr)
    {
	BUG_ON((unsigned)n > 0xFF);
	_set_gate(n, GATE_INTERRUPT, addr, 0, 0, __KERNEL_CS);
    }


  + Trap Gate: 
    An Intel trap gate that can't be accessed from user mode (DPL = 0). Most
    exception handlers are activated using Trap Gates.

    --> include/asm-x86/desc.h:
    static inline void set_trap_gate(unsigned int n, void *addr)
    {
	BUG_ON((unsigned)n > 0xFF);
	_set_gate(n, GATE_TRAP, addr, 0, 0, __KERNEL_CS);
    }

  + Task Gate: 
    An Intel task gate that can't be accessed from user mode. Only case is the
    handler of "Double Fault" exception (a kernel *mis*behaviour).

    --> include/asm-x86/desc.h:
    static inline void set_task_gate(unsigned int n, unsigned int gdt_entry)
    {
	BUG_ON((unsigned)n > 0xFF);
	_set_gate(n, GATE_TASK, (void *)0, 0, 0, (gdt_entry<<3));
    }

  + System Gate: 
    An intel _trap_ gate that can be accessed by a usermode process. Three linux
    exception handlers with vector numbers 4, 5, and 128 are activated by means
    of system gates.

    4    ---> Overflow		into		"Check for overflow"
    5	 ---> Bounds check 	bound 		"check on address bound"
    128  ---> System call 	int $0x80 	"Switch to kern mode"

    --> include/asm-x86/desc.h:
    static inline void set_system_gate(unsigned int n, void *addr)
    {
	BUG_ON((unsigned)n > 0xFF);
	/* Note: dpl here = 0x3 */
	_set_gate(n, GATE_TRAP, addr, 0x3, 0, __KERNEL_CS);
    }

  + System interrupt Gate: 
    An intel interrupt gate that can be issued in user mode (DPL = 3). like vector3.

    3    ---> Break point   	 int3	       "Usually inserted by a debugger"

    /*
     * This routine sets up an interrupt gate at directory privilege level 3.
     */
    static inline void set_system_intr_gate(unsigned int n, void *addr)
    {
	BUG_ON((unsigned)n > 0xFF);
	_set_gate(n, GATE_INTERRUPT, addr, 0x3, 0, __KERNEL_CS);
    }

* The IDT is stored in the idt_table table which includes 256 entries. 6byte
  idt_desr is used in system initialization when setting up the IDT address using 
  ``lidt'' assembly code.

  idt_descr is the IDT table described by below graph:
    31                23                15                7               0
   +-----------------+-----------------+-----------------+-----------------+
   |                                 BASE                                  |2
   +-----------------+-----------------------------------+-----------------|
                                       |               LIMIT               |0
                                       +-----------------+-----------------+

* During kernel initialization, setup_idt() assembly function strarts by filling
  the 256 IDT entries with the same interrupt gate, which refers to ignore_int():

  --> arch/x86/kernel/head_32.S:  
  /*
   *  setup_idt
   *
   *  sets up a idt with 256 entries pointing to ignore_int, interrupt gates. It
   *  doesn't actually load idt - that can be done only after paging has been
   *  enabled and the kernel moved to PAGE_OFFSET. Interrupts are enabled
   *  elsewhere, when we can be relatively sure everything is ok.
   *  Warning: %esi is live across this function.
   *
   *  FROM DARWISH: 
   *  In below code, %edx and %eax are used to fill the 8bytes entry as follows:
   *                         80386 INTERRUPT GATE (type E)
   *   31                23                15                7                0
   * +-----------------+-----------------+-----------------+-----+-----------+
   * |           OFFSET 31..16           | P |DPL|0 1 1 1 0|0 0 0|(NOT USED) |4   Filled by %edx
   * |-----------------------------------+-----------------------------------|
   * |             SELECTOR              |           OFFSET 15..0            |0   Filled by %eax
   * +-----------------+-----------------+-----------------+-----------------+
   */
  setup_idt:
	/* Store ignore_int address in %edx */
	lea ignore_int,%edx
	/* Store __KERNEL_CS Segment descriptor in high-order 16bits of %eax */
	movl $(__KERNEL_CS << 16),%eax
	/* Store low-order 16bits of handler address in %eax */
	movw %dx,%ax		
	/* Store the Flags in low order 16bit of %edx */
	movw $0x8E00,%dx	/* interrupt gate, dpl=0, present */

	lea idt_table,%edi
	mov $256,%ecx

  /* Loop till filling the 256 entries */
  rp_sidt:
	movl %eax,(%edi)
	movl %edx,4(%edi)
	addl $8,%edi
	dec %ecx
	jne rp_sidt

* ignore_int() assembly function can be considered as a null handler. It does the
  following:

  --> arch/i386/kernel/head.S:
  /* This is the default interrupt "handler" :-) */
  ignore_int:
	cld

  1- Saves the contents of some registers in stack (will be modified).

  #ifdef CONFIG_PRINTK
	pushl %eax
	pushl %ecx
	pushl %edx
	pushl %es
	pushl %ds

  Now we have a stack in this state (Not much sure actually):
     --> (Automatically put by the processor)
     %eflags        <-- 32(%esp)
     %cs	    <-- 28(%esp)
     %eip	    <-- 24(%esp)
     error-code     <-- 20(%esp)

     %eax	    <-- 16(%esp)
     %ecx	    <--	12(%esp)
     %edx	    <-- 8(%esp)
     %es	    <-- 4(%esp)
     %ds	    <-- (%esp)

	movl $(__KERNEL_DS),%eax
	movl %eax,%ds
	movl %eax,%es
	/* To avoid infinite recursion of invoking ignore_int() in case of errors */
	cmpl $2,early_recursion_flag
	je hlt_loop
	incl early_recursion_flag

  2- invokes prinkt() with below message:

     	/* The message to be printed [defined before the function]
     	int_msg:
	.asciz "Unknown interrupt or fault at EIP %p %p %p\n"

     	/* Push printk 5 parameters */
	pushl 16(%esp) 	   /* Above pushed %eax */
	pushl 24(%esp)     /* %eip */
	pushl 32(%esp)     /* %eflags */
	pushl 40(%esp)     /* %ss */
	pushl $int_msg

  #ifdef CONFIG_EARLY_PRINTK
	call early_printk
  #else
	call printk
  #endif
	/* Pop the printk 5 prameters */
	addl $(5*4),%esp

  3- Restore registers contents from stack
	popl %ds
	popl %es
	popl %edx
	popl %ecx
	popl %eax
  #endif

  4- Executes iret to restart the interrupted program
	iret

* Exception handlers have a standard structure consisting of three steps:
  1- Saves the contents of most registers in the kernel mode stack.
  2- Handles the exception by means of a high level C function.
  3- Exit handler using the ret_from_exeption() function.

* After inintializiong IDT with ignore_int, trap_init() function inserts the
  final values in the IDT entries that refer to exceptions and nonmaskable
  interrupts.

  --> arch/x86/kernel/traps_32.c:

  DECLARE_BITMAP(used_vectors, NR_VECTORS);
  EXPORT_SYMBOL_GPL(used_vectors);

  void __init trap_init(void)
  {
  #ifdef CONFIG_X86_LOCAL_APIC
	init_apic_mappings();
  #endif

	set_trap_gate(0,&divide_error);
	set_intr_gate(1,&debug);
	set_intr_gate(2,&nmi);
	set_system_intr_gate(3, &int3); /* int3/4 can be called from all */
	set_system_gate(4,&overflow);
	set_trap_gate(5,&bounds);
	set_trap_gate(6,&invalid_op);
	set_trap_gate(7,&device_not_available);
	set_task_gate(8,GDT_ENTRY_DOUBLEFAULT_TSS);
	set_trap_gate(9,&coprocessor_segment_overrun);
	set_trap_gate(10,&invalid_TSS);
	set_trap_gate(11,&segment_not_present);
	set_trap_gate(12,&stack_segment);
	set_trap_gate(13,&general_protection);
	set_intr_gate(14,&page_fault);
	set_trap_gate(15,&spurious_interrupt_bug);
	set_trap_gate(16,&coprocessor_error);
	set_trap_gate(17,&alignment_check);
   #ifdef CONFIG_X86_MCE
	set_trap_gate(18,&machine_check);
   #endif
	set_trap_gate(19,&simd_coprocessor_error);
	[...]
	set_system_gate(SYSCALL_VECTOR,&system_call);

	/* Reserve all the builtin and the syscall vector. */
	for (i = 0; i < FIRST_EXTERNAL_VECTOR; i++)
		set_bit(i, used_vectors);
	set_bit(SYSCALL_VECTOR, used_vectors);
   }

* "Double Fault" Exception is handled by means of task gate because it represents
  a serious kernel misbehaviour. Exception handler that tries to print out the
  register values don't trust the value of %esp. The CPU fetches the task gate
  descriptor which points to a special TSS segment descriptor stored in 32 entry
  of GDT. 

	.quad 0x0000000000000000	/* 0xf8 - GDT entry 31: double-fault TSS */  
	--> include/asm-i386:
	#define GDT_ENTRY_DOUBLEFAULT_TSS	31  


* Taking the invalid_op as an exception handler example. 

  --> arch/x86/kernel/entry_32.S:
  ENTRY(invalid_op)
	RING0_INT_FRAME
	/* push a null value for stack padding in case of no error will be 
	 * returned 
	 * Just to decrease code complexity and to keep uniform stack frame */	
	pushl $0
	/* CFI_*: Macros for dwarf2 CFI unwind table entries. see $(info as)
	 * Google Frame Unwinding functions 
	 * Removed to ease redability in below codes*/
	/* Push the address of the high level function in the stack */
	pushl $do_invalid_op
	/* Assembly Language fragment, similar for most handlers */
	jmp error_code
  END(invalid_op)

  So the error_code job is to:
  
  1- Saves the registers that might be used by the high level C functions on
     stack.

error_code:
	/* the function address is in %fs's slot on the stack */
	pushl %es
	pushl %ds
	pushl %eax
	pushl %ebp
	pushl %edi
	pushl %esi
	pushl %edx
	pushl %ecx
	pushl %ebx

     So now we have a stack in this form:
     (Check also ptrace.h::pt_regs and entry.S first comment)

     --> (Automatically put by the processor)
     %oldss         <-- 3C(%esp)             
     %oldesp        <-- 38(%esp)
     %eflags        <-- 34(%esp)
     %cs	    <-- 30(%esp)
     %eip	    <-- 2C(%esp)

     --> put by push $0 (faked) or a real error code    
     error-code     <-- 28(%esp), PT_ORIG_EAX(%esp)

     --> put by push $do_invalid_op
     %fs      	    <-- 24(%esp), PT_FS(%esp)

     --> pushed by above code block in error_code
     %es      	    <-- 20(%esp)
     %ds            <-- 1C(%esp)
     %eax     	    <-- 18(%esp) 
     %ebp     	    <-- 14(%esp)
     %edi     	    <-- 10(%esp)
     %esi     	    <-- C(%esp)
     %edx     	    <-- 8(%esp)
     %ecx     	    <-- 4(%esp)
     %ebx     	    <-- %esp            [Stack Head]
     
  2- Issues a cld instruction (Clear Direction Flag). Clearing direction flag
     will cause the string instructions done forward. String instructions
     typically uses %esi to denote the source string and %edi pair to denote the
     destination string

     	cld
	/* I don't know the reason of below code */
	pushl %fs
	movl $(__KERNEL_PDA), %ecx
	movl %ecx, %fs
	UNWIND_ESPFIX_STACK
	popl %ecx

  3- Loads %edi with the address of the high_level do_handler_name() C function,
     Copies the hardware error code in %edx, Stores -1 in the same stack location
     (used to separate 0x80 exceptions [sys calls] from others).
  
	/* Rememeber the comment above about storing address at %fs position ? */
	movl PT_FS(%esp), %edi		# get the function address

	/* This was stored before the func (null if no error code exist) */

	 * This is passed as the **second argument** to *%edi function */
	movl PT_ORIG_EAX(%esp), %edx	# get the error code

	movl $-1, PT_ORIG_EAX(%esp)	# no syscall to restart
	mov  %ecx, PT_FS(%esp)

   4- Loads in %eax the current top location of the kernel mode stack. This
      contains the address of the last register saved in step one. This also
      represents the standard registers saving pt_regs structure passed as the
      first argument to do_* functions i.e. in "call *%edi".

        /* The **first argument** to the *%edi method */
	movl %esp,%eax			# pt_regs pointer

   5- Loads user data segment selector into the ds and es registers.
	movl $(__USER_DS), %ecx
	movl %ecx, %ds
	movl %ecx, %es

   6- Invokes the high level C function:

   	call *%edi

   7- Function recieves its parameters from the %eax and %edx registers. %eax
      stores the origianal register values, %edx stores the error code ;).

      --> arch/x86/kernel/traps_32.c:
      void do_invalid_op(struct pt_regs *, unsigned long);      

* Example of a handler, do_general_protection:

  fastcall void __kprobes do_general_protection(struct pt_regs * regs,
					      long error_code)
  {
	int cpu = get_cpu();
	struct tss_struct *tss = &per_cpu(init_tss, cpu);
	struct thread_struct *thread = &current->thread;


  1- Store the hardware error code and the exception vector in the 
     current process descriptror:

     	current->thread.error_code = error_code;
	current->thread.trap_no = 13;

  2- The exception always check whether the interrupt is from user mode or kernel
     mode. In the latter case, it's a kernel bug. The kernel kills the proecss
     after issuing oops to avoid data corruption on hard disks.

	if (!user_mode(regs))
		goto gp_in_kernel;

gp_in_kernel:
	if (!fixup_exception(regs)) {
		if (notify_die(DIE_GPF, "general protection fault", regs,
				error_code, 13, SIGSEGV) == NOTIFY_STOP)
			return;
		die("general protection fault", regs, error_code);
	}


  3- Otherwise (issued from user mode), current takes a signal right after the
     termination of the signal handler. 

        force_sig(SIGSEGV, current);


Interrupt Handling:
------------------

* IMPORTANT: Most exceptions (as stated above) are handled by sending a Unix
  signal to the userspace process. The kernel can process the exception quickly.

  This can't be done in interrupts, cause when they're triggered, a completely
  unrelated process may be running (and that's usually the case). So signalling
  the current process is *useless*.

* Interrupt handling depends on the kind of the interrupt. They can be classified
  to:
  1- I/O interrupts: An I/O device requires attention.
  2- Timer interrupts: Some timer, a local APIC or external timer issued an
     	   	       interrupt. This tells the kernel that a fixed-time
  		       interval has elpased.
  3- Interprocessor interrupts: CPU issued an interrupt to another CPU.


* Interrunt handler must be flexible. Ex, PCI bus architecture allows several
  devices to share the same IRQ line. This flexibility can be achieved by two
  ways:

  1- IRQ sharing: Handler executes _several_ Interrupt Service Routines (ISR). Each
     ISR is related to a single device. This is done cause it can't be known in
     advance which device signalled the interrupt.

  2- IRQ dynamic allocation: IRQ line is associated with a device at the last
     	 	 	     possible minute (like only when entering a floppy).


* Long noncritical operations should be deferred in interrupt handlers. WHY
  ?. While an interrupt handler is running, signals on the corresponding IRQ line
  are temporarily ignored. Also (important point) cause the process the interrupt
  is running on behalf of must alwasy stay in TASK_RUNNING or a system freeze can
  occur. Therefore interrupts can not perform any blocking procedure.

* Linux devides actions to be performed following an interrupt to:

  1- Critical:
     Such as acknowledging the interrupt to the PIC, reprogramming the PIC or
     device controller. 
     Can be executed quickly and are critical, cause they must be performed as
     soon as possible.
     Executed within the handler. Maskable interrupts _disabled_.

  2- Noncritical:
     Such as updating data structures that are accessed by the processor. 
     Can be executed quickly.
     Executed within the handler, with interrupts enabled.
     Ex: Reading scan code after a key has been pushed.

  3- Noncritical Deferrable:
     Such as copying a buffer's content into the address space of a process. 
     Executed by external functions by means of tasklets.

* IMB PCs requires some devices to be statically connected to specific IRQs, 
  1- interval timer , IRQ 0
  2- slave 8259A PIC , IRQ 2 (deprecated by APICs)
  3- external mathematical coprocessor , IRQ 13 . recent 80x86 processors no
     longer use them
  4- I/O device can be connected to limited number of IRQ lines in general.

* IRQ Addresses map:
  0 - 19 , 0x0 - 0x13			Nonmaskable interrupts and exceptions
  20 - 31, 0x14 - 0x1f			Intel reserved
  32 - 127, 0x20 - 0x7f			External interrupts (IRQs)
  128  0x80 	   			Programmed exception for sys calls
  129 - 238, 0x81 - 0xee		External interrupts (IRQs)
  239  0xef  	    			Local APIC timer interrupt
  240  0xf0				Local APIC thermal interrupt (pentium 4)
  241 - 250 , 0xf1 - 0xfa		Linux reserved
  251 - 253 , 0xfb - 0xfd		Interprocessor interrupts
  254 0xfe    	     			Local APIC error interrupt
  255 0xff				Local APIC spurious interrupt


* Every interrupt vector has his own irq_desc_t descriptor. 
  --> include/linux/irq.h:
  /**
   * struct irq_desc - interrupt descriptor
   *
   * @handle_irq:	highlevel irq-events handler [if NULL, __do_IRQ()]
   * @chip:		low level interrupt hardware access
   * @msi_desc:		MSI descriptor
   * @handler_data:	per-IRQ data for the irq_chip methods
   * @chip_data:	platform-specific per-chip private data for the chip
   *			methods, to allow shared chip implementations
   * @action:		the irq action chain
   * @status:		status information
   * @depth:		disable-depth, for nested irq_disable() calls
   * @wake_depth:	enable depth, for multiple set_irq_wake() callers
   * @irq_count:	stats field to detect stalled irqs
   * @irqs_unhandled:	stats field for spurious unhandled interrupts
   * @lock:		locking for SMP
#ifdef CONFIG_SMP
   * @affinity:		IRQ affinity on SMP
   * @cpu:		cpu index useful for balancing
#endif
   * @pending_mask:	pending rebalanced interrupts
   * @dir:		/proc/irq/ procfs entry
   * @affinity_entry:	/proc/irq/smp_affinity procfs entry on SMP
   * @name:		flow handler name for /proc/interrupts output
   */

* An interrupt is unexpected if there's no ISRs associated with the lines, or if
  no ISR associated with the line recongnizes the device. Kernel disables the IRQ
  line iff number of unhandled interrupts `irq_unahandled' > 99,000 for each
  100,000 interrupt.

* `status' discribes the "IRQ line status" using below flags:

  #define IRQ_INPROGRESS	0x00000100	/* IRQ handler active - do not enter! */
  #define IRQ_DISABLED		0x00000200	/* IRQ disabled - do not enter! */
  #define IRQ_PENDING		0x00000400	/* IRQ pending - replay on enable */
  #define IRQ_REPLAY		0x00000800	/* IRQ has been replayed but not acked yet */
  #define IRQ_AUTODETECT	0x00001000	/* IRQ is being autodetected */
  #define IRQ_WAITING		0x00002000	/* IRQ not yet seen - for autodetection */
  #define IRQ_LEVEL		0x00004000	/* IRQ level triggered */
  #define IRQ_MASKED		0x00008000	/* IRQ masked - shouldn't be seen again */
  #define IRQ_PER_CPU		0x00010000	/* IRQ is per CPU */
  #define IRQ_NOAUTOEN		0x00080000	/* IRQ will not be enabled on request irq */

* {enable,disable}_irq() playes with the `depth' field as follows:
  
  --> kernel/irq/manage.c:
  /**
   *	enable_irq - enable handling of an irq
   *	@irq: Interrupt to enable
   *
   *	Undoes the effect of one call to disable_irq().  If this
   *	matches the last disable, processing of interrupts on this
   *	IRQ line is re-enabled.
   *
   *	This function may be called from IRQ context.
   */
  void enable_irq(unsigned int irq)
  {
	struct irq_desc *desc = irq_desc + irq;
	unsigned long flags;

	if (irq >= NR_IRQS)
		return;

	/* Aiee, discreptor lock! */
	spin_lock_irqsave(&desc->lock, flags);
	switch (desc->depth) {
	case 0:
		printk(KERN_WARNING "Unbalanced enable for IRQ %d\n", irq);
		WARN_ON(1);
		break;
	/* In all cases, decrement the depth field. really enable IRQs when
	 * enable/disable_irq balance occurs (i.e. --depth = 0 */
	case 1: {
		unsigned int status = desc->status & ~IRQ_DISABLED;

		/* Prevent probing on this irq: */
		desc->status = status | IRQ_NOPROBE;

		/* This forces hardware to resend a lost IRQ */
		check_irq_resend(desc, irq);
		/* fall-through */
	}
	default:
		desc->depth--;
	}
	spin_unlock_irqrestore(&desc->lock, flags);
  }
  EXPORT_SYMBOL(enable_irq);

  /**
   *	disable_irq_nosync - disable an irq without waiting
   *	@irq: Interrupt to disable
   *
   * 	Disable the selected interrupt line.  Disables and Enables are
   *	nested.
   *	Unlike disable_irq(), this function does not ensure existing
   *	instances of the IRQ handler have completed before returning.
   *
   *	This function may be called from IRQ context.
   */
  void disable_irq_nosync(unsigned int irq)
  {
	struct irq_desc *desc = irq_desc + irq;
	unsigned long flags;

	if (irq >= NR_IRQS)
		return;

	spin_lock_irqsave(&desc->lock, flags);
	if (!desc->depth++) {
		desc->status |= IRQ_DISABLED;
		desc->chip->disable(irq);
	}
	spin_unlock_irqrestore(&desc->lock, flags);
  }
  EXPORT_SYMBOL(disable_irq_nosync);


Initializing Interrupts (not exceptions) IDT entries:
-----------------------------------------------------

* During system initialization, native_init_IRQ() is issued. it replaces IDT
  entries with new ones instead of the default ignore_int (null) handlers set up
  by setup_idt: 

  --> include/asm-x86/mach-default/irq_vectors.h:
  /*
   * This file should contain #defines for all of the interrupt vector
   * numbers used by this architecture.
   *
   * In addition, there are some standard defines:
   *
   *	FIRST_EXTERNAL_VECTOR:
   *		The first free place for external interrupts
   *
   *	SYSCALL_VECTOR:
   *		The IRQ vector a syscall makes the user to kernel transition
   *		under.
   *
   *	TIMER_IRQ:
   *		The IRQ number the timer interrupt comes in at.
   *
   *	NR_IRQS:
   *		The total number of interrupt vectors (including all the
   *		architecture specific interrupts) needed.
   *
   */			
   
   /*
    * IDT vectors usable for external interrupt sources start
    * at 0x20:
    */
   #define FIRST_EXTERNAL_VECTOR	0x20
   #define SYSCALL_VECTOR		0x80
   #define TIMER_IRQ 0
   
   /*
    * The maximum number of vectors supported by i386 processors
    * is limited to 256. For processors other than i386, NR_VECTORS
    * should be changed accordingly.
    */
  #define NR_VECTORS 256

  --> arch/x86/kernel/i8259_32.c:

  /* Overridden in paravirt.c */
   void init_IRQ(void) __attribute__((weak, alias("native_init_IRQ")));

  void __init native_init_IRQ(void)
  {
	[...]
	/*
	 * Cover the whole vector space, no vector can escape
	 * us. (some of these will be overridden and become
	 * 'special' SMP interrupts)
	 */
	for (i = 0; i < (NR_VECTORS - FIRST_EXTERNAL_VECTOR); i++) {
		int vector = FIRST_EXTERNAL_VECTOR + i;
		/*
		 * NR_IRQS are limited by our PIC chipset.
		 * Two cascaded 8259As supports only 16 IR.
		 * APIC support the full 224 range
		 * NR_IRQS max is 224. This is so cause 386 max INT is 255 and
		 * the first 32 vectors are already reserved.
		 */
		if (i >= NR_IRQS)
			break;

		/* Pre 2.6.25: */
		if (vector != SYSCALL_VECTOR) 
		   	/* 
	 		 * Inserts an interrupt  gate in the `n'th entry of IDT,
  			 * containing the address of the handler interrupt[i].
	 		 */
			set_intr_gate(vector, interrupt[i]);

		/* 2.6.25 +: */
		/* SYSCALL_VECTOR was reserved in trap_init. */
		if (!test_bit(vector, used_vectors))
			set_intr_gate(vector, interrupt[i]);
	}
	[...]
  }
 
* Linux supports several PICs other than the i8259A chip. Other chips include the
  SMP IO-APIC, Intel PIIX4's internal PIC, SGI's (IO-)APIC. Linux uses a uniform
  way to deal with them consisting of a PIC object ``irq_chip'' and number of
  standard methods to handle it. 

  --> include/linux/irq.h:
  /* struct irq_chip - hardware interrupt chip descriptor */
  struct irq_chip {
   	/* PIC name for /proc/interrupts */
	const char	*name;
	/* Start or shutdown an IRQ line in the chip */
	unsigned int	(*startup)(unsigned int irq);
	void		(*shutdown)(unsigned int irq);
	/* Enable or disable an IRQ line 
	 * Enable/start, Disable/shutdown are equivalent on 8259A */
	void		(*enable)(unsigned int irq);
	void		(*disable)(unsigned int irq);
	/* Acknowledge a recieved IRQ by sending proper bytes to the chip I/O
   	ports */
	void		(*ack)(unsigned int irq);
	void		(*mask)(unsigned int irq);

	/* invoked when IRQ's interrupt handler terminate */
	void		(*end)(unsigned int irq);
	void		(*set_affinity)(unsigned int irq, cpumask_t dest);

	/*
	 * For compatibility, ->typename is copied into ->name.
	 * Will disappear.
	 */
	const char	*typename;
  };

  --> arch/x86/kernel/i8259_32.c:

  /* hardware depended methods */
  static struct irq_chip i8259A_chip = {
	.name		= "XT-PIC",
	.mask		= disable_8259A_irq,
	.disable	= disable_8259A_irq,
	.unmask		= enable_8259A_irq,
	.mask_ack	= mask_and_ack_8259A,
  };

* Multiple devices can share the same IRQ. Kernel maintains an`irqaction'
  descriptor, each refers to a specific hardware device and a specific 
  interrupt.

  --> include/linux/interrupt.h:

  struct irqaction {
  	/* points to an ISR for an I/O device, key structure element */
	irq_handler_t handler;
	/* Describes some relations between the IRQ line and the I/O device */
	unsigned long flags;
	/* Not used */
	cpumask_t mask;
	/* I/O device name, for /proc/interrupts */
	const char *name;
	/* Private field for the I/O device (identifies the device)*/
	void *dev_id;
	/* next element in the list of irqaction descriptors */
	struct irqaction *next;
	/* IRQ line */
	int irq;
	/* proc dir entry for proc/irq/n directory associated with IRQ n */
	struct proc_dir_entry *dir;
  };

  /*
   * These flags used only by the kernel as part of the
   * irq handling routines.
   *
   * IRQF_DISABLED - keep irqs disabled when calling the action handler
   * IRQF_SAMPLE_RANDOM - irq is used to feed the random generator
   * IRQF_SHARED - allow sharing the irq among several devices
   * IRQF_PROBE_SHARED - set by callers when they expect sharing mismatches to occur
   * IRQF_TIMER - Flag to mark this interrupt as timer interrupt
   * IRQF_PERCPU - Interrupt is per cpu
   * IRQF_NOBALANCING - Flag to exclude this interrupt from irq balancing
   */
  #define IRQF_DISABLED		0x00000020
  #define IRQF_SAMPLE_RANDOM	0x00000040
  #define IRQF_SHARED		0x00000080
  #define IRQF_PROBE_SHARED	0x00000100
  #define IRQF_TIMER		0x00000200
  #define IRQF_PERCPU		0x00000400
  #define IRQF_NOBALANCING	0x00000800

  /*
   * These correspond to the IORESOURCE_IRQ_* defines in
   * linux/ioport.h to select the interrupt line behaviour.  When
   * requesting an interrupt without specifying a IRQF_TRIGGER, the
   * setting should be assumed to be "as already configured", which
   * may be as per machine or firmware initialisation.
   */
  #define IRQF_TRIGGER_NONE	0x00000000
  #define IRQF_TRIGGER_RISING	0x00000001
  #define IRQF_TRIGGER_FALLING	0x00000002
  #define IRQF_TRIGGER_HIGH	0x00000004
  #define IRQF_TRIGGER_LOW	0x00000008
  #define IRQF_TRIGGER_MASK	(IRQF_TRIGGER_HIGH | IRQF_TRIGGER_LOW | \
				 IRQF_TRIGGER_RISING | IRQF_TRIGGER_FALLING)
  #define IRQF_TRIGGER_PROBE	0x00000010


  About Interrupt trigger levels from wikipedia:

  There are two types of trigger mechanisms, level-triggered interrupts and
  edge-triggered interrupts. An edge-triggered interrupt is a class of interrupts
  that are signalled by a level transition on the interrupt line, either a
  falling edge (1 to 0) or (usually) a rising edge (0 to 1). A level-triggered
  interrupt is a class of interrupts where the presence of an unserviced
  interrupt is indicated by a high level (1), or low level (0). Devices sharing
  the same IRQ must use the same triggering method. New PCI-Express uses a
  message signalling scheme.

* During System Bootstrap, booting CPU executes the setup_IO_APIC_irqs() function
  to initialize the I/O APIC chip. 24 entries of the Interrupt Redirection Table
  chip are filled. All IRQ signals from I/O devices can be routed to each CPU in
  the system according to ``lowest piriority'' scheme discussed above.

* In some cases, the hardware fails to distribute interrupts among processors in
  a fair way. Kirqd is used -if necessary- to correct the automatic hardware
  assignment to CPUs. Kirdq peridically executes do_irq_balance() which keeps
  track of number of interrupts recieved by every CPU, If an IRQ imbalance found,
  it modifies the routing table as needed.

Initializing the interrupt[] array: 
(Which holds each general/base interrupt handler)
-----------------------------------
    
* Important. The kernel expects the stack to be layed out in a specific format
  after interrupt initialization. This format is described the pt_regs structure
  described in include/asm-x86/ptrace.h and also described in the main entry_32.S
  comment.

  --> include/asm-x86/ptrace.h:
  #define SAVE_ALL \
	cld; \
	pushl %fs; \
	pushl %es; \
	pushl %ds; \
	pushl %eax; \
	pushl %ebp; \
	pushl %edi; \
	pushl %esi; \
	pushl %edx; \
	pushl %ecx; \
	pushl %ebx; \
	movl $(__USER_DS), %edx; \
	movl %edx, %ds; \
	movl %edx, %es; \
	movl $(__KERNEL_PDA), %edx; \
	movl %edx, %fs

  /* this struct defines the way the registers are stored on the 
   * stack during a system call. */

  struct pt_regs {
  /*  Mostly pushed by the SAVE_ALL macro (in the SAME order) */
	long bx;
	long cx;
	long dx;
	long si;
	long di;
	long bp;
	long ax;
	int  ds;
	int  es;
	int  fs;
	/* int  gs; */

   /* This represents interrupt value or system call number 
    * Called orig_eax cause system call numbers are passed in %eax 
    * +ve: means system call number. 
    * -ve: means externel IRQ value (from 32, to 127) pushed in a 
    * 	   negative state (see `push $~(vector)' below)
    */
	long orig_ax;

   /* Automatically pushed by the i386 control unit */
	long ip;
	int  cs;
	long flags;
	long sp;
	int  ss;
  };

  Speaking from bottom to up, process sees an interrupt, pushes the first 5
  registers. after that the instruction "push $~(vector)" (below) fills the
  orig_eax slot, then SAVE_ALL saves other registers.

* The `interrupt' array is built through entry.S. Array includes NR_IRQS
  elements. NR_IRQS = 16 if system uses old cascaded 8259A chips or 224 (256 - 32
  = 244) for new I/O APIC chips. The element at index `n' stores the address of
  the following code snippet:

  --> arch/x86/kernel/entry_32.S:

  /*
   * Build the entry stubs and pointer table with
   * some assembler magic.
   */
  .data
  ENTRY(interrupt)
  .text

  ENTRY(irq_entries_start)
	RING0_INT_FRAME
  vector=0
  .rept NR_IRQS
	ALIGN
    .if vector
	CFI_ADJUST_CFA_OFFSET -4
    .endif

  Push NOT(~) value of IRQ number in the stack (+ve values used to identify
  system calls). Then, a call a code snipped used by all interrupts:

  1:	pushl $~(vector)           /* interrupt array index */
	CFI_ADJUST_CFA_OFFSET 4
	jmp common_interrupt
  .previous
	.long 1b
  .text
  vector=vector+1
  .endr
  END(irq_entries_start)

  Common code in all interrups then start from common_interrupt:

  --> arch/x86/kernel/entry_32.S:

  /*
   * the CPU automatically disables interrupts when executing an IRQ vector,
   * so IRQ-flags tracing has to follow that.
   * Note: Cause this is an intel interrupt gate (IF is cleared)
   */
   common_interrupt:
	/* Save all CPU registers that maybe used in stack. %eflags, %cs, %eip,
  	 * %ss and %esp are already saved automatically by the control unit */
	SAVE_ALL
	TRACE_IRQS_OFF
	/* %eax is the pt_regs for the do_IRQ() method */
	movl %esp,%eax
	call do_IRQ
	jmp ret_from_intr
   ENDPROC(common_interrupt)
	
* do_IRQ() function is invoked to execute all the ISRs associated with the
  interrupt. 
  
  --> arch/x86/kernel/irq_32.c:

  unsigned int do_IRQ(struct pt_regs *regs)
  {	
	struct pt_regs *old_regs;
	/* high bit used in ret_from_ code */
	int irq = ~regs->orig_eax;
	/* irq_desc: An array of descriptors described in below point */
	struct irq_desc *desc = irq_desc + irq;

  1- Do some sanity checks

  	if (unlikely((unsigned)irq >= NR_IRQS)) {
		printk(KERN_EMERG "%s: cannot handle IRQ %d\n",
					__FUNCTION__, irq);
		BUG();
	}
	old_regs = set_irq_regs(regs);

  2- Executes irq_enter() macro which increases a counter representing the number
     of nested interrupt handlers. Counter is stored in `preempt_count' field of
     the thread_info structure. 

     	irq_enter();

  3- Check for stack overflow if stack debuggin symbols is enabled.

     --> include/asm-i386/thread_info.h:
     #define PREEMPT_ACTIVE		0x10000000
     #ifdef CONFIG_4KSTACKS
     #define THREAD_SIZE            (4096)
     #else
     #define THREAD_SIZE		(8192)
     #endif

     #define STACK_WARN             (THREAD_SIZE/8)

     --> Back to our function:

     #ifdef CONFIG_DEBUG_STACKOVERFLOW
	/* Debugging check for stack overflow: is there less than 1KB free? */
	{
		long esp;
		
		/* Below expression can be translated to: 
		 * andl %esp $THREAD_SIZE-1, put results back to C var esp
		 *
		 * So esp variable represents the free value of the 2pages stack
		 * 
		 * use %%esp to distinguish between operands and registers 
		 */

		__asm__ __volatile__("andl %%esp,%0" :

		/* "r": operand constraint, use any GPR (General Purspose
		 * Register) to store the operand.
		 * Some other common constraints:
    		 * +---+--------------------+
    		 * | r |    Register(s)     |
    		 * +---+--------------------+
    		 * | a |   %eax, %ax, %al   |
    		 * | b |   %ebx, %bx, %bl   |
    		 * | c |   %ecx, %cx, %cl   |
    		 * | d |   %edx, %dx, %dl   |
    		 * | S |   %esi, %si        |
    		 * | D |   %edi, %di        |
    		 * +---+--------------------+
		 * "=": constraint modifier, output operand is a write only
		 * (esp): esp is the C variable output operand, represented in
		 *         the asm expression as %0
		 */
					"=r" (esp) : 
		/* "0": single variable serve as both input and output operand */

					"0" (THREAD_SIZE - 1));

		if (unlikely(esp < (sizeof(struct thread_info) + STACK_WARN))) {
			printk("do_IRQ: stack overflow: %ld\n",
				esp - sizeof(struct thread_info));
			dump_stack();
		}
	}
     #endif

  4- Call the specific handler of the specific vector (As stated above, it's
     usually the __do_IRQ method):

     	     	/* From first lines */
		int irq = ~regs->orig_eax;
		[...]
     		desc->handle_irq(irq, desc);
  
* __do_IRQ is the highlevel IRQ handler (it stays in non architecture specific
  kernel section) . i.e., the one responsible for firing the
  real ISRs (if appropriate): 

  --> kernel/irq/handle.c:

  /**
   * __do_IRQ - original all in one highlevel IRQ handler
   * @irq:	the interrupt number
   *
   * __do_IRQ handles all normal device IRQ's (the special
   * SMP cross-CPU interrupts have their own specific
   * handlers).
   *
   * This is the original x86 implementation which is used for every
   * interrupt type.
   */
  fastcall unsigned int __do_IRQ(unsigned int irq)
  {
  	struct irq_desc *desc = irq_desc + irq;
	struct irqaction *action;
	unsigned int status;

  1- Acquire the irq descriptor lock and send an ack to the PIC

	spin_lock(&desc->lock);
	if (desc->chip->ack)
		desc->chip->ack(irq);

  2- Clear the IRQ_REPLAY and IRQ_WAITING flags. Set the IRQ_PENDING flag.
     The PENDING flag is set cause the interrupt has been acknowledged but not
     handled yet (but we'll handle it soon). 

    #define IRQ_PENDING		0x00000400	/* IRQ pending - replay on enable */
    #define IRQ_REPLAY		0x00000800	/* IRQ has been replayed but not acked yet */
    #define IRQ_WAITING		0x00002000	/* IRQ not yet seen - for autodetection */

	/*
	 * REPLAY is when Linux resends an IRQ that was dropped earlier
	 * WAITING is used by probe to mark irqs that are being tested
	 */
	status = desc->status & ~(IRQ_REPLAY | IRQ_WAITING);
	status |= IRQ_PENDING; /* we _want_ to handle it */

  3- Do NOT execute ISRs if IRQ_DISABLED or IRQ_INPROGRESS are set. Don't also
     execute ISRs if the descriptor action element is already NULL!.

     IRQ_DISABLED: interrupt from a disabled IRQ line ! ?. Yes, a spurious/buggy
     		    interrupt
     IRQ_INPROGRESS: happens when another CPU is handling another same kind of
     		      triggered interrupt. kernel ISRs are not reentrant, they
     		      are serialized (thus we won't execute our ISR now).

     NOTE: an interrupt can never trigger two processors at the same time. The
     	   above case can only happen when two interrupts of the _same_ kind are
     	   triggered in close times.

	/*
	 * If the IRQ is disabled for whatever reason, we cannot
	 * use the action we have.
	 */
	action = NULL;
	if (likely(!(status & (IRQ_DISABLED | IRQ_INPROGRESS)))) {
		action = desc->action;
		status &= ~IRQ_PENDING; /* we commit to handling */
		status |= IRQ_INPROGRESS; /* we are handling it */
	}
	desc->status = status;

  4- Clear everything if we won't execute the ISRs:
  
  	/*
	 * If there is no IRQ handler or it was disabled, exit early.
	 * Since we set PENDING, if another processor is handling
	 * a different instance of this same irq, the other processor
	 * will take care of it.
	 */
	if (unlikely(!action))
		goto out;

	[...]
	
	out:
	/*
	 * The ->end() handler has to deal with interrupts which got
	 * disabled while the handler was running.
	 */
	desc->chip->end(irq);
	spin_unlock(&desc->lock);

	return 1;
		
  5- If everything is fine. As stated in point 3, IRQ_INPROGRESS flag is set and
     _PENDING flag got cleared. A loop starts after that:

     	for (;;) {
		irqreturn_t action_ret;

		spin_unlock(&desc->lock);

		/* At last, handle that IRQ */
		action_ret = handle_IRQ_event(irq, action);
		if (!noirqdebug)
			note_interrupt(irq, desc, action_ret);

		spin_lock(&desc->lock);
		/* 
		 * if a similar IRQ was triggered on another processor:
		 * 1- Disable the IRQ_PENDING (we'll handle this interrupt now)
		 * 2- loop to handle the interrupt
		 * 3- The IRQ_INPROGRESS flag will still be set. We're taking the
		 *    complete responsibility of that line till now.
		 *
		 * This situation happens when another processes recieved the
		 * same kind of interrupt 
		 *
		 * In other words, this for(;;) loop exists so we can handle the
		 * same interrupt if it's triggered again and received by another
		 * CPU (which set the status again to IRQ_PEDNING) (cause we are 
		 * already at it).
		 *
		 * This does not introduce race conditions cause we acquire the
		 * lock before checking and while clearing the flag. We clear
		 * the flag under the lock to indicate that we took the job
		 * of handling this interrupt.
		 */
		if (likely(!(desc->status & IRQ_PENDING)))
			break;
		desc->status &= ~IRQ_PENDING;
	}
	desc->status &= ~IRQ_INPROGRESS;

  6- Clears everything. Call end() which will trigger pending interrupts
  (interrupts disabled while executing a handler for a similar previous
  interrupt) and relase the lock

	out:
	/*
	 * The ->end() handler has to deal with interrupts which got
	 * disabled while the handler was running.
	 */
	desc->chip->end(irq);
	spin_unlock(&desc->lock);

	return 1;
  

Initializing the irq_desc[] array:
--------------------------------

* IRQ descriptors for each IRQ are stored in the irq_desc array, which is
  _null_ initialized as follows:

  --> kernel/irq/handle.c:
  /*
   * Linux has a controller-independent interrupt architecture.
   * Every controller has a 'controller-template', that is used
   * by the main code to do the right thing. Each driver-visible
   * interrupt source is transparently wired to the appropriate
   * controller. Thus drivers need not be aware of the
   * interrupt-controller.
   *
   * The code is designed to be easily extended with new/different
   * interrupt controllers, without having to do assembly magic or
   * having to touch the generic code.
   *
   * Controller mappings for all interrupt sources:
   */
   struct irq_desc irq_desc[NR_IRQS] __cacheline_aligned = {
	[0 ... NR_IRQS-1] = {
		.status = IRQ_DISABLED,
		.chip = &no_irq_chip,

		/* IMPORTANT: In sane cases, this points to
		 * 	      __do_irq()
		 */
		.handle_irq = handle_bad_irq,

		.depth = 1,
		.lock = __SPIN_LOCK_UNLOCKED(irq_desc->lock),
   #ifdef CONFIG_SMP
		.affinity = CPU_MASK_ALL
   #endif
	}
   };

   void fastcall
   handle_bad_irq(unsigned int irq, struct irq_desc *desc)
   {
	/* Inform the user about this interrupt details */
	print_irq_desc(irq, desc);
	kstat_this_cpu.irqs[irq]++;
	ack_bad_irq(irq);
   }

   print_irq_desc() inform the user about the triggered bad interrupt. You can
   see such messages by this sample module:

   ###########################################
   #include <linux/module.h>
   #include <linux/moduleparam.h>
   #include <linux/init.h>

   MODULE_LICENSE("GPL");

   int init(void) { asm("int $100");  return 0;}

   void exit(void) { ; }

   module_init(init);
   module_exit(exit);
   ###########################################

   Where the result will be:

   [42287.956973] irq 68, desc: c03d7c10, depth: 1, count: 0, unhandled: 0
   [42287.956981] ->handle_irq():  c014a870, handle_bad_irq+0x0/0x270
   [42287.956994] ->chip(): c03daf40, no_irq_chip+0x0/0x40
   [42287.957000] ->action(): 00000000
   [42287.957002]   IRQ_DISABLED set
   [42287.957005] unexpected IRQ trap at vector 44

* In x86 using the 82591 interrupt controller, the following code is used to
  glue the irq_desc entries with the chip and the handler:

  --> arch/x86/kernel/i8259_32.c:
  void __init init_ISA_irqs (void)
  {
	int i;

  #ifdef CONFIG_X86_LOCAL_APIC
	init_bsp_APIC();
  #endif
	init_8259A(0);

	/* I think below initialization is redundant, and already statically done
	 * above */
	for (i = 0; i < NR_IRQS; i++) {
		irq_desc[i].status = IRQ_DISABLED;
		irq_desc[i].action = NULL;
		irq_desc[i].depth = 1;

		if (i < 16) {
			/*
			 * 16 old-style INTA-cycle interrupts:
			 */
			set_irq_chip_and_handler_name(i, &i8259A_chip,
						      handle_level_irq, "XT");
		} else {
			/*
			 * 'high' PCI IRQs filled in on demand
			 */
			irq_desc[i].chip = &no_irq_chip;
		}
	}

	/* Note: I've send a patch, and it's applied. Probably here's how
	 * the function will look in 2.6.26 */
	for (i = 0; i < 16; i++)
		set_irq_chip_and_handler_name(i, &i8259A_chip,
					      handle_level_irq, "XT");

  }  

* Important: handle_level_irq() - similar to __do_IRQ() - call below
  handle_IRQ_event() to execute the ISRs.

* When an interrupt handler must execute all the ISRs, it invokes the
  handle_IRQ_event() which essentially do the following:

  --> kernel/irq/handle.c:

  /**
   * handle_IRQ_event - irq action chain handler
   * @irq:	the interrupt number
   * @action:	the interrupt action chain for this irq
   *
   * Handles the action chain of an irq event
   */
  irqreturn_t handle_IRQ_event(unsigned int irq, struct irqaction *action)
  {
	irqreturn_t ret, retval = IRQ_NONE;
	unsigned int status = 0;


  1- Enable the local interrupts with the sti instruction. IRQF_DISABLED makes
     the interrupt handlers atomic by disabling another interrupts durring the
     handler lifetime. This is required by some kernel debugging options
     (LOCKDEP).
     
     	if (!(action->flags & IRQF_DISABLED))
		local_irq_enable_in_hardirq();

  2- Executes each interrupt service routine. As said above, an interrupt is
     considred handled iff at least one ISR executes and returns a IRQ_HANDLED
     flag.

	do {
		ret = action->handler(irq, action->dev_id);
		if (ret == IRQ_HANDLED)
			status |= action->flags;
		retval |= ret;
		action = action->next;
	} while (action);

  3- Disable local interrupts with cli.

     	/* Feed the randomness seed */
  	if (status & IRQF_SAMPLE_RANDOM)
		add_interrupt_randomness(irq);
	local_irq_disable();
  
  4- return retval which is 0 if no ISR recognized the interrupt, 1 otherwise.
  
  	return retval;
  }

* Before activating a device that's going to use an IRQ, driver invokes
  request_irq(). request_irq() creates a new irqaction and initialize it with
  parameter values.

  --> kernel/irq/manage.c:

  /**
   *	request_irq - allocate an interrupt line
   *	@irq: Interrupt line to allocate
   *	@handler: Function to be called when the IRQ occurs
   *	@irqflags: Interrupt type flags
   *	@devname: An ascii name for the claiming device
   *	@dev_id: A cookie passed back to the handler function
   *
   *	This call allocates interrupt resources and enables the
   *	interrupt line and IRQ handling. From the point this
   *	call is made your handler function may be invoked. Since
   *	your handler function must clear any interrupt the board
   *	raises, you must take care both to initialise your hardware
   *	and to set up the interrupt handler in the right order.
   *
   *	Dev_id must be globally unique. Normally the address of the
   *	device data structure is used as the cookie. Since the handler
   *	receives this value it makes sense to use it.
   *
   *	If your interrupt is shared you must pass a non NULL dev_id
   *	as this is required when freeing the interrupt.
   *
   *	Flags:
   *
   * 	IRQF_SHARED		Interrupt is shared
   *	IRQF_DISABLED	Disable local interrupts while processing
   *	IRQF_SAMPLE_RANDOM	The interrupt can be used for entropy
   *
   */
   int request_irq(unsigned int irq, irq_handler_t handler,
		   unsigned long irqflags, const char *devname, void *dev_id)
   {
	struct irqaction *action;
	int retval;

  1- Do some sanity checks on the parameters.

  #ifdef CONFIG_LOCKDEP
	/*
	 * Lockdep wants atomic interrupt handlers:
	 */
	irqflags |= IRQF_DISABLED;
  #endif
	/*
	 * Sanity-check: shared interrupts must pass in a real dev-ID,
	 * otherwise we'll have trouble later trying to figure out
	 * which interrupt is which (messes up the interrupt freeing
	 * logic etc).
	 */
	if ((irqflags & IRQF_SHARED) && !dev_id)
		return -EINVAL;
	if (irq >= NR_IRQS)
		return -EINVAL;
	if (irq_desc[irq].status & IRQ_NOREQUEST)
		return -EINVAL;
	if (!handler)
		return -EINVAL;

  2- Allocate and intialize a new irqaction.

	action = kmalloc(sizeof(struct irqaction), GFP_ATOMIC);
	if (!action)
		return -ENOMEM;

	action->handler = handler;
	action->flags = irqflags;
	cpus_clear(action->mask);
	action->name = devname;
	action->next = NULL;
	action->dev_id = dev_id;

	select_smp_affinity(irq);

  3- Some debugging needed by kernel debugging flags. As said by the
      CONFIG_DEBUG_IRQ explanations:

      ``Enable this to generate a spurious interrupt as soon as a shared
      	interrupt handler is registered, and just before one is
        deregistered. Drivers ought to be able to handle interrupts coming in at
      	those points; some don't and need to be caught.''

   #ifdef CONFIG_DEBUG_SHIRQ

	if (irqflags & IRQF_SHARED) {
		/*
		 * It's a shared IRQ -- the driver ought to be prepared for it
		 * to happen immediately, so let's make sure....
		 * We do this before actually registering it (Darwish:
		 * registering it using setup_irq() below), to make sure that
		 * a 'real' IRQ doesn't run in parallel with our fake.
		 */
		if (irqflags & IRQF_DISABLED) {
			unsigned long flags;

			local_irq_save(flags);
			handler(irq, dev_id);
			local_irq_restore(flags);
		} else
			handler(irq, dev_id);
	}
   #endif

  4- Register the irq using setup_irq(). 
      	retval = setup_irq(irq, action);
	if (retval)
		kfree(action);

	return retval;
  }	
  EXPORT_SYMBOL(request_irq);
   
* setup_irq() registers the irqaction by putting the irqaction in the proper 
  IRQ list.
 
  --> kernel/irq/manage.c:     
  /*
   * Internal function to register an irqaction - typically used to
   * allocate special interrupts that are part of the architecture.
   */
  int setup_irq(unsigned int irq, struct irqaction *new)
  {
	struct irq_desc *desc = irq_desc + irq;
	struct irqaction *old, **p;
	const char *old_name = NULL;
	unsigned long flags;
	int shared = 0;

      
  1- Some sanity checks on passed parameters:

	if (irq >= NR_IRQS)
		return -EINVAL;

	if (desc->chip == &no_irq_chip)
		return -ENOSYS;

  3- Feed the system random pool if IRQF_SAMPLE_RANDOM is set.

      	if (new->flags & IRQF_SAMPLE_RANDOM) {
		/*
		 * This function might sleep, we want to call it first,
		 * outside of the atomic block.
		 * Yes, this might clear the entropy pool if the wrong
		 * driver is attempted to be loaded, without actually
		 * installing a new handler, but is this really a problem,
		 * only the sysadmin is able to do this.
		 */
		rand_initialize_irq(irq);
	}

  4- Begin a series of checks to see if the passed irqaction can be added to
     the respective IRQ list with no problems. Devices that share an IRQ line
     must be designed to be capable of sharing IRQs in its design (declared
     by the driver by setting the IRQF_SHARED flag). Also they must have the
     same type of interrupts triggering (level-triggered or edge-triggered).

	/*
	 * The following block of code has to be executed atomically
	 */
	spin_lock_irqsave(&desc->lock, flags);
	p = &desc->action;
	old = *p;
	/* Darwish: If an irqaction list already exists for this IRQ line */
	if (old) {
		/*
		 * Can't share interrupts unless both agree to and are
		 * the same type (level, edge, polarity). So both flag
		 * fields must have IRQF_SHARED set and the bits which
		 * set the trigger type must match.
		 */
		if (!((old->flags & new->flags) & IRQF_SHARED) ||
		    ((old->flags ^ new->flags) & IRQF_TRIGGER_MASK)) {
			old_name = old->name;
			goto mismatch;
		}

  5- Examine if sharing interrupts must agree on the same CPU (using a kernel
     config option)

	 #if defined(CONFIG_IRQ_PER_CPU)
		/* All handlers must agree on per-cpuness */
		if ((old->flags & IRQF_PERCPU) !=
		    (new->flags & IRQF_PERCPU))
			goto mismatch;
	 #endif

  6- At last, this irqaction can be added to list. let p represent the last
     node in queue.

      		/* add new interrupt at end of irq queue */
		do {
			/* Darwish: don't be fuzzled here, p holds the address of
			 * the NULL element and don't equal to null */
			p = &old->next;
			old = *p;
		} while (old);
		shared = 1;
	  }
	  *p = new;

  7- the new irqaction added may need our irq_chip to change some of its
     flags.

	/* Exclude IRQ from balancing */
	if (new->flags & IRQF_NOBALANCING)
	   	desc->status |= IRQ_NO_BALANCING;

  8- If this is the first irqaction in our IRQ list (!shared), do some
     irq_desc[IRQ] initializations:

 	if (!shared) {

      	 #if defined(CONFIG_IRQ_PER_CPU)
		if (new->flags & IRQF_PERCPU)
			desc->status |= IRQ_PER_CPU;
         #endif
		
		irq_chip_set_defaults(desc->chip);

   9- Then do some needed updates:

       	  	/* Setup the type (level, edge polarity) if configured */
		if (new->flags & IRQF_TRIGGER_MASK) {
			if (desc->chip && desc->chip->set_type)
				desc->chip->set_type(irq,
						new->flags & IRQF_TRIGGER_MASK);
			else
				/*
				 * IRQF_TRIGGER_* but the PIC does not support
				 * multiple flow-types?
				 */
				printk(KERN_WARNING "No IRQF_TRIGGER set_type "
				       "function for IRQ %d (%s)\n", irq,
				       desc->chip ? desc->chip->name :
				       "unknown");
		} else
			compat_irq_chip_set_default_handler(desc);

		desc->status &= ~(IRQ_AUTODETECT | IRQ_WAITING |
				  IRQ_INPROGRESS);

		if (!(desc->status & IRQ_NOAUTOEN)) {
			desc->depth = 0;
			desc->status &= ~IRQ_DISABLED;
			if (desc->chip->startup)
				desc->chip->startup(irq);
			else
				desc->chip->enable(irq);
		} else
			/* Undo nested disables: */
			desc->depth = 1;
	  }       	  

	
  10- At last, do some little initializations

	  /* Reset broken irq detection when installing new handler */
	  desc->irq_count = 0;
	  desc->irqs_unhandled = 0;
	  spin_unlock_irqrestore(&desc->lock, flags);

	  new->irq = irq;
	  register_irq_proc(irq);
	  new->dir = NULL;
	  register_handler_proc(irq, new);

	  return 0;

  11- and in case of a mismatch between the irqaction and the IRQ line list:
	
	  mismatch:
	  #ifdef CONFIG_DEBUG_SHIRQ
	  if (!(new->flags & IRQF_PROBE_SHARED)) {
		printk(KERN_ERR "IRQ handler type mismatch for IRQ %d\n", irq);
		if (old_name)
			printk(KERN_ERR "current handler: %s\n", old_name);
		dump_stack();
	  }
	  #endif
	  spin_unlock_irqrestore(&desc->lock, flags);
	  return -EBUSY;
	}

* Device drivers remove an interrupt handler using the free_irq() method. Code
  needs no comments, as it's very easy to follow:

  --> kernel/irq/manage.c:
  void free_irq(unsigned int irq, void *dev_id)
  {
	struct irq_desc *desc;
	struct irqaction **p;
	unsigned long flags;
	irqreturn_t (*handler)(int, void *) = NULL;

	WARN_ON(in_interrupt());
	if (irq >= NR_IRQS)
		return;

	desc = irq_desc + irq;
	spin_lock_irqsave(&desc->lock, flags);
	p = &desc->action;
	for (;;) {
		struct irqaction *action = *p;

		if (action) {
			struct irqaction **pp = p;

			p = &action->next;
			if (action->dev_id != dev_id)
				continue;

			/* Found it - now remove it from the list of entries */
			*pp = action->next;

			if (!desc->action) {
				desc->status |= IRQ_DISABLED;
				if (desc->chip->shutdown)
					desc->chip->shutdown(irq);
				else
					desc->chip->disable(irq);
			}
			spin_unlock_irqrestore(&desc->lock, flags);
			unregister_handler_proc(irq, action);

			/* Make sure it's not being used on another CPU */
			synchronize_irq(irq);
			if (action->flags & IRQF_SHARED)
				handler = action->handler;
			kfree(action);
			return;
		}
		printk(KERN_ERR "Trying to free already-free IRQ %d\n", irq);
		spin_unlock_irqrestore(&desc->lock, flags);
		return;
	}
#ifdef CONFIG_DEBUG_SHIRQ
	if (handler) {
		/*
		 * It's a shared IRQ -- the driver ought to be prepared for it
		 * to happen even now it's being freed, so let's make sure....
		 * We do this after actually deregistering it, to make sure that
		 * a 'real' IRQ doesn't run in parallel with our fake
		 */
		handler(irq, dev_id);
	}
#endif
}

*********************************************************************************
Softirqs and Tasklets:
----------------------

* Softirqs are statically allocated while Tasklets can be allocated
  dynamically. Softirqs can concurrently run on different CPUs, so it needs to be
  reentrant. Tasklets need not be reentrant since they are serialized.

  Note: deferrable function that has been activated by a given CPU must be 
  	executed on the same CPU

* Softirqs usually run in an interrupt context. As will be noted in the end of 
  this section, After the end of handling each hardirq, kernel checks for 
  pending softirqs by calling invoke_softirq()/do_sofitirq().
     
  As pointed to me by Dmitry Adamushko:
  The common sequence is ... -> do_IRQ() --> irq_exit() --> invoke_softirq()

  --> kernel/softirq.c:
  #ifdef __ARCH_IRQ_EXIT_IRQS_DISABLED
  # define invoke_softirq()	__do_softirq()
  #else
  # define invoke_softirq()	do_softirq()
  #endif

* Linux uses only six kinds of software IRQs (32 are available), defined as
  follows:

  Softirq    	 Index/Priority		Description
-----------------------------------------------------
  HI_SOFTIRQ 	      0   	    Handles high priority tasklets
  TIMER_SOFTIRQ	      1		    Tasklets related to timer interrupts
  NET_TX_SOFTIRQ      2		    Transmets packets to network cards.
  NET_RX_SOFTIRQ      3		    Recievices packets from network cards.
  SCSI_SOFTIRQ	      4		    Post-interrupt processing of SCSI commands
  TASKLET_SOFTIRQ     5		    Handles regular tasklets

* Main data structure used to represent softirqs is the softirq_vec array.

  --> kernel/softirq.c:
  static struct softirq_action softirq_vec[32] __cacheline_aligned_in_smp;

* the softirq_action is defined as follows:

  --> include/linux/interrupt.h:
  /* PLEASE, avoid to allocate new softirqs, if you need not _really_ high
   frequency threaded job scheduling. For almost all the purposes
   tasklets are more than enough. F.e. all serial device BHs et
   al. should be converted to tasklets, not to softirqs.
  */

  enum
  {
	HI_SOFTIRQ=0,
	TIMER_SOFTIRQ,
	NET_TX_SOFTIRQ,
	NET_RX_SOFTIRQ,
	BLOCK_SOFTIRQ,
	TASKLET_SOFTIRQ,
	SCHED_SOFTIRQ,
  #ifdef CONFIG_HIGH_RES_TIMERS
	HRTIMER_SOFTIRQ,
  #endif
	RCU_SOFTIRQ, 	/* Preferable RCU should always be the last softirq */
  };

  /*
   * softirq mask and active fields moved to irq_cpustat_t in
   * asm/hardirq.h to get better cache usage.  KAO
   */
  struct softirq_action
  {
	void	(*action)(struct softirq_action *); /* Function to be called */
	void	*data;		 		    /* Data maybe needed by above method */
  };

* Their exist a 32bit field named preempt_count exist in every thread_info
  structure. It's job is to keep track of kernel preemption and nesting of kernel
  control pathes.

  --> include/asm/thread_info.h::struct thread_info: 
  int			preempt_count;	/* 0 => preemptable, <0 => BUG */

  its bits are used as follows:
  0->7	      	   Preemption counter (max = 255)
    - keeps track of how many times kernel preemption has been disabled on Local
      CPU.
      0: preemption is not disabled at all (100% enabled).

  8->15		   Softirq counter (max = 255)
    - specifies how many levels deep disabling of deferrable functions is. 
      0: means deferrable function is enabled.

  16->27	   Hardirq counter (max = 4096)
    - Number of nested interrupt handlers on the local CPU. modified by
      irq_enter() and irq_exit().

  28		   PREEMPT_ACTIVE flag

* in_interrupt() macros make use of the above fields to say if the current
  context is in an interrupt context. 

  --> include/linux/hardirq.h:
  /*
   * We put the hardirq and softirq counter into the preemption
   * counter. The bitmask has the following meaning:
   *
   * - bits 0-7 are the preemption count (max preemption depth: 256)
   * - bits 8-15 are the softirq count (max # of softirqs: 256)
   *
   * The hardirq count can be overridden per architecture, the default is:
   *
   * - bits 16-27 are the hardirq count (max # of hardirqs: 4096)
   * - ( bit 28 is the PREEMPT_ACTIVE flag. )
   *
   * PREEMPT_MASK: 0x000000ff
   * SOFTIRQ_MASK: 0x0000ff00
   * HARDIRQ_MASK: 0x0fff0000
   */
  #define PREEMPT_BITS	8
  #define SOFTIRQ_BITS	8

  /* this can be set architecture specific */
  #ifndef HARDIRQ_BITS
  #define HARDIRQ_BITS	12
  #endif

  #define PREEMPT_SHIFT	0
  #define SOFTIRQ_SHIFT	(PREEMPT_SHIFT + PREEMPT_BITS)
  #define HARDIRQ_SHIFT	(SOFTIRQ_SHIFT + SOFTIRQ_BITS)

  #define PREEMPT_OFFSET	(1UL << PREEMPT_SHIFT)
  #define SOFTIRQ_OFFSET	(1UL << SOFTIRQ_SHIFT)
  #define HARDIRQ_OFFSET	(1UL << HARDIRQ_SHIFT)

  --> include/linux/preempt.h:
  #define preempt_count()	(current_thread_info()->preempt_count)


  --> kernel/sched.c:
  void add_preempt_count(int val)
  {
	/*
	 * Underflow?
	 */
	if (DEBUG_LOCKS_WARN_ON((preempt_count() < 0)))
		return;
	preempt_count() += val;
	/*
	 * Spinlock count overflowing soon?
	 */
	DEBUG_LOCKS_WARN_ON((preempt_count() & PREEMPT_MASK) >=
				PREEMPT_MASK - 10);
  }
  EXPORT_SYMBOL(add_preempt_count);

  --> include/linux/hardirq.h:
  /*
   * It is safe to do non-atomic ops on ->hardirq_context,
   * because NMI handlers may not preempt and the ops are
   * always balanced, so the interrupted value of ->hardirq_context
   * will always be restored.
   */
  #define __irq_enter()					\
	do {						\
		account_system_vtime(current);		\
		add_preempt_count(HARDIRQ_OFFSET);	\
		trace_hardirq_enter();			\
	} while (0)

  /*
   * Exit irq context without processing softirqs:
   */
  #define __irq_exit()					\
	do {						\
		trace_hardirq_exit();			\
		account_system_vtime(current);		\
		sub_preempt_count(HARDIRQ_OFFSET);	\
	} while (0)

  /* If any field > 0, the '& **_MASK' field won't return 0 */
  #define hardirq_count()	(preempt_count() & HARDIRQ_MASK)
  #define softirq_count()	(preempt_count() & SOFTIRQ_MASK)
  #define irq_count()	(preempt_count() & (HARDIRQ_MASK | SOFTIRQ_MASK))

  /*
   * Are we doing bottom half or hardware interrupt processing?
   * Are we in a softirq context? Interrupt context?
   */
  #define in_irq()		(hardirq_count())
  #define in_softirq()		(softirq_count())
  #define in_interrupt()	(irq_count())

* irq_stat array includes NR_CPUS entries, one for each CPU. Each entry of
  irq_cpustat_t includes few counters and flags to keep track of what each CPU is
  doing regarding to interrupts (see the comment above 'struct softirq_action').

* open_softirq takes care of softirq initialization. It limits itself to
  initialize the proper entry of softirq_vec array.

  --> kernel/softirq.c:
  void open_softirq(int nr, void (*action)(struct softirq_action*), void *data)
  {
	softirq_vec[nr].data = data;
	softirq_vec[nr].action = action;
  }

  Example of open_softirq code (used to initialize timer softirq): 

  --> kernel/timer.c:
  void __init init_timers(void)
  {
	int err = timer_cpu_notify(&timers_nb, (unsigned long)CPU_UP_PREPARE,
				(void *)(long)smp_processor_id());

	init_timer_stats();

	BUG_ON(err == NOTIFY_BAD);
	register_cpu_notifier(&timers_nb);
	open_softirq(TIMER_SOFTIRQ, run_timer_softirq, NULL);
  }


* Softirqs are activated by means of raise_softirq() method. it got called by
  different methods of the kernel.

  --> kernel/softirq.c:
  void fastcall raise_softirq(unsigned int nr)
  {
	unsigned long flags;

	local_irq_save(flags);
	/* Marks the softirq as pending */
	raise_softirq_irqoff(nr);
	local_irq_restore(flags);
  }

  inline fastcall void raise_softirq_irqoff(unsigned int nr)
  {
	/* 
	 * Mark the softirq as pending (as we do in real interrupts)
	 */
	__raise_softirq_irqoff(nr);

	/*
	 * If we're in an interrupt or softirq, we're done
	 * (this also catches softirq-disabled code). We will
	 * actually run the softirq once we return from
	 * the irq or softirq.
	 *
	 * Otherwise we wake up ksoftirqd to make sure we
	 * schedule the softirq soon.
	 */
	if (!in_interrupt())
		wakeup_softirqd();
  }

  --> include/linux/interrupt.h:
  #define __raise_softirq_irqoff(nr) do { or_softirq_pending(1UL << (nr)); }
  	  			     while (0)
  
  --> kernel/softirq.c:
  /*
   * we cannot loop indefinitely here to avoid userspace starvation,
   * but we also don't want to introduce a worst case 1/HZ latency
   * to the pending events, so lets the scheduler to balance
   * the softirq load for us (by calling wake_up_process).
   */
  static inline void wakeup_softirqd(void)
  {
	/* Interrupts are disabled: no need to stop preemption */
	struct task_struct *tsk = __get_cpu_var(ksoftirqd);

	if (tsk && tsk->state != TASK_RUNNING)
		wake_up_process(tsk);
  }


* If pending softirqs are detected using the local_softirq_pending() method,
  kernel invokes do_softirq() to take care of them. Function do the following:
  
  --> kernel/softirq.c:

  /* Sometimes architectures provide an optimized version */  
  #ifndef __ARCH_HAS_DO_SOFTIRQ

  asmlinkage void do_softirq(void)
  {
	__u32 pending;
	unsigned long flags;
	
	/* indicates do_softirq has been invoked from an interrupt context or
	 * softirqs are disabled (see __do_softirq()). */
	if (in_interrupt())
		return;

	local_irq_save(flags);

	pending = local_softirq_pending();

	if (pending)
		__do_softirq();

	local_irq_restore(flags);
  }
  EXPORT_SYMBOL(do_softirq);

  #endif

* __do_softirq reads the softirq bit mask of the local CPU and executes all the
  deferrable functions corresponding to every set bit. While executing a softirq
  function, new pending softirqs may arise. To insure a low latency, __do_softirq
  keep running until all pending softirqs have been executed.. This may affects
  fairness, leading to a locked user space process, to deal with this
  __do_softirq() performs a fixed number of iterations then returns as follows:

  --> kernel/softirq.c:
  /*
   * We restart softirq processing MAX_SOFTIRQ_RESTART times,
   * and we fall back to softirqd after that.
   *
   * This number has been established via experimentation.
   * The two things to balance is latency against fairness -
   * we want to handle softirqs as soon as possible, but they
   * should not be able to lock up the box.
   */
  #define MAX_SOFTIRQ_RESTART 10

  asmlinkage void __do_softirq(void)
  {
	struct softirq_action *h;
	__u32 pending;
	int max_restart = MAX_SOFTIRQ_RESTART;
	int cpu;

  1- copies the softirq bit mask of local CPU to pending variable.

  	pending = local_softirq_pending();

  2- disable deferrable functions from now on, so every new instance of
     do_softirq() will exit at the in_interrupt() check. This is done to
     serialize the executiion of softirqs (same softirqs do not interleave 
     over a single CPU - as in hardware interrupts).

     	__local_bh_disable((unsigned long)__builtin_return_address(0));
	trace_softirq_enter();

	cpu = smp_processor_id();

  3- Clear the softirq bitmap of the local CPU so new softirqs can be activated.

  	/* Reset the pending bitmask before enabling irqs */
	set_softirq_pending(0);

  4- Enable local interrupts, which makes new deferrable methods available in
     most cases.
  
	local_irq_enable();

  5- For each bit in the local `pending' variable, executes the corresponding
     softirq function.

     	h = softirq_vec;
	do {
		if (pending & 1) {
			h->action(h);
			rcu_bh_qsctr_inc(cpu);
		}
		h++;
		pending >>= 1;
	} while (pending);

   6- Disable local interrupts
      
        local_irq_disable();

   7- Copies the softirq bit mask of local CPU again to the local pending
      variable. Do this till the bit mask is empty or till the iterations consume
      our max iterations variable available.

      	pending = local_softirq_pending();
	if (pending && --max_restart)
		goto restart;

   8- If there are more pending softirqs after we finished our maximum allowed
      number of servicing tasklets, execute wakeup_softirqd() to wakeup the
      kernel thread that takes care of softirqs for the local CPU (scheduled by
      the scheduler).

      	if (pending)
		wakeup_softirqd();

	trace_softirq_exit();

   9- Reenable deferrable functions.

      	_local_bh_enable();
   }

IMPORTANT CLARIFICATION: 
------------------------
* After all this discussion, Where does the softirqs begin invocation from ?

  1- the MOST IMPORTANT invocation is in _interrupt context_, after the 
     end of handling a hardirq by calling irq_exit(): 
     
     As pointed to me by Dmitry Adamushko:
     The common sequence is ... -> do_IRQ() --> irq_exit() --> invoke_softirq()
     
  2- ksoftirq, which calls do_softirq() and is waken up by wakup_softirqd().
  3- by do_softirq() invoking itself when it iterates 10 times to execute
     pending softirqs and still others remain. so it calls
     wakeup_softirq() to let the thread take the job at suitable
     time determined by thh scheduler.
  4- by local_bh_enable(), which enable bottom halves.


* Read section 4.7.1.5 of ULK _CAREFULLY_.
  "The ksoftirqd/n kernel threads try to solve this difficult trade-off problem. The
   do_softirq( ) function determines what softirqs are pending and executes their
   functions. After a few (MAX_SOFTIRQ_RESTART) iterations, if the flow of
   softirqs does not stop, the function wakes up the kernel thread and terminates.
   The kernel thread has low priority, so user programs have a chance to run;
   but if the machine is idle, the pending softirqs are executed quickly."

   Real example from LDD3 book:

     $ cat /proc/jitasklet
     time   delta inirq      pid   cpu command
    6076139    0     0      4370   0   cat
    6076140    1     1      4368   0   cc1
    6076141    1     1      4368   0   cc1
    6076141    0     1         2   0   ksoftirqd/0
    6076141    0     1         2   0   ksoftirqd/0
    6076141    0     1         2   0   ksoftirqd/0

   A kernel module that creates proc file /proc/jitasklets. When opening the proc
   file, a function named jit_tasklet is called. method creates and initializees
   a tasklet with a function named jit_tasklet_fn. It then types the time pid and 
   command of current process (will always be cat) and sleeps. 

   When tasklet interrupt handlers got executed. our jit_tasklet_fn will be
   executed to. this method's job is to type the current time and current process
   in a newline of proc file. Method does this for each new tasklets triggered
   till a specified number of loops after that it exits.

   So output presents the times (in jiffies) tasklets/softirqs got
   trigerred. What's cool about those data ? ...
   As you may notice, the cpu was busy executing usermode apps in the first 3
   jiffies. so softirqs was triggered only "at the end of each time tick". At
   jiffie 41 do_softirq() waked up ksoftirqd and it seems that no userspace apps
   needed the processor so the kernel thread kept servicing the tasklets.
   
Tasklets:
---------

* Tasklets are the preferred way to implement deferrable functions in I/O
  devices. Tasklets are built over two sysirqs HI_SOFTIRQ,
  TASKLET_SOFTIRQ. Several tasklets maybe associated with the same IRQ. each
  tasklet carrying its own function. 

  You can understand how tasklets are used in drivers from the following example.
  The logic of below code is explained in above article.
  The above table is the result from opening proc files produced by below code:

  /*
   * The code is derived from the book "Linux Device Drivers" by Alessandro
   * Rubini and Jonathan Corbet, published by O'Reilly & Associates
   */

  #include <linux/module.h>
  #include <linux/init.h>
  #include <linux/kernel.h>
  #include <linux/proc_fs.h>
  #include <linux/types.h>
  #include <linux/spinlock.h>
  #include <linux/interrupt.h>
  #include <asm/hardirq.h>

  MODULE_LICENSE("Dual BSD/GPL");

  #define JIT_ASYNC_LOOPS 5

  /* This data structure used as "data" for the timer and tasklet functions */
  struct jit_data {
	struct tasklet_struct tlet;
	int hi;                      /* tasklet or tasklet_hi */
	wait_queue_head_t wait;
	unsigned long prevjiffies;
	unsigned char *buf;
	int loops;
  };

  void jit_tasklet_fn(unsigned long arg)
  {
	struct jit_data *data = (struct jit_data *)arg;
	unsigned long j = jiffies;
	data->buf += sprintf(data->buf, "%9li  %3li     %i    %6i   %i   %s\n",
			     j, j - data->prevjiffies, in_interrupt() ? 1 : 0,
			     current->pid, smp_processor_id(), current->comm);

	if (--data->loops) {
		data->prevjiffies = j;
		if (data->hi)
			tasklet_hi_schedule(&data->tlet);
		else
			tasklet_schedule(&data->tlet);
	} else {
		wake_up_interruptible(&data->wait);
	}
  }

  /* the /proc function: allocate everything to allow concurrency */
  int jit_tasklet(char *buf, char **start, off_t offset,
	        int len, int *eof, void *arg)
  {
	struct jit_data *data;
	char *buf2 = buf;
	unsigned long j = jiffies;
	long hi = (long)arg;

	data = kmalloc(sizeof(*data), GFP_KERNEL);
	if (!data)
		return -ENOMEM;

	init_waitqueue_head (&data->wait);

	/* write the first lines in the buffer */
	buf2 += sprintf(buf2, "   time   delta  inirq    pid   cpu command\n");
	buf2 += sprintf(buf2, "%9li  %3li     %i    %6i   %i   %s\n",
			j, 0L, in_interrupt() ? 1 : 0,
			current->pid, smp_processor_id(), current->comm);

	/* fill the data for our tasklet function */
	data->prevjiffies = j;
	data->buf = buf2;
	data->loops = JIT_ASYNC_LOOPS;
	
	/* register the tasklet */
	tasklet_init(&data->tlet, jit_tasklet_fn, (unsigned long)data);
	data->hi = hi;
	if (hi)
		tasklet_hi_schedule(&data->tlet);
	else
		tasklet_schedule(&data->tlet);

	/* wait for the buffer to fill */
	wait_event_interruptible(data->wait, !data->loops);

	if (signal_pending(current))
		return -ERESTARTSYS;
	buf2 = data->buf;
	kfree(data);
	*eof = 1;
	return buf2 - buf;
  }

  int __init jit_init(void)
  {
	create_proc_read_entry("jitasklet", 0, NULL, jit_tasklet, NULL);
	create_proc_read_entry("jitasklethi", 0, NULL, jit_tasklet, (void *)1);

	return 0; 
  }

  void __exit jit_cleanup(void)
  {
	remove_proc_entry("jitasklet", NULL);
	remove_proc_entry("jitasklethi", NULL);
  }

  module_init(jit_init);
  module_exit(jit_cleanup);

* Tasklets and high priority tasklets are stored in the tasklet_vec and
  tasklet_hi_vec which are percpu element.

  --> kernel/softirq.c:
  /* Some compilers disobey section attribute on statics when not
   *  initialized -- RR */
  static DEFINE_PER_CPU(struct tasklet_head, tasklet_vec) = { NULL };
  static DEFINE_PER_CPU(struct tasklet_head, tasklet_hi_vec) = { NULL };

  /* Tasklets */
  struct tasklet_head
  {
	struct tasklet_struct *list;
  };

* Tasklet descriptor is a data structure of type tasklet_struct, whose fields are
  shown in:
  
  --> include/linux/interrupt.h:
  /* Tasklets --- multithreaded analogue of BHs.

   Main feature differing them of generic softirqs: tasklet
   is running only on one CPU simultaneously.

   Main feature differing them of BHs: different tasklets
   may be run simultaneously on different CPUs.

   Properties:
   * If tasklet_schedule() is called, then tasklet is guaranteed
     to be executed on some cpu at least once after this.
   * If the tasklet is already scheduled, but its excecution is still not
     started, it will be executed only once.
   * If this tasklet is already running on another CPU (or schedule is called
     from tasklet itself), it is rescheduled for later.
   * Tasklet is strictly serialized wrt itself, but not
     wrt another tasklets. If client needs some intertask synchronization,
     he makes it with spinlocks.
   */

   struct tasklet_struct
   {
	struct tasklet_struct *next;
	unsigned long state;
	atomic_t count;
	void (*func)(unsigned long);
	unsigned long data;
   };

* state field can include two flags:

  --> include/linux/interrupt.h:
  enum
  {
	/* Darwish: This means the tasklet is pending (scheduled for execution),
  	 * it also means that tasklet descriptor is inserted in the tasklet_vec
   	 * or tasklet_hi_vec vectors */
	TASKLET_STATE_SCHED,	/* Tasklet is scheduled for execution */
	/* Indicates tasklet is being executed. Not used on a uniprocessor system
  	 * since we have no interest to know wheter a specific tasklet is running */
	TASKLET_STATE_RUN	/* Tasklet is running (SMP only) */
  };
  
* A device driver creates a new tasklet descriptor, then initializes it using the
  tasklet_inti(). To activate the tasklet you should invoke either
  tasklet_schedule or tasklet_hi_schedule. They perform the following actions:

  --> include/linux/interrupt.h:
  static inline void tasklet_schedule(struct tasklet_struct *t)
  {
	/* If the task is already scheduled, don't do anything */
	if (!test_and_set_bit(TASKLET_STATE_SCHED, &t->state))
		__tasklet_schedule(t);
  }

  --> kernel/softirq.c:
  void fastcall __tasklet_schedule(struct tasklet_struct *t)
  {
	unsigned long flags;

  1- Disable the interrupts, we don't want another scheduled tasklet that can
     break the tasklet_vec list cause of race conditions.

	local_irq_save(flags);

  2- Put the node in the beginning of the tasklet_vec list

	t->next = __get_cpu_var(tasklet_vec).list;
	__get_cpu_var(tasklet_vec).list = t;

  3- Activate the tasklet with. It's definition is found above (basically it
     runs the ksoftirqd thread if appropriate).

	raise_softirq_irqoff(TASKLET_SOFTIRQ);

  4- Restore the interrupts.

	local_irq_restore(flags);
  }
  EXPORT_SYMBOL(__tasklet_schedule);

* Tasklets got executed using the tasklet_hi_action() or tasklet_action().

  --> kernel/softirq.c:
  void __init softirq_init(void)
  {
	open_softirq(TASKLET_SOFTIRQ, tasklet_action, NULL);
	open_softirq(HI_SOFTIRQ, tasklet_hi_action, NULL);
  }

  static void tasklet_action(struct softirq_action *a)
  {
	struct tasklet_struct *list;

  1- Disables local interrupts.	Stores the address of the list pointed by
  tasklet_vec[PROCESSOR_ID] in list local variable. then empties this list. then
  reenable interrupts.

  	local_irq_disable();
     	/* tasklet_vec is a tasklist_head structures contains only a one element,
     	 * a list_head */
     	list = __get_cpu_var(tasklet_vec).list;
	__get_cpu_var(tasklet_vec).list = NULL;
  	local_irq_enable();

  2- For each tasklet descriptor in the list

     while (list) {
		struct tasklet_struct *t = list;

		list = list->next;

     a- In a SMP system, checks the TASKLET_STATE_RUN flag. Also, if the flag
     	is clear, set it.

	--> include/linux/interrupt.h:
	#ifdef CONFIG_SMP
	static inline int tasklet_trylock(struct tasklet_struct *t)
	{
	   return !test_and_set_bit(TASKLET_STATE_RUN, &(t)->state);
	}

	static inline void tasklet_unlock(struct tasklet_struct *t)
	{
	   smp_mb__before_clear_bit(); 
	   clear_bit(TASKLET_STATE_RUN, &(t)->state);
	}
	#else
	#define tasklet_trylock(t) 1
	#define tasklet_unlock_wait(t) do { } while (0)
	#define tasklet_unlock(t) do { } while (0)
	#endif		

	the bit is not set, no tasklet is running beside us, continue our job
	dude.

		if (tasklet_trylock(t)) {
		   	/* A counter to monitor if tasklets are disabled or not
			 * used by tasklet_enable, tasklet_disable */		
			if (!atomic_read(&t->count)) {

				/* Ooh, we are going to run and our state is not SCHED, 
			 	 * mm BUG() you idiot */
				 if (!test_and_clear_bit(TASKLET_STATE_SCHED, &t->state))
					BUG();

				 /* Execution the function */		
				 t->func(t->data);

				 /* Now we're fine, continue examining others */
				 tasklet_unlock(t);
				 continue;
			}
			
			tasklet_unlock(t);
		}

		/*
		 * If we're here, then the tasklet wasn't lucky enough to
		 * be executed, so it get rescheduled.
		 * mmm, let's defere this one by reinserting it in the
		 * tasklet_vec list. In this way executing of a tasklet is
		 * deferred till no other tasklets of the same type are running
		 * on other CPUs or till tasklets got enabled.
		 *
		 * As explained to me by Ingo, __tasklet_schedule() wasn't called
		 * cause it calls raise_softirq_irqoff and not __raise_softirq_irqoff
		 * which will make tasklets be able to schedule themselves indefinitely.
		 */

		local_irq_disable();
		t->next = __get_cpu_var(tasklet_vec).list;
		__get_cpu_var(tasklet_vec).list = t;
		__raise_softirq_irqoff(TASKLET_SOFTIRQ);
		local_irq_enable();
	}
     }


Workqueues:
-----------

* Tasklets (deferrable functions) and workqueus are quite different. Deferrable
  functions run in interrupt context while work queues work in a process
  context. Running in a process context is the only way so the code can be able
  to block. 

  Why interrupt context can't block ??.
  A: No process switch can take place in interrupt context, so if an interrupt
     got blocked, all the system got locked.


  A deferrable function can't make any assumption of the process currenty
  running. On the other hand, a function in a work queue is executed by a kernel
  thread. 

* Workqueus is represented by a descriptor called workqueue_struct. It contains
  among other things an array of NR_CPUS elements. Each element is of type
  cpu_workqueue_struct.

  --> kernel/workqueue.c:
  /*
   * The externally visible workqueue abstraction is an array of
   * per-CPU workqueues:
   */
  struct workqueue_struct {
  	/* To be extracted by per_cpu_ptr, see below */
	struct cpu_workqueue_struct *cpu_wq;
	const char *name;
	struct list_head list;		/* Empty if single thread */
	int freezeable;			/* Freeze the thread during suspend */

  };

  /*
   * The per-CPU workqueue (if single thread, we always use the first
   * possible cpu).
   *
   * The sequence counters are for flush_scheduled_work().  It wants to wait
   * until all currently-scheduled works are completed, but it doesn't
   * want to be livelocked by new, incoming ones.  So it waits until
   * remove_sequence is >= the insert_sequence which pertained when
   * flush_scheduled_work() was called.
   */
   struct cpu_workqueue_struct {

	spinlock_t lock;		/* spinlock for structure protection */

	long remove_sequence;		/* Least-recently added (next to run) - Sequence number used by flush_workqueue() */
	long insert_sequence;		/* Next to add - Sequence number used by flush_workqueue() */

	struct list_head worklist;	/* Head of the list of pending functionss */
	wait_queue_head_t more_work;    /* Wait queue where the worker thread waiting for more work to be done sleeps */
	wait_queue_head_t work_done;    /* Wait queue where the processes waiting for the work queue to be flushed sleep */

	struct workqueue_struct *wq;	/* Pointer to the work_queue structure containing this descriptor */
	struct task_struct *thread;     /* Process descriptor pointer of the worker thread of the structure */

	int run_depth;			/* Detect run_workqueue() recursion depth */

    } ____cacheline_aligned;

* above worklist field is a head of a doubly linked list, collecting the pending
  functions of the workqueue. Every pending function is represented by a
  work_struct data structure. 

  --> include/linux/workqueue.h:
  struct work_struct {
	atomic_long_t data;
  #define WORK_STRUCT_PENDING 0		/* T if work item pending execution */
  #define WORK_STRUCT_NOAUTOREL 1	/* F if work item automatically released on exec */
  #define WORK_STRUCT_FLAG_MASK (3UL)
  #define WORK_STRUCT_WQ_DATA_MASK (~WORK_STRUCT_FLAG_MASK)
	struct list_head entry;		/* pointers to next and previous elements of the list of pending functions */
	work_func_t func;		/* Address of pending functin */
  };

* create_workqueue() returns the address a newly created workqueue structure with
  a name as passed in the parameter. 

  --> include/linux/workqueue.h:
  #define create_workqueue(name) __create_workqueue((name), 0, 0)

  --> kernel/workqueue.c:

  /* All the per-cpu workqueues on the system, for hotplug cpu to add/remove
   threads to each one as cpus come/go. */
  static DEFINE_MUTEX(workqueue_mutex);
  static LIST_HEAD(workqueues);

  static int singlethread_cpu;

  struct workqueue_struct *__create_workqueue(const char *name,
					    int singlethread, int freezeable)
  {
	int cpu, destroy = 0;
	struct workqueue_struct *wq;
	struct task_struct *p;

	wq = kzalloc(sizeof(*wq), GFP_KERNEL);
	if (!wq)
		return NULL;

	wq->cpu_wq = alloc_percpu(struct cpu_workqueue_struct);
	if (!wq->cpu_wq) {
		kfree(wq);
		return NULL;
	}

	wq->name = name;
	mutex_lock(&workqueue_mutex);
	if (singlethread) {
		INIT_LIST_HEAD(&wq->list);
		p = create_workqueue_thread(wq, singlethread_cpu, freezeable);
		if (!p)
			destroy = 1;
		else
			wake_up_process(p);
	} else {
	        /* Why this ?. We may know later */
		list_add(&wq->list, &workqueues);
		for_each_online_cpu(cpu) {
			p = create_workqueue_thread(wq, cpu, freezeable);
			if (p) {
			        /* Bind a thread to each CPU */
				kthread_bind(p, cpu);
				wake_up_process(p);
			} else
				destroy = 1;
		}
	}
	mutex_unlock(&workqueue_mutex);

	/*
	 * Was there any error during startup? If yes then clean up:
         */
        if (destroy) {
                destroy_workqueue(wq);
                wq = NULL;
        }
        return wq;
  }
	
* create_workqueue_thread() creates NR_CPUS worker threads. named as foo/0,
  foo/1, and so on. 

  static struct task_struct *create_workqueue_thread(struct workqueue_struct *wq,
						   int cpu, int freezeable)
  {
	/* per_cpu_ptr extracts only the value related to passed CPU number */
	struct cpu_workqueue_struct *cwq = per_cpu_ptr(wq->cpu_wq, cpu);
	struct task_struct *p;

	spin_lock_init(&cwq->lock);
	cwq->wq = wq;
	cwq->thread = NULL;
	cwq->insert_sequence = 0;
	cwq->remove_sequence = 0;
	cwq->freezeable = freezeable;
	/* worklist: head list of pending functions, see discussion below */
	INIT_LIST_HEAD(&cwq->worklist);
	init_waitqueue_head(&cwq->more_work);
	init_waitqueue_head(&cwq->work_done);

	if (is_single_threaded(wq))
		p = kthread_create(worker_thread, cwq, "%s", wq->name);
	else
		p = kthread_create(worker_thread, cwq, "%s/%d", wq->name, cpu);
	if (IS_ERR(p))
		return NULL;
	cwq->thread = p;
	return p;
  }

* destroy_workqueue() is invoked to destroy a work queue. At the end, I haven't
  understood workqueues very much :(.

  static void cleanup_workqueue_thread(struct workqueue_struct *wq, int cpu)
  {
	struct cpu_workqueue_struct *cwq;
	unsigned long flags;
	struct task_struct *p;

	cwq = per_cpu_ptr(wq->cpu_wq, cpu);
	spin_lock_irqsave(&cwq->lock, flags);
	p = cwq->thread;
	cwq->thread = NULL;
	spin_unlock_irqrestore(&cwq->lock, flags);
	if (p)
		kthread_stop(p);
  }

  /**
   * destroy_workqueue - safely terminate a workqueue
   * @wq: target workqueue
   *
   * Safely destroy a workqueue. All work currently pending will be done first.
   */
  void destroy_workqueue(struct workqueue_struct *wq)
  {
	int cpu;

	flush_workqueue(wq);

	/* We don't need the distraction of CPUs appearing and vanishing. */
	mutex_lock(&workqueue_mutex);
	if (is_single_threaded(wq))
		cleanup_workqueue_thread(wq, singlethread_cpu);
	else {
		for_each_online_cpu(cpu)
			cleanup_workqueue_thread(wq, cpu);
		list_del(&wq->list);
	}
	mutex_unlock(&workqueue_mutex);
	free_percpu(wq->cpu_wq);
	kfree(wq);
  }
  EXPORT_SYMBOL_GPL(destroy_workqueue);

* In most cases, creating a whole set of worker threads in order to run a
  function is overkill. Therefore, the kernel offers a predefined work queue
  called ``events'', which can be freely used by every kernel developer. The
  predefined work queue is nothing more than a standard work queue that may
  include functions of different kernel layers and I/O drivers.

  In other words from LDD3 - (Last section in chapter 7):
  
  A device driver, in many cases, does not need its own workqueue. If you only
  submit tasks to the queue occasionally, it may be more efficient to simply use
  the shared, default workqueue that is provided by the kernel. If you use this
  queue, however, you must be aware that you will be sharing it with others. 
  Among other things, that means that you should not monopolize the queue for
  long periods of time (no long sleeps), and it may take longer for your tasks to
  get their turn in the processor.


Returning from interrupts and exceptions:
-----------------------------------------

* To resume execution of some program, several issues must be considered:
  1- Number of kernel control paths being concurrently executed.
     	    kernel must get back to user mode if only one.
  2- Pending process switch request.	 
     	    kernel must not return to normal process, it must do process
  	    scheduling.
  3- Pending signals.
     	    If a signal is sent to the current process, it must be handled.
  4- Single-step mode.
            If a debugger is tracing execution, signle-step mode must be
  	    restored.

* A few flags are used to keep track of above points and others. There're stored
  in `flags' field of thread_info descriptor.
  
  --> include/asm-x86/thread_info_32.h:

  /*
   * thread information flags
   * NOTE: They're saved in thread_info->flags field.
   * - these are process state flags that various assembly files may need to access
   * - pending work-to-be-done flags are in LSW
   * - other flags in MSW
   */
  #define TIF_SYSCALL_TRACE	0	/* System calls are being traced */
  #define TIF_NOTIFY_RESUME	1	/* resumption notification requested - No use on x86 */
  #define TIF_SIGPENDING	2	/* signal pending */
  #define TIF_NEED_RESCHED	3	/* rescheduling necessary */
  #define TIF_SINGLESTEP	4	/* restore singlestep on return to user mode */
  #define TIF_IRET		5	/* Force return from system call using iret instead of sysexit */
  #define TIF_SYSCALL_EMU	6	/* syscall emulation active */
  #define TIF_SYSCALL_AUDIT	7	/* syscall auditing active */
  #define TIF_SECCOMP		8	/* secure computing */
  #define TIF_RESTORE_SIGMASK	9	/* restore signal mask in do_signal() */
  #define TIF_MEMDIE		16	/* process destroyed to reclaim memory */
  #define TIF_DEBUG		17	/* uses debug registers */
  #define TIF_IO_BITMAP		18	/* uses I/O bitmap */
  #define TIF_FREEZE		19	/* is freezing for suspend */

  #define _TIF_SYSCALL_TRACE	(1<<TIF_SYSCALL_TRACE)
  #define _TIF_NOTIFY_RESUME	(1<<TIF_NOTIFY_RESUME)
  #define _TIF_SIGPENDING	(1<<TIF_SIGPENDING)
  #define _TIF_NEED_RESCHED	(1<<TIF_NEED_RESCHED)
  #define _TIF_SINGLESTEP	(1<<TIF_SINGLESTEP)
  #define _TIF_IRET		(1<<TIF_IRET)
  #define _TIF_SYSCALL_EMU	(1<<TIF_SYSCALL_EMU)
  #define _TIF_SYSCALL_AUDIT	(1<<TIF_SYSCALL_AUDIT)
  #define _TIF_SECCOMP		(1<<TIF_SECCOMP)
  #define _TIF_RESTORE_SIGMASK	(1<<TIF_RESTORE_SIGMASK)
  #define _TIF_DEBUG		(1<<TIF_DEBUG)
  #define _TIF_IO_BITMAP	(1<<TIF_IO_BITMAP)
  #define _TIF_FREEZE		(1<<TIF_FREEZE)

  /* work to do on interrupt/exception return (see ENTRY(resume_userspace))*/
  #define _TIF_WORK_MASK \
  (0x0000FFFF & ~(_TIF_SYSCALL_TRACE | _TIF_SYSCALL_AUDIT | \
		  _TIF_SECCOMP | _TIF_SYSCALL_EMU))

* Returning from interrupts code:

  Remember that: Masking out the least significant THREAD_SIZE bits of %esp leads
  to the base address of the thread_info structure. and $-THREAD_SIZE is
  equivalent to ~(THREAD_SIZE - 1).

  --> include/asm-x86/thread_info_32.h:
  /* how to get the thread information struct from ASM */
  #define GET_THREAD_INFO(reg) \
	movl $-THREAD_SIZE, reg; \
	andl %esp, reg

  --> include/asm-x86/segment.h:
  /* Bottom two bits of selector give the ring privilege level */
  #define SEGMENT_RPL_MASK	0x3
  /* Bit 2 is table indicator (LDT/GDT) */
  #define SEGMENT_TI_MASK	0x4

  --> arch/x86/kernel/entry_32.S:  
  #ifdef CONFIG_PREEMPT
  #define preempt_stop(clobbers)    DISABLE_INTERRUPTS(clobbers)
 
  --> include/asm-x86/irqflags.h:
  #define DISABLE_INTERRUPTS(clobbers)	cli

  Some of the offsets used in below assembly code:

  --> arch/x86/kernel/asm-offsets_32.c:

  /* Associate a symbol to a value in assembly code */
  #define DEFINE(sym, val) \
        asm volatile("\n->" #sym " %0 " #val : : "i" (val))

  #define OFFSET(sym, str, mem) \
	DEFINE(sym, offsetof(struct str, mem));

  OFFSET(TI_task, thread_info, task);
  OFFSET(TI_exec_domain, thread_info, exec_domain);
  OFFSET(TI_flags, thread_info, flags);
  OFFSET(TI_status, thread_info, status);
  OFFSET(TI_preempt_count, thread_info, preempt_count);
  OFFSET(TI_addr_limit, thread_info, addr_limit);
  OFFSET(TI_restart_block, thread_info, restart_block);
  OFFSET(TI_sysenter_return, thread_info, sysenter_return);
  OFFSET(TI_cpu, thread_info, cpu);

  OFFSET(PT_EBX, pt_regs, bx);
  OFFSET(PT_ECX, pt_regs, cx);
  OFFSET(PT_EDX, pt_regs, dx);
  OFFSET(PT_ESI, pt_regs, si);
  OFFSET(PT_EDI, pt_regs, di);
  OFFSET(PT_EBP, pt_regs, bp);
  OFFSET(PT_EAX, pt_regs, ax);
  OFFSET(PT_DS,  pt_regs, ds);
  OFFSET(PT_ES,  pt_regs, es);
  OFFSET(PT_FS,  pt_regs, fs);
  OFFSET(PT_ORIG_EAX, pt_regs, orig_ax);
  OFFSET(PT_EIP, pt_regs, ip);
  OFFSET(PT_CS,  pt_regs, cs);
  OFFSET(PT_EFLAGS, pt_regs, flags);
  OFFSET(PT_OLDESP, pt_regs, sp);
  OFFSET(PT_OLDSS,  pt_regs, ss);

  When returning from interrupts, local interrupts are already disabled (as 
  found in handle_IRQ_event()). thus interrupts are disabled only when 
  returning from exceptions.

  --> include/asm-x86/segment.h:
  /* Bottom two bits of selector give the ring privilege level */
  #define SEGMENT_RPL_MASK	0x3

  /* User mode is privilege level 3 */
  #define USER_RPL		0x3

  --> arch/x86/kernel/entry_32.S:  
  ret_from_exception:
	preempt_stop(CLBR_ANY)
  ret_from_intr:
	/*
	 * From now on, %ebp will represent the kernel mode stack and
	 * the thread info structure.
	 */
	GET_THREAD_INFO(%ebp)
  check_userspace:
	/* 
	 * Note that first 8bits of EFLAGS are necessary to userspace apps ONLY,
  	 * so let's make use of them 
	 *
	 * Below statements store original register values when the interrupt was
	 * tirggered.
	 */
	movl PT_EFLAGS(%esp), %eax	# mix EFLAGS and CS
	movb PT_CS(%esp), %al 		# %al part of %eax ofcourse

	/* 
	 * VM_MASK: mask address of VM flag, virtual 8086 mode
	 * SEGMENT_RPL_MASK: RPL field of the %cs register (since it's mixed) 
	 * i.e.: zero all %eax EXCEPT %eflags VM and %cs RPL fields, leave them
	 * 	 on their state.
	 */
	andl $(VM_MASK | SEGMENT_RPL_MASK), %eax

	/* If neither VM or RPL flags set (00 -> kernel mode), then resume
  	 * kernel: , otherwise continue to resume_userspace.
	 * Note that VM mode has no RPLs */
	cmpl $USER_RPL, %eax
	jb resume_kernel		# not returning to v8086 or userspace

  /* here, program to be resumed is running in user mode */
  ENTRY(resume_userspace)
 	DISABLE_INTERRUPTS(CLBR_ANY)	# make sure we don't miss an interrupt
					# setting need_resched or sigpending
					# between sampling and the iret
	movl TI_flags(%ebp), %ecx
	andl $_TIF_WORK_MASK, %ecx	# is there any work to be done on
					# int/exception return?
	jne work_pending

	/* 
	 * If no remaining work to be done, jump to restore all. thus remaining a
  	 * usermode program 
	 */
	jmp restore_all
  END(ret_from_exception)

  
  /* Come here if we won't return to v8086 or userspace */
  #ifdef CONFIG_PREEMPT
  ENTRY(resume_kernel)
	DISABLE_INTERRUPTS(CLBR_ANY)
	
	/*	
	 * We're going to return to a kernel control path.
	 * mm, a kernel control path and preemption is disabled 
	 * (i.e. with preempt count > 0) ?
	 * Yess, cause we'll return to another interrupt handler!.
	 * Directly (without checking), restore the registers and end the 
	 * interrupt/syscall
	 */
	cmpl $0,TI_preempt_count(%ebp)	# if preempt_count != 0 (preemption disabled)
	jnz restore_nocheck		# goto restore_nocheck:

  /* 
   * If we're here, NONE of unfinished kernel control paths is an interrupt
   * handler 
   */
  need_resched:
	/* 
	 * If TIF_NEED_RESCHED is not set, no process switch is required, return
  	 * simply to restore_all 
	 */
	movl TI_flags(%ebp), %ecx	# need_resched set ?
	testb $_TIF_NEED_RESCHED, %cl
	jz restore_all

	/* 
	 * NOTE: Below answers took SEVERAL hours to deduce!.
	 *
	 * Remember that if we're here, then we are not returning to an interrupt
	 * context path (preempt_count = 0). --> (1)
	 *
	 * Also remember that if we're here, then we are returning to a kernel
	 * control path (CPL = 0). --> (2)
	 * 
	 * If we're returning to a kernel control path were interrupts are
	 * disabled, Do NOT do a process switch. we may corrupt kernel data
	 * structures. 
	 *
	 * Q: Why ? 
	 * 
	 * disabling local interrupts is the most tight interrupTIONS prevention
	 * kernel mechanism. So, if interrupts is disabled in a kernel path, it 
	 * means that this kernel path do not want to get control out IN ALL CASES.
	 * Giving out control in such paths may lead to data corruptions.
	 *
	 * Q: Why it's saying in comment 'exception path' ?
	 *
	 * Direct result from (1) and (2)
	 *
	 * Q: OK, great. but HOW ON EARTH an exception path with INTERRUPTS DISABLED 
	 * be preempted (disabling interrupts disables all kinds of interruptions) ?
	 *
	 * the "Page Fault " exception may occur in Kernel Mode. This happens
	 * when the process attempts to address a page that belongs to its address
	 * space but is not currently in RAM. While handling such an exception, the
	 * kernel may suspend the current process and replace it with another one
	 * until the requested page is available. The kernel control path that
	 * handles the "Page Fault" exception resumes execution as soon as the
	 * process gets the processor again.
	 *
	 * Because the "Page Fault" exception handler never gives rise to
         * further exceptions, at most two kernel control paths associated with
         * exceptions (the first one caused by a system call invocation, the
         * second one caused by a Page Fault) may be stacked, one on top of the
         * other.
	 *
	 * Helpful Note:
	 * there could be up to two kernel control paths associated with exceptions 
	 * (beside the one that is terminating).
  	 * Kernel premption is disabled if preempt_filed > 0. This can ONLY happens if:	
	 * 1- Kernel is in the midlle of an ISR
  	 * 2- Deferrable functions are disabled (occurs when a softirq is running)
  	 * 3- Kernel preemption is explicitly disabled
	 *
  	 * So kernel can be preempted ONLY in an exception handler (system call) 
	 * and when preemption is not explictly disabled (preempt_count = 0).
	 */	 
	testl $IF_MASK,PT_EFLAGS(%esp)	# interrupts off (exception path) ?
	jz restore_all

	/* mm, now a process switch is required. */
	call preempt_schedule_irq
	jmp need_resched
  END(resume_kernel)
  #endif

  restore_all:
	/* 
	 * Warning: PT_OLDSS(%esp) contains the wrong/random values if we
	 * are returning to the kernel.
	 * See comments in process.c:copy_thread() for details.
	 */
	movl PT_EFLAGS(%esp), %eax	# mix EFLAGS, SS and CS
	movb PT_OLDSS(%esp), %ah
	movb PT_CS(%esp), %al

	/* 
	 * TI (Table Indicator) mask is in SS descriptor, thus the 8bits
	 * left shifting (cause it's saved in %ah 
	 */
	andl $(VM_MASK | (SEGMENT_TI_MASK << 8) | SEGMENT_RPL_MASK), %eax
	/*  If eax equal to SEGMENT_LDT and USER_RPL, jump to ldt_ss */
	cmpl $((SEGMENT_LDT << 8) | USER_RPL), %eax
	je ldt_ss			# returning to user-space with LDT SS

  /* 
   * Jump here, to restore everything to the original state and return from
   * interrupt/exception (iret)
   */
  restore_nocheck:
	TRACE_IRQS_IRET
  restore_nocheck_notrace:
	RESTORE_REGS			# defined above
	addl $4, %esp			# skip orig_eax/error_code
  1:	INTERRUPT_RETURN		# aka, iret (to handel virtualizaton complexities)

**********************************************************************************
Kernel Synchronization:
***********************

* In an easy language, what's kernel preemption ?
  A preemptive kernel is a kernel that can force a process in the middle of a
  kernel-mode code to stop and runs another kernel code path. 

  Examples A:
  1- process A is running an exception handler (system call) and in the middle of
     it (kernel mode).
  2- An IRQ rises which awakens the highper priority process B
  3- If kernel is preemtible, process B begins running even when A (as in our
     case) in a middle of a kernel context path.
  NOTE: This occurs in the need_resched: label, which is under the 
  	resume_kernel: path.

  Example B:
  1- process A is running an exception handler (system call)
  2- process time quantum expires
  3- If kernel is preemptible, it will force a process switch. If it's not,
     process will be running till system call ends or it voluntary calls
     schedule()

* The main motivation from making the kernel code preemptive is to reduce
  dispatch latency of user-mode processes.

  Dispatch latency: The amount of time a system takes to respond to a request for
  	   	    a process to begin operation. process scheduling speaking:
  	   	    it's the time delay between the process being runnable
  	   	    (request running) and the time it really starts running.

* From above discussion:

  --> include/asm/thread_info.h::struct thread_info: 
  int			preempt_count;	/* 0 => preemptable, <0 => BUG */

  its bits are used as follows:
  0->7	      	   Preemption counter (max = 255)
    - keeps track of how many times kernel preemption has been disabled on Local
      CPU, 0 value means preemption is not disabled at all.

  8->15		   Softirq counter (max = 255)
    - specifies how many levels deep disabling of deferrable functions is. 0
      means deferrable function is enabled.
      WHY ?
      If a deferrable function is running, others got disabled. 0 means no
      deferrables are running, which means their occurence is enabled.

  16->27	   Hardirq counter (max = 4096)
    - Number of nested interrupt handlers on the local CPU. modified by
      irq_enter() and irq_exit().

  28		   PREEMPT_ACTIVE flag

  Kernel premption is disabled if preempt_filed > 0. This can heppens only in:	
  1- Kernel is in the midlle of an ISR
  2- Deferrable functions are disabled
  3- Kernel preemption is explicitly disabled

  So kernel can be preempted only in an exception handler (system call) and when
  it's not explictly disabled.

  *** VERY IMPORTANT CLARIFICATION:
  ---------------------------------
  Kernel preemption mostly occurs in two cases:
  1- when returning from an interrupt context (hardirq: Example A, Point 2),
     (softirq: Example B, Point 3). i.e. in the need_resched: point (Which is
     ONLY defined if CONFIG_PREEMPT is defined).

  2- when enabling kernel preemption using preempt_enable() macro which calls
     preempt_schedule() function. This function does a preocess switch.	
     This function is also code above in need_resched: path if everythng
     went fine (i.e. interrupts were enabled)

* Some macros to deal with preempt_macro field safely:
  
  --> include/linux/preempt.h:

  /* Selects preempt_count field */
  #define preempt_count()	(current_thread_info()->preempt_count)

  /* Disables preempting by increasing the number of disabling preemptions by one */
  #define preempt_disable() \
  do { \
	inc_preempt_count(); \
	barrier(); \
  } while (0)

  /* decrease by one preemption counter */
  #define preempt_enable_no_resched() \
  do { \
	barrier(); \
	dec_preempt_count(); \
  } while (0)

  /* Enable preemption and check if process needs to be rescheduled */
  #define preempt_enable() \
  do { \
	preempt_enable_no_resched(); \
	barrier(); \
	preempt_check_resched(); \
  } while (0)

  /*
   * From below macro we know that kernle preemption either happens in:
   * 1- At the end of a kernel control path (i.e. ISR)
   * 2- When exception handler reenables kernel preemption
   */
  #define preempt_check_resched() \
  do { \
	if (unlikely(test_thread_flag(TIF_NEED_RESCHED))) \
		preempt_schedule(); \
  } while (0)

* Notes about interleaved kernel paths:
  1- Interrupt handlers do _not_ need to be reentrant
     Reason:
     All interrupt handlers acknowledge the PIC before execution and disable
     similar kinds of interrupts (for all CPUs) from the PIC till they finish.
     
  2- Tasklets do _not_ need to be reentrant
     Reason:
     Tasklets are highly serialized wrt to itself. No two tasklets of the same
     kind will ever run (even on multiple CPUs system)

  3- Softirqs need not to synchronize per-cpu variables usage
     Reason:
     A softirq can not interleave another softirq of the same kind on the same
     CPU. though they can run simultinusly on different CPUs (SMPs).

Synchronization Primitives:
---------------------------

1- Per-CPU data:

* Elements of per-cpu array are aligned in main memory so that each data
  structures falls on a different line of hardware cache. So concurrent access
  won't lead to cache invalidation (high cost operation).

* Per-CPU Data needs explicit proctection. Kernel preemption MUST BE disabled
  when accessing such variables to avoid this case:

  	struct this_needs_locking tux[NR_CPUS];
	tux[smp_processor_id()] = some_value;

	/* task is preempted here and was awaked on another CPU ... */

	something = tux[smp_processor_id()];

  Please read first section in Documentation/preempt-locking.txt for mor info. 
  To avoid this case two macros get_cpu_var and put_cpu_var are created to do
  kernel preemption disable automatically.

  --> include/linux/percpu.h:

  /*
   * Must be an lvalue. Since @var must be a simple identifier,
   * we force a syntax error here if it isn't.
   */
  #define get_cpu_var(var) (*({				\
	extern int simple_identifier_##var(void);	\
	preempt_disable();				\
	&__get_cpu_var(var); }))
  #define put_cpu_var(var) preempt_enable()

  Usage example:

  struct some_struct *ptr = &get_cpu_var(each_cpu_data);  
  [... ptr is accessed (set/get) here ...]
  put_cpu_var(each_cpu_data)

  Note:
  That's the same reason smp_processor_id() can't be called from a preemptible
  context. If it's called on such a context , it'll trigger a Kernel BUG.

* To create a per-cpu data structure Statically:

  DEFINE_PER_CPU(int, mypcint);
  get_cpu_var(mypcint)
  [...]
  put_cpu_var(mypcint)  

* To create a per-cpu data-structure dynamically:

  void *alloc_percpu(type);
  void free_percpu(const void *);
  
2- Atomic Operations:

* X86 operations related to atomity:

  1- Assembly language instructions that make zero or one aligned memory access
     are atomic

  2- Read-modify-write assembly language instructions whose opcode is prefixed by
     the lock byte (0xf0) are atomic even on a multiprocessor system

  --> include/asm-x86/alternative.h:
  #ifdef CONFIG_SMP
  #define LOCK_PREFIX \
		".section .smp_locks,\"a\"\n"	\
		"  .align 4\n"			\
		"  .long 661f\n" /* address */	\
		".previous\n"			\
	       	"661:\n\tlock; "		
		**> (removing \n \t) above is the lock perfix 

  #else /* ! CONFIG_SMP */
  #define LOCK_PREFIX ""
  #endif
  
  "Lock" prefix can be added to below instructions only:

  BT, BTS, BTR, BTC   (mem, reg/imm)
  XCHG, XADD  (reg, mem / mem, reg)
  ADD, OR, ADC, SBB   (mem, reg/imm)
  AND, SUB, XOR   (mem, reg/imm)
  NOT, NEG, INC, DEC  (mem)

  XCHG, XADD and All "X" instructions family are multi-threading safe.

* atomic_t is an atomically accessible counter with some special methods that act
  on it:
 
  --> include/asm-x86/atomic_32.h:
  /*
   * Atomic operations that C can't guarantee us.  Useful for
   * resource counting etc..
   */

  /*
   * Make sure gcc doesn't try to be clever and move things around
   * on us. We need to use _exactly_ the address the user gave us,
   * not some alias that contains the same information.
   */
  typedef struct { int counter; } atomic_t;
  /* 
   * the { (i) } is used to set the atomic_t "counter" element directly
   * A similar case:
   * struct test_struct { int counter; };
   * int main(void) { struct test_struct bla = { (5) }; }
   */
  #define ATOMIC_INIT(i)	{ (i) }

  On kernel newbies, A good guy asked why is it put on a struct, the reply was:

  1) it works around some gcc bugs in old gcc's
  2) it provides type safety; it prevents accidental mixing of atomic_t
     with other types, which would be code bugs.

* Other methods to read/write to atomic_t types:
  
  --> include/asm-x86/atomic_32.h:  
  /**
   * atomic_read - read atomic variable
   * @v: pointer of type atomic_t
   * 
   * Atomically reads the value of @v.
   */ 
  #define atomic_read(v)		((v)->counter)

  /**
   * atomic_set - set atomic variable
   * @v: pointer of type atomic_t
   * @i: required value
   * 
   * Atomically sets the value of @v to @i.
   */ 
  #define atomic_set(v,i)		(((v)->counter) = (i))

  /**
   * atomic_add - add integer to atomic variable
   * @i: integer value to add
   * @v: pointer of type atomic_t
   * 
   * Atomically adds @i to @v.
   */
  static __inline__ void atomic_add(int i, atomic_t *v)
  {
	__asm__ __volatile__(
		LOCK_PREFIX "addl %1,%0"
		:"+m" (v->counter)
		:"ir" (i));
  }

  /**
   * atomic_sub_and_test - subtract value from variable and test result
   * @i: integer value to subtract
   * @v: pointer of type atomic_t
   * 
   * Atomically subtracts @i from @v and returns
   * true if the result is zero, or false for all
   * other cases.
   */
  static __inline__ int atomic_sub_and_test(int i, atomic_t *v)
  {
	unsigned char c;

	__asm__ __volatile__(
		LOCK_PREFIX "subl %2,%0; sete %1"
		:"+m" (v->counter), "=qm" (c)
		:"ir" (i) : "memory");
	return c;
  }

  /**
   * atomic_inc - increment atomic variable
   * @v: pointer of type atomic_t
   * 
   * Atomically increments @v by 1.
   */ 
  static __inline__ void atomic_inc(atomic_t *v)
  {
	__asm__ __volatile__(
		LOCK_PREFIX "incl %0"
		:"+m" (v->counter));
  }

  /*
   * the xadd 486+ command:
   * Exchanges the first operand (destination operand) with the second operand
   * (source operand), then loads the sum of the two values into the destination
   * operand. The destination operand can be a register or a memory location
   */

  /**
   * atomic_add_return - add integer and return
   * @v: pointer of type atomic_t
   * @i: integer value to add
   *
   * Atomically adds @i to @v and returns @i + @v
   */
  static __inline__ int atomic_add_return(int i, atomic_t *v)
  {
	int __i;
  #ifdef CONFIG_M386
	unsigned long flags;
	if(unlikely(boot_cpu_data.x86 <= 3))
		goto no_xadd;
  #endif

	/* Modern 486+ processor */
	__i = i;
	__asm__ __volatile__(
		LOCK_PREFIX "xaddl %0, %1"
		:"+r" (i), "+m" (v->counter)
		: : "memory");
	return i + __i;

  #ifdef CONFIG_M386

  /* 
   * Darwish:
   * Why disabling interrupts here ?
   * 
   * By disabling interrupts, we:
   * 1- Disable any hard IRQs
   * 2- Disable any soft IRQs (deferrable functions like tasklets)
   * 3- Disable Kernel Preemption
   * 4- Since this a 386, it does not feature SMP ofcourse, so no other
   * 	interrupts (non-local interrupts) will interrupt other processes.
   *
   * So This makes sure that on the special 386 case, no other code path
   * can run concurrently with us.
   */
  no_xadd: /* Legacy 386 processor */
	local_irq_save(flags);
	__i = atomic_read(v);
	atomic_set(v, i + __i);
	local_irq_restore(flags);
	return i + __i;
  #endif
  }

3- Optimization and Mem barriers:

* An optimization barrier primitive ensures that the assembly language
  instructions corresponding to C statements placed before the primitive are not
  mixed by the compiler with assembly language instructions corresponding to C
  statements placed after the primitive

* Following kinds of instructions are said to be serializing, and they also acts
  as memory barriers:
  1- All I/O ports instructions
  2- All instructions prefixed by the "lock" byte
  3- All instructions that write into control registers, system registers, or 
     debug registers (i.e. cli, sli which modify eflags)
  4- lfence, sfence and mfence instructions (Pentium4 process and above ONLY)
  5- iret instruction that terminations ISRs and exception handlers

  --> include/asm-x86/system.h:

  /*
   * Force strict CPU ordering.
   * And yes, this is required on UP too when we're talking
   * to devices.
   *
   * For now, "wmb()" doesn't actually do anything, as all
   * Intel CPU's follow what Intel calls a *Processor Order*,
   * in which all writes are seen in the program order even
   * outside the CPU.
   *
   * I expect future Intel CPU's to have a weaker ordering,
   * but I'd also expect them to finally get their act together
   * and add some real memory barriers if so.
   *
   * Some non intel clones support out of order store. wmb() ceases to be a
   * nop for these.
   *
   * Darwish(1): This acts as an optimization barrier. the addl command 
   * 	      	 itself is useless, the important thing is the "lock" byte 
   *	      	 which makes the instruction a memory barrier for the cpu
   *
   * Darwish(2): The "alternative" macro is a macro to let the appropriate 
   * 		 instruction be loaded. i.e., for CPUs that does not support
   * 		 lfence, the lock bit is used, and so on
   */
 
  #define mb() alternative("lock; addl $0,0(%%esp)", "mfence", X86_FEATURE_XMM2)
  #define rmb() alternative("lock; addl $0,0(%%esp)", "lfence", X86_FEATURE_XMM2)

  #ifdef CONFIG_X86_OOSTORE
  /* Actually there are no OOO store capable CPUs for now that do SSE, 
     but make it already an possibility. */
  #define wmb() alternative("lock; addl $0,0(%%esp)", "sfence", X86_FEATURE_XMM)
  #else
  #define wmb()	__asm__ __volatile__ ("": : :"memory")
  #endif

* Linked list head write need locking to avoid corruption. Lockless read 
  can be achieved using memory barriers. Check below's Al Viro message:

  (skp is the new node, smack_known is the list head)
  > CPU1 sets skp->smk_next to smack_known. 
  > CPU1 fills in the rest of the entry.
  > CPU1 sets smack_known to skp (the entry).
  > 
  > CPU2 will either see the old value for smack_known,
  > in which case this entry isn't actually on the list yet,
  > or it will see the new value in smack_known. Since smk_next
  > is set before the entry is added to the list, it seems that
  > the scenario you've outlined shouldn't happen.

  Why?  Writes from CPU1 don't have to be seen in the same order on CPU2.
  Compiler has every right to turn that
  	load from head
  	store head to new->next
  	...
	store new to head

   into
	load from head
	store new to head
	...
	store head to new->smk_next

  and there's no reason whatsoever why these two stores won't be seen
  out of order on CPU2 - there's no barrier and all we have is a "bunch
  of assignments from registers to various (nonrepeating) addresses in
  memory".  On out-of-order architectures they can be reordered just fine
  by CPU itself...

  You need at least a barrier, assuming that you want to keep that kind
  of lockless access in readers.

4- Spin Locks:

* As a general rule, kernel preemption is * disabled * in every critical region
  protected by spin locks. In the case of a uniprocessor system, the locks
  themselves are useless, and the spin lock primitives just disable or enable the
  kernel preemption. Please notice that kernel preemption is still enabled during
  the busy wait phase, thus a process waiting for a spin lock to be released
  could be replaced by a higher priority process.

* Spinlock main structures:
  Note: All stuff related to spinlock debugging is reduced to reduce complexity.

  --> include/linux/spinlock_types.h:
  #if defined(CONFIG_SMP)
  # include <asm/spinlock_types.h>
  #else
  # include <linux/spinlock_types_up.h>
  #endif
  
  ** For SMP systems:
  --> include/asm/spinlock_types.h 
  typedef struct {
  	/* 
	 * slock: represent spinlock state.
	 * slock <= 0 --> locked
	 * slock > 0 --> unclocked
	 */
	unsigned int slock;
  } raw_spinlock_t
  #define __RAW_SPIN_LOCK_UNLOCKED	{ 1 }

  ** For UP systems:
  --> include/linux/spinlock_types_up.h:
  typedef struct { } raw_spinlock_t;
  #define __RAW_SPIN_LOCK_UNLOCKED { }

  --> include/linux/spinlock_types.h:
  typedef struct {
	raw_spinlock_t raw_lock;

	/* Flag signalling that a process is busy waiting on the lock */
  #if defined(CONFIG_PREEMPT) && defined(CONFIG_SMP)
	unsigned int break_lock;
  #endif

  } spinlock_t;

* Macros to deal with spinlocks safely:

< (1)- Initializing the spinlock: >

  --> include/linux/spinlock.h:
  # define spin_lock_init(lock)					\
	do { *(lock) = SPIN_LOCK_UNLOCKED; } while (0)

< (2)- Acquiring the spinlock: >

  --> include/linux/spinlock.h:
  /*
   * Another layer of abstraction is used cause there are two types:
   * 1- one defined as a C function that do other extra debug stuff
   * 2- second defined as a macro that do pure locking only
   */
  #define spin_lock(lock)		_spin_lock(lock)

  ((2.a)) Unicporcessors:
  -----------------------

  --> include/linux/spinlock_api_up.h: (UP)
  /*
   * In the UP-nondebug case there's no real locking going on, so the
   * only thing we have to do is to keep the preempt counts and irq
   * flags straight, to supress compiler warnings of unused lock
   * variables, and to add the proper checker annotations
   *
   * Darwish: This does not do any locking, just disables kernel 
   * 	      preemption.
   */
  #define __LOCK(lock) \
    do { preempt_disable(); __acquire(lock); (void)(lock); } while (0)

  #define _spin_lock(lock)		__LOCK(lock)
  #define _spin_trylock(lock)		({ __LOCK(lock); 1; })

  ((2.b)) SMP systems:
  --------------------

  --> kernel/spinlock.c: (SMP)
  /*
   * This file contains the spinlock/rwlock implementations for the
   * SMP and the DEBUG_SPINLOCK cases. (UP-nondebug inlines them)
   */
  void __lockfunc _spin_lock(spinlock_t *lock)
  {
	preempt_disable();
	spin_acquire(&lock->dep_map, 0, 0, _RET_IP_);
	/* 
	 * Darwish: First test _raw_spin_trylock, if it failed,
	 * go with _raw_spin_lock 
	 */
	LOCK_CONTENDED(lock, _raw_spin_trylock, _raw_spin_lock);
  }

  --> include/linux/lockdep.h:
  #define LOCK_CONTENDED(_lock, try, lock)			\
  do {								\
	if (!try(_lock)) {					\
		lock_contended(&(_lock)->dep_map, _RET_IP_);	\
		lock(_lock);					\
	}							\
	lock_acquired(&(_lock)->dep_map);			\
  } while (0)

* Now here are the core/raw spinlock function implementation:

  --> include/linux/spinlock.h:
  # define _raw_spin_lock(lock)		__raw_spin_lock(&(lock)->raw_lock)
  # define _raw_spin_trylock(lock)	__raw_spin_trylock(&(lock)->raw_lock)

  --> include/asm-x86/spinlock.h:
  /* 
   * NOTE1:
   * There's a new implementation that's more fair on several-processors.
   * Check LWN article on ticket spinlocks: 
   * http://lwn.net/Articles/267968/
   */
  static inline void __raw_spin_lock(raw_spinlock_t *lock)
  {
	asm volatile("\n1:\t"
		     /* Decrement spinlock counter */
		     LOCK_PREFIX " ; decb %0\n\t"

		     /* jump if the lock counter isn't negative */
		     "jns 3f\n"

		     /* 
		      * If we're here, the lock is already taken
		      * loop till it's free
		      */
		     "2:\t"
		     "rep;nop\n\t"
		     "cmpb $0,%0\n\t"
		     "jle 2b\n\t"

		     /* The lock became free, try to acquire it again */
		     "jmp 1b\n"
		     
		     /* The lock is our own now ! */
		     "3:\n\t"
		     : "+m" (lock->slock) : : "memory");
  }

  /* Used when kernel preemption is ENABLED */
  static inline int __raw_spin_trylock(raw_spinlock_t *lock)
  {
	char oldval;
	asm volatile(
		/* Exchange oldval with lock->slock atomically */
		"xchgb %b0,%1"
		:"=q" (oldval), "+m" (lock->slock)
		:"0" (0) : "memory");

	/* Return true if slock was positive, we've acquired the lock */
	return oldval > 0;
  }
 
  ** Very Important Note:

  Imagine the following:
  1- An Interrupt occurs, and in the middle of its ISR it busy loops
  2- Another interrupt of different kind got triggered which its ISR
     wakeup a higher priority process.
  3- This will set the _TIF_NEED_RESCHED_FLAG
  4- After returning from the ISR, kernel reaches the resume_kernel: label
  5- It will NOT reach need_resched: cause no process ever reach this point
     in an interrupt context (preempt_count != 0).

  ---> User space							  --->
       |					       resume_userspace: ^
       | IRQ5								 |
       |								 |
       v  ISR busy waiting						 |
       -----------------.		   resume_kernel:  --------------|	
			|				   ^ won't reach need_resched:
			| IRQ11				   |
			|				   |
			v  ISR wakes higher prio process   |
			-----------------------------------|

< (3)- Releasing the lock: >
  
  --> include/linux/spinlock.h:
  # define spin_unlock(lock) \
    do {__raw_spin_unlock(&(lock)->raw_lock); __release(lock); } while (0)
    
  --> include/asm-x86/spinlock.h:
  #if defined(CONFIG_X86_32) && \
	(defined(CONFIG_X86_OOSTORE) || defined(CONFIG_X86_PPRO_FENCE))
  /*
   * On PPro SMP or if we are using OOSTORE, we use a locked operation to unlock
   * (PPro errata 66, 92)
   * 
   * Otherwise, normal sane-functioning intel processors don't need barries
   * on their memory write operations
   */
  # define UNLOCK_LOCK_PREFIX LOCK_PREFIX
  #else
  # define UNLOCK_LOCK_PREFIX
  #endif

  static inline void __raw_spin_unlock(raw_spinlock_t *lock)
  {
	__asm__ __volatile__(
		UNLOCK_LOCK_PREFIX "incb %0"
		:"+m" (lock->slock)
		:
		:"memory", "cc");
  }


5- Read/Write Spinlocks (not revised for 2.6.25 additions):

* Basic Structure:

  --> include/linux/spinlock_types.h:
  typedef struct {
	raw_rwlock_t raw_lock;
  #if defined(CONFIG_PREEMPT) && defined(CONFIG_SMP)
	unsigned int break_lock;
  #endif

  } rwlock_t;

  --> include/asm-i386/spinlock_types.h:
  typedef struct {
  	/*
	 * This field serves for two purpsoses:
	 * 1- two complement's value of a 24bit field representing current number
  	 *    of readers in the low 24 bits
	 * 2- bit 24 is used a flag which is:
	 *    set: when no kernel path is reading or writing
	 *    cleared: otherwise
	 *
	 * Notes:
	 * lock = 0x01000000 --> lock is idle (no readers, nor writers)
	 * lock = 0x00000000 --> acquired for writing
	 * lock = 0x00 ffffff ~ 0x00 000001 --> acquired for reading, num of
  	 *  	  readers = ~(lock && 0x0011 1111) + 1.
	 */
	unsigned int lock;
  } raw_rwlock_t;

  #define __RAW_RW_LOCK_UNLOCKED	{ RW_LOCK_BIAS }
  
  --> include/asm-i386/rwlock.h:
  #define RW_LOCK_BIAS		 0x01000000
  #define RW_LOCK_BIAS_STR	"0x01000000"

* Basic Operations:
  
  1- Initializing the rwlock:
  
  --> include/linux/spinlock.h:
  # define rwlock_init(lock)				\
	do { *(lock) = RW_LOCK_UNLOCKED; } while (0)

  2- Acquiring lock for reading:
  
  a- For a non-preemptive kernel:

  --> kernel/spinlock.c:
  /*
   * If lockdep is enabled then we use the non-preemption spin-ops
   * even on CONFIG_PREEMPT, because lockdep assumes that interrupts are
   * not re-enabled during lock-acquire (which the preempt-spin-ops do):
   */
   #if !defined(CONFIG_PREEMPT) || !defined(CONFIG_SMP) || \
	defined(CONFIG_DEBUG_LOCK_ALLOC)

  void __lockfunc _read_lock(rwlock_t *lock)
  {
	preempt_disable();
	rwlock_acquire_read(&lock->dep_map, 0, 0, _RET_IP_);
	/* First try _raw_read_trylock, if it failed, use _raw_read_lock */
	LOCK_CONTENDED(lock, _raw_read_trylock, _raw_read_lock);
  }
  EXPORT_SYMBOL(_read_lock);
  
  --> include/asm-i386/spinlock.h:
  static inline int __raw_read_trylock(raw_rwlock_t *lock)
  {
	atomic_t *count = (atomic_t *)lock;
	atomic_dec(count);
	if (atomic_read(count) >= 0)
	   	/*
		 * count could change here (between if and return), but this
		 * ain't dangerous cause we have only three cases here:
		 * 1- count = 0x0000 0000 (A writer) which will make method
		 *    already fail
		 * 2- count = 0x00ff ffff ~ 0x0000 0001 (another readers), since
		 *    we're also a reader, we must succeed any way. No writers
  		 *    can enter while other readers are allowed.
		 * 3- count = 0x0100 0000 will transform to 0x00ff ffff which
		 *    will allow another readers and prevent writers.
		 */
		return 1;

	/* We failed */
	atomic_inc(count);
	return 0;
  }
    
  static inline void __raw_read_lock(raw_rwlock_t *rw)
  {
	asm volatile(LOCK_PREFIX " subl $1,(%0)\n\t"
	    	     /* If not negative, jump to 1: */
		     "jns 1f\n"
		     "call __read_lock_failed\n\t"
		     "1:\n"
		     ::"a" (rw) : "memory");
  }

  --> arch/i386/lib/Semaphore.S:
  ENTRY(__read_lock_failed)
  2: 	
  	LOCK_PREFIX
	incl	(%eax)
  1:	
  	rep; nop
	/* Loop till %eax becomes positve (%eax >= 1) */
	cmpl	$1,(%eax)
	js	1b
	LOCK_PREFIX
	decl	(%eax)
	js	2b
	ret
  END(__read_lock_failed)

  b- For a preemptive kernel:

  --> kernel/spinlock.c: (After CPP preprocessing)
  void __lockfunc _read_lock(rwlock_t *lock) 
  { 
	for (;;) { 
		preempt_disable(); 
		if (likely(_raw_read_trylock(lock))) 
			break; 
		preempt_enable(); 
		if (!(lock)->break_lock) 
			(lock)->break_lock = 1; 
		while (!read_can_lock(lock) && (lock)->break_lock) 
			_raw_read_relax(&lock->raw_lock); 
	} 
	(lock)->break_lock = 0; 
  } 
  EXPORT_SYMBOL(_read_lock); 

  3- Putting Reading lock:

  --> kernel/spinlock.c:
  void __lockfunc _read_unlock(rwlock_t *lock)
  {
	rwlock_release(&lock->dep_map, 1, _RET_IP_);
	_raw_read_unlock(lock);
	preempt_enable();
  } 

  --> include/asm-i386/spinlock.h:
  static inline void __raw_read_unlock(raw_rwlock_t *rw)
  {
	asm volatile(LOCK_PREFIX "incl %0" :"+m" (rw->lock) : : "memory");
  }
 
  4- Acquiring lock for writing:

  Same stuff as acquiring reading except in the __raw methods.

  --> include/asm-i386/spinlock.h:
  static inline int __raw_write_trylock(raw_rwlock_t *lock)
  {
	atomic_t *count = (atomic_t *)lock;
	if (atomic_sub_and_test(RW_LOCK_BIAS, count))
		return 1;
	atomic_add(RW_LOCK_BIAS, count);
	return 0;
  }
  
  static inline void __raw_write_lock(raw_rwlock_t *rw)
  {
	asm volatile(LOCK_PREFIX " subl $" RW_LOCK_BIAS_STR ",(%0)\n\t"
		     "jz 1f\n"
		     "call __write_lock_failed\n\t"
		     "1:\n"
		     ::"a" (rw) : "memory");
  }

  --> arch/i386/lib/Semaphore.S:
  ENTRY(__write_lock_failed)
  2: 	
  	LOCK_PREFIX
	addl	$ RW_LOCK_BIAS,(%eax)
  1:	
  	/* Tight loop till the lock is freed */
  	rep; nop
	cmpl	$ RW_LOCK_BIAS,(%eax)
	jne	1b
	subl	$ RW_LOCK_BIAS,(%eax)
	jnz	2b
	ret
	END(__write_lock_failed)

  5- Putting a writing lock:

  Same stuff as putting reading one except in the __raw methods.

  static inline void __raw_write_unlock(raw_rwlock_t *rw)
  {
	asm volatile(LOCK_PREFIX "addl $" RW_LOCK_BIAS_STR ", %0"
				 : "+m" (rw->lock) : : "memory");
  }

6- SeqLocks (Not revised for 2.6.25 additions):

* Writers are allowed to proceed if some readers are active. Writers never wait,
  while readers may read data several times till they get a valid copy.

  --> include/linux/seqlocks.h:
  /*
   * Reader/writer consistent mechanism without starving writers. This type of
   * lock for data where the reader wants a consistent set of information
   * and is willing to retry if the information changes.  Readers never
   * block but they may have to retry if a writer is in
   * progress. Writers do not wait for readers. 
   *
   * This is not as cache friendly as brlock. Also, this will not work
   * for data that contains pointers, because any writer could
   * invalidate a pointer that a reader was following.
   *
   * Expected reader usage:
   * 	do {
   *	    seq = read_seqbegin(&foo);
   * 	...
   *      } while (read_seqretry(&foo, seq));
   *
   *
   * On non-SMP the spin locks disappear but the writer still needs
   * to increment the sequence variables because an interrupt routine could
   * change the state of the data.
   *
   * Based on x86_64 vsyscall gettimeofday 
   * by Keith Owens and Andrea Arcangeli
   */

  #include <linux/spinlock.h>
  #include <linux/preempt.h>

  typedef struct {
	unsigned sequence;
	spinlock_t lock;
  } seqlock_t;

* Initialization stuff:

  --> include/linux/seqlock.h:
 #define __SEQLOCK_UNLOCKED(lockname) \
		 { 0, __SPIN_LOCK_UNLOCKED(lockname) }

 #define SEQLOCK_UNLOCKED \
		 __SEQLOCK_UNLOCKED(old_style_seqlock_init)

 #define seqlock_init(x)				\
	do {						\
		(x)->sequence = 0;			\
		spin_lock_init(&(x)->lock);		\
	} while (0)

 #define DEFINE_SEQLOCK(x) \
		seqlock_t x = __SEQLOCK_UNLOCKED(x)

* How writers acquire and de-acquire the lock:

  /* Lock out other writers and update the count.
   * Acts like a normal spin_lock/unlock.
   * Don't need preempt_disable() because that is in the spin_lock already.
   */
  static inline void write_seqlock(seqlock_t *sl)
  {
	spin_lock(&sl->lock);
	++sl->sequence;
	smp_wmb();
  }

  /* 
   * When releasing lock, increment counter once more
   * This will assure that if a writer is in the middle, "sequence"
   * will have an odd value
   */
  static inline void write_sequnlock(seqlock_t *sl)
  {
	smp_wmb();
	sl->sequence++;
	spin_unlock(&sl->lock);
  }
  
  static inline int write_tryseqlock(seqlock_t *sl)
  {
	int ret = spin_trylock(&sl->lock);

	if (ret) {
		++sl->sequence;
		smp_wmb();
	}
	return ret;
  }

* Trying to reading critical content right:

  /* Start of read calculation -- fetch last complete writer token */
  static __always_inline unsigned read_seqbegin(const seqlock_t *sl)
  {
	unsigned ret = sl->sequence;
	smp_rmb();
	return ret;
  }
  
  /* Test if reader processed invalid data.
   * Fail if:
   * If initial values is odd, 
   *	then writer had already started when section was entered
   * If sequence value changed
   *	then writer changed data while in section
   *    
   * Using xor saves one conditional branch.
   */
  static __always_inline int read_seqretry(const seqlock_t *sl, unsigned iv)
  {
	smp_rmb();
	return (iv & 1) | (sl->sequence ^ iv);
  }

7- Semaphores (Not revised for 2.6.25 additions):

* Whenever a kernel control path tries to acquire a locked semaphore, the
  corresponding process is suspended. It becomes runnable again when the resource
  is released.

  --> include/asm-i386/semaphore.h:
  struct semaphore {
  	/* count > 0 : free
	 * count = 0 : busy but no other porcess is waiting
	 * count < 0 : busy, at least one process is waiting */	 
	atomic_t count;
	/* A flag whether some processes are waiting on semaphore */
	int sleepers;
	/* Address of a wait_queue list of sleeping processes, empty 
	 * if count >= 0 */
	wait_queue_head_t wait;
  };

  #define __SEMAPHORE_INITIALIZER(name, n)				\
  {									\
	.count		= ATOMIC_INIT(n),				\
	.sleepers	= 0,						\
	.wait		= __WAIT_QUEUE_HEAD_INITIALIZER((name).wait)	\
  }

  #define __DECLARE_SEMAPHORE_GENERIC(name,count) \
	struct semaphore name = __SEMAPHORE_INITIALIZER(name,count)

  #define DECLARE_MUTEX(name) __DECLARE_SEMAPHORE_GENERIC(name,1)
  #define DECLARE_MUTEX_LOCKED(name) __DECLARE_SEMAPHORE_GENERIC(name,0)

  static inline void sema_init (struct semaphore *sem, int val)
  {
	atomic_set(&sem->count, val);
	sem->sleepers = 0;
	init_waitqueue_head(&sem->wait);
  }

  static inline void init_MUTEX (struct semaphore *sem)
  {
	sema_init(sem, 1);
  }

  static inline void init_MUTEX_LOCKED (struct semaphore *sem)
  {
	sema_init(sem, 0);
  }

* Releasing the semaphore:

  --> include/asm-x86/semaphore_32.h:
  /*
   * Note! This is subtle. We jump to wake people up only if
   * the semaphore was negative (== somebody was waiting on it).
   */
  static inline void up(struct semaphore * sem)
  {
	__asm__ __volatile__(
		"# atomic up operation\n\t"
		LOCK_PREFIX "incl %0\n\t"     /* ++sem->count */

		/* If greater than zero, jump to 1:
		 * Otherwise, wakeup other sleeping processes */
		"jg 1f\n\t"

		/* 
		"lea %0,%%eax\n\t"
		"call __up_wakeup\n"
		"1:"
		:"+m" (sem->count)
		:
		:"memory","ax");
  }

  fastcall void __up_wakeup(void /* special register calling convention */);

  --> arch/x86/lib/semaphore_32.S:
  /*
   * The semaphore operations have a special calling sequence that
   * allow us to do a simpler in-line version of them. These routines
   * need to convert that sequence back into the C sequence when
   * there is contention on the semaphore.
   *
   * %eax contains the semaphore pointer on entry. Save the C-clobbered
   * registers (%eax, %edx and %ecx) except %eax whish is either a return
   * value or just clobbered..
   */
  ENTRY(__up_wakeup)
	pushl %edx
	pushl %ecx
	call __up
	popl %ecx
	popl %edx
	ret
  END(__up_wakeup)

  --> lib/semaphore-sleepers.c:
  fastcall void __up(struct semaphore *sem)
  {
	/* Wake up only one process (avoid thundering hurd problem) */
	wake_up(&sem->wait);
  }

* Acquiring the semaphore:

  --> include/asm-x86/semaphore_32.h:
  /*
   * This is ugly, but we want the default case to fall through.
   * "__down_failed" is a special asm handler that calls the C
   * routine that actually waits. See arch/i386/kernel/semaphore.c
   */
  static inline void down(struct semaphore * sem)
  {
	might_sleep();
	__asm__ __volatile__(
		"# atomic down operation\n\t"
		LOCK_PREFIX "decl %0\n\t"     /* --sem->count */

		/* Jump to 2: if count >= 0 */
		"jns 2f\n"
		"\tlea %0,%%eax\n\t"
		"call __down_failed\n"
		"2:"
		:"+m" (sem->count)
		:
		:"memory","ax");
  }

  --> arch/x86/lib/semaphore_32.S:

  /* See comment above __up_wakeup */
  .section .sched.text
  ENTRY(__down_failed)
	pushl %edx
	pushl %ecx
	call __down
	popl %ecx
	popl %edx
	ret
  END(__down_failed)

  --> lib/semaphore-sleepers.c:
  /*
   * Logic:
   *  - only on a boundary condition do we need to care. When we go
   *    from a negative count to a non-negative, we wake people up.
   *
   *  - when we go from a non-negative count to a negative do we
   *
   *    (a) synchronize with the "sleeper" count 
   *
   * 	(b) make sure that we're on the wakeup list before we synchronize so that
   *	    we cannot lose wakeup events.
   *
   * Main task of __down is to suspend the process till the semaphore
   * is released.
   */
  fastcall void __sched __down(struct semaphore * sem)
  {
	struct task_struct *tsk = current;
	DECLARE_WAITQUEUE(wait, tsk);
	unsigned long flags;

	tsk->state = TASK_UNINTERRUPTIBLE;

	/* The waitqueue spin lock is used here not only to protect 
	 * the waitqueue but to protect other semaphore fields too 
	 * (sleepers) 
	 * This is used for optimization and avoidance of two spinlocks */
	spin_lock_irqsave(&sem->wait.lock, flags);
	add_wait_queue_exclusive_locked(&sem->wait, &wait);

	sem->sleepers++;

	/*
	 * Cases for the loop:
	 *
	 * 1- Mutex closed, no sleeping processes (count (after mod) = -1,
	 *    sleepers (after mod) = 1)
	 *
	 * Note: In this case atomic_add_negative() won't change count, just
	 * test about its signdness.
	 *
	 * if (!atomic_add_negative) failed (count is negative), then there's only one
	 * sleeper, sleep and schedule, __up will wake us up to do the test again
	 * in a later time.
	 *
	 * if (!atomic_add_negative) succeded (count is >= 0), we reset
         * sleepers to 0, wakeup other processes in queue (it's already empty)
	 * and continue our code
	 *
	 *
	 * 2- Mutex closed, sleeping processes (count (after mod) = -2, 
	 *    sleepers (after mod) = 2)
	 * 
	 */
	for (;;) {
		int sleepers = sem->sleepers;

		/*
		 * Add "everybody else" into it. They aren't
		 * playing, because we own the spinlock in
		 * the wait_queue_head.
		 */
		if (!atomic_add_negative(sleepers - 1, &sem->count)) {
			sem->sleepers = 0;
			break;
		}
		sem->sleepers = 1;	/* us - see -1 above */
		spin_unlock_irqrestore(&sem->wait.lock, flags);

		schedule();

		spin_lock_irqsave(&sem->wait.lock, flags);
		tsk->state = TASK_UNINTERRUPTIBLE;
	}
	remove_wait_queue_locked(&sem->wait, &wait);
	wake_up_locked(&sem->wait);
	spin_unlock_irqrestore(&sem->wait.lock, flags);
	tsk->state = TASK_RUNNING;
  }

  < Above for loop needs more explanation >

8- Completions:

* Completions were introduced as a solution to some semaphore SMPs race
  conditions. Main problem is that up() and down() are not protected from each
  other and can work concurrently. This is done to optimize up and down()
  code. down() is only protected from other down calls (using the waitqueue
  spinlock) but not from up()s.

  Imagine following situation:

  Thread A:

  struct semaphore *sem = kmalloc(sizeof(struct semaphore), GFP_KERNEL);
  init_MUTEX_LOCKED(sem);
  --> start_THREAD_B(sem);
  down(sem);
  free(sem);
    
  The intent of the above code is to not let Thread A continue till Thread B
  finishes its job. This is racy as follows in a SMP system:

  A	       	    	 |	    B
  		    	 |
  down()	    	 |	    Normal execution
  call __down_failed	 |  	    Normal execution
  call __down		 |	    Normal execution
  <another thread before |	    up()
   acquiring waitqueue 	 |	    call __up_wakeup
   lock >    		 |	    <time tick - another thread>
  if(!atomic_..) succeed |	    <another thread>
  ...		 	 |	    <another thread>
  free(sem)		 |	    <another thread>
  normal execution	 |	    call _up
  normal execution 	 |	    ** __up refers to freed sem pointer **

* Complemetion is a synchronization primitive that is specifically designed to
  solve this problem

  --> include/linux/completion.h:
  struct completion {
	unsigned int done;
	wait_queue_head_t wait;
  };

  #define COMPLETION_INITIALIZER(work) \
	{ 0, __WAIT_QUEUE_HEAD_INITIALIZER((work).wait) }

  #define COMPLETION_INITIALIZER_ONSTACK(work) \
	({ init_completion(&work); work; })

  #define DECLARE_COMPLETION(work) \
	struct completion work = COMPLETION_INITIALIZER(work)
  
  static inline void init_completion(struct completion *x)
  { 
	x->done = 0;
	init_waitqueue_head(&x->wait);
  }

* To say that we've completed the needed task we use:

  --> kernel/sched.c:
  /*
   * NOTE: In contrast of semaphore up(), complete() acquires
   * the structure lock.
   * complete and wait_for_completion can NOT execute concurrently. 
   * This is the main difference between completions and semaphores.
   */
  void fastcall complete(struct completion *x)
  {
	unsigned long flags;

	spin_lock_irqsave(&x->wait.lock, flags);
	x->done++;
	__wake_up_common(&x->wait, TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE,
			 1, 0, NULL);
	spin_unlock_irqrestore(&x->wait.lock, flags);
  }
  EXPORT_SYMBOL(complete);

  /* Wakeup all sleeping threads */
  void fastcall complete_all(struct completion *x)
  {
	unsigned long flags;

	spin_lock_irqsave(&x->wait.lock, flags);
	x->done += UINT_MAX/2;
	__wake_up_common(&x->wait, TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE,
			 0, 0, NULL);
	spin_unlock_irqrestore(&x->wait.lock, flags);
  }
  EXPORT_SYMBOL(complete_all);

* To wait till the needed task finishes:

  void fastcall __sched wait_for_completion(struct completion *x)
  {
	might_sleep();

	spin_lock_irq(&x->wait.lock);
	if (!x->done) {
	   	/* Work has not been yet completed */

		/* Add current to the tail of wait queue */
		DECLARE_WAITQUEUE(wait, current);
		wait.flags |= WQ_FLAG_EXCLUSIVE;
		__add_wait_queue_tail(&x->wait, &wait);

		/* Sleep till the work is done */
		do {
			__set_current_state(TASK_UNINTERRUPTIBLE);
			spin_unlock_irq(&x->wait.lock);
			schedule();
			spin_lock_irq(&x->wait.lock);
		} while (!x->done);
		__remove_wait_queue(&x->wait, &wait);
	}

	/* Work has been completed, quit happily */
	x->done--;
	spin_unlock_irq(&x->wait.lock);
  }
  EXPORT_SYMBOL(wait_for_completion);
  
* From a nice lwn.net article:

  " If you do not use complete_all(), you should be able to use a completion
  structure multiple times without problem. It does not hurt, however, to
  reinitialize the structure before each use - so long as you do it before
  initiating the process that will call complete()! The macro INIT_COMPLETION()
  can be used to quickly reinitialize a completion structure that has been fully
  initialized at least once. "


9- Local Interrupts Disabling:

* Local interrupts disabling is useful if the ISRs access a protected driver
  structure. By itself it's not useful on SMPs since interrupts may be triggered
  on another CPUs. For this reason local interrupts disabling is often coupled
  with spinlocks.

  --> include/asm-x86/irqflags_32.h:
  static inline void raw_local_irq_disable(void)
  {
	native_irq_disable();
  }

  static inline void native_irq_disable(void)
  {
	asm volatile("cli": : :"memory");
  }

* When kernel enters a critical section it disables interrupts (clear IF
  flag). when it ends it can't simply set it again cause interrupts are nested
  and the IF flag maybe cleared even before we cleared it again. 
  For this reason to disable interrupts and enable it, we save the eflags at the
  beginning, clear IF, restore eflags to its original value

  --> include/asm-x86/irqflags_32.h:
  static inline unsigned long native_save_fl(void)
  {
	unsigned long f;
	asm volatile("pushfl ; popl %0":"=g" (f): /* no input */);
	return f;
  }

  static inline void native_restore_fl(unsigned long f)
  {
	asm volatile("pushl %0 ; popfl": /* no output */
			     :"g" (f)
			     :"memory", "cc");
  }

  /*
   * For spinlocks (spin_lock_irqsave), etc:
   */
  static inline unsigned long __raw_local_irq_save(void)
  {
	unsigned long flags = __raw_local_save_flags();

	raw_local_irq_disable();

	return flags;
  }

10- Disabling deferrable functions:

* Can be disabled easily by disabling interrupts, since disabling 
  interrupts will also disable softirqs/tasklets.

* More fine-grained way is to control the softirq field of preempt_count.

  Disabling deferrable functions (softirqs):

  --> kernel/softirq.c:
  static inline void __local_bh_disable(unsigned long ip)
  {
	/* in softirq field */
	add_preempt_count(SOFTIRQ_OFFSET);
	barrier();
  }
  void local_bh_disable(void)
  {
	__local_bh_disable((unsigned long)__builtin_return_address(0));
  }

  Enabling deferrable stuff:

  --> kernel/softirq.c:
  void local_bh_enable(void)
  {
	WARN_ON_ONCE(irqs_disabled());

	/*
	 * Are softirqs going to be turned on now:
	 */
	if (softirq_count() == SOFTIRQ_OFFSET)
		trace_softirqs_on((unsigned long)__builtin_return_address(0));
	/*
	 * Keep preemption disabled until we are done with
	 * softirq processing:
 	 */
 	sub_preempt_count(SOFTIRQ_OFFSET - 1);

	/* Invoke long pending softirqs quickly (not waiting for timer tick) */
	if (unlikely(!in_interrupt() && local_softirq_pending()))
		do_softirq();

	dec_preempt_count();

	/* Check whether TIF_NEED_RESCHED flag is set, preempt kerenel if 
	 * needed */ 
	preempt_check_resched();
  }

* Data structure synchronization:
---------------------------------

Example1: Linked list

* Linked list write need locking to avoid corruption. Lockless read can be
  achieved using memory barriers. Check below's Al Viro message:

  (skp is the new node, smack_known is the list head)
  > CPU1 sets skp->smk_next to smack_known. 
  > CPU1 fills in the rest of the entry.
  > CPU1 sets smack_known to skp (the entry).
  > 
  > CPU2 will either see the old value for smack_known,
  > in which case this entry isn't actually on the list yet,
  > or it will see the new value in smack_known. Since smk_next
  > is set before the entry is added to the list, it seems that
  > the scenario you've outlined shouldn't happen.

  Why?  Writes from CPU1 don't have to be seen in the same order on CPU2.
  Compiler has every right to turn that
  	load from head
  	store head to new->next
  	...
	store new to head

   into
	load from head
	store new to head
	...
	store head to new->smk_next

  and there's no reason whatsoever why these two stores won't be seen
  out of order on CPU2 - there's no barrier and all we have is a "bunch
  of assignments from registers to various (nonrepeating) addresses in
  memory".  On out-of-order architectures they can be reordered just fine
  by CPU itself...

  You need at least a barrier, assuming that you want to keep that kind
  of lockless access in readers.

*  Whenever a kernel control path acquires a spin lock (as well as a read/write
   lock, a seqlock, or a RCU "read lock"), disables the local interrupts, or
   disables the local softirqs, kernel preemption is automatically disabled.

********************************************************************************
Memory Management (MM):
***********************

* Dynamic memory is a valuable resource needed not only by the processes, but by
  the kernel itself. The performance of the entire system depends on how efficiently
  dynamic memory is managed.

* A page frame in dynamic memory is free if it does not contain any useful
  data. It is not free when the page frame contains data of a User Mode process,
  data of a software cache, dynamically allocated kernel data structures,
  buffered data of a device driver, code of a kernel module, and so on.

* State information of a page frame is kept in a page descriptor of type page.

  --> include/linux/mm_types.h:
  /*
   * Each physical page in the system has a struct page associated with
   * it to keep track of whatever it is we are using the page for at the
   * moment. Note that we have no way to track which tasks are using
   * a page, though if it is a pagecache page, rmap structures can tell us
   * who is mapping it.
   */
  struct page {
  	/*
	 * To save memory space, this flags field also saves the node
	 * and the zone in the relative node this page belongs too
	 *
	 * Saving space here is very important cause this struct exist
	 * for EACH system page
	 */
	unsigned long flags;		/* Atomic flags, some possibly
					 * updated asynchronously */

	atomic_t _count;		/* Reference counter, see below. */


	union {
	    struct {
	    	/* 
		 * Darwish: the private field is only used by the buddy system.
		 * When the page is the start of free block, this field represent
		 * the order of the block (k, where block size = 2 ^ k)
		 */
		unsigned long private;		/* Mapping-private opaque data:
					 	 * usually used for buffer_heads
						 * if PagePrivate set; used for
						 * swp_entry_t if PageSwapCache;
						 * indicates order in the buddy
						 * system if PG_buddy is set.
						 */
		struct address_space *mapping;	/* If low bit clear, points to
						 * inode address_space, or NULL.
						 * If page mapped as anonymous
						 * memory, low bit is set, and
						 * it points to anon_vma object:
						 * see PAGE_MAPPING_ANON below.
						 */
	    };

	    ...
	};

	/* Check struct zone -> {lru_list, active_list, inactive_list} */
	/*
	 * Darwish: This is also used for the buddy allocator. if this
	 * page is the beginning of a block, this lru field links the page
	 * to other pages that are considered a head of a equally ordered
	 * block. In this case, the private field represent the order
	 * of the block and PgBuddy is set
	 */
	struct list_head lru;		/* Pageout list, eg. active_list
					 * protected by zone->lru_lock !
					 */

	/*
	 * On machines where all RAM is mapped into kernel address space,
	 * we can simply calculate the virtual address. On machines with
	 * highmem some memory is mapped into kernel virtual memory
	 * dynamically, so we need a place to store that address.
	 * Note that this field could be 16 bits on x86 ... ;)
	 *
	 * Architectures with slow multiplication can define
	 * WANT_PAGE_VIRTUAL in asm/page.h
	 *
	 * Darwish: Check the HIGHMEM discussion below too.
	 */
  #if defined(WANT_PAGE_VIRTUAL)
	void *virtual;			/* Kernel virtual address (NULL if
					   not kmapped, ie. highmem) */
  #endif

	/* Various other components discussed later */
	...
  }

* Possible values of _count:
  -1: corresponding page frame is free
  >= 0: page is assigned to one or more processes.
  
  --> include/linux/mm.h:
  static inline int page_count(struct page *page)
  {
	return atomic_read(&compound_head(page)->_count);
  }

* Possible page flags:

  --> include/linux/page-flags.h:
  /*
   * See the original header for a big comment that explains
   * some of the meanings of below flags
   */

  /*
   * Don't use the *_dontuse flags.  Use the macros.  Otherwise you'll break
   * locked- and dirty-page accounting.
   *
   * The page flags field is split into two parts, the main flags area
   * which extends from the low bits upwards, and the fields area which
   * extends from the high bits downwards.
   *
   *  | FIELD | ... | FLAGS |
   *  N-1     ^             0
   *          (N-FLAGS_RESERVED)
   *
   * The fields area is reserved for fields mapping zone, node and SPARSEMEM
   * section.  The boundry between these two areas is defined by
   * FLAGS_RESERVED which defines the width of the fields section
   * (see linux/mmzone.h).  New flags must _not_ overlap with this area.
   */
  #define PG_locked	 	 0	/* Page is locked (i.e. I/O operation). Don't touch. */
  #define PG_error		 1	/* An I/O error occured while transfering the page */
  #define PG_referenced		 2	/* Page has been recently accessed */
  #define PG_uptodate		 3	/* Set after completing an I/O operation */

  #define PG_dirty	 	 4	/* Page has been modified */
  #define PG_lru		 5	/* Page is in the active OR inactive page list */
  #define PG_active		 6	/* Page is in the active page list */
  #define PG_slab		 7	/* Page fram is included in a Slab */

  #define PG_owner_priv_1	 8	/* Owner use. If pagecache, fs may use*/
  #define PG_arch_1		 9	/* No use on 80x86 */
  #define PG_reserved		10	/* Page is reserved for kernel use or unusable */
  #define PG_private		11	/* The `private' field stores meaningful data */

  #define PG_writeback		12	/* Page is under writeback (Page is being written to disk using writepage() ) */
  #define PG_compound		14	/* Part of a compound page (Page is handled using extended pagin mechanism) */
  #define PG_swapcache		15	/* Swap page: swp_entry_t in private (Page belongs to the swap cache) */

  #define PG_mappedtodisk	16	/* Has blocks allocated on-disk (All data correspons to disk blocks) */
  #define PG_reclaim		17	/* To be reclaimed asap (Page is marked to be written to disk to save memory) */
  #define PG_buddy		19	/* Page is free, on buddy lists */


* Linux supports NUMA in which access times for different CPU locations may
  vary. The several memory is partitioned to several nodes. Access time
  to pages in a single node is the same (wrt to a single processor).

* Each node, has a descriptor with pg_data_t. 
  
  --> include/linux/mmzone.h:
  /*
   * The pg_data_t structure is used in machines with CONFIG_DISCONTIGMEM
   * (mostly NUMA machines?) to denote a higher-level memory zone than the
   * zone denotes.
   *
   * On NUMA machines, each NUMA node would have a pg_data_t to describe
   * it's memory layout.
   *
   * Memory statistics and page replacement data structures are maintained on a
   * per-zone basis (in lru, active and inactive lists).
   */
  typedef struct pglist_data {
	struct zone node_zones[MAX_NR_ZONES];
	struct zonelist node_zonelists[MAX_ZONELISTS];
	int nr_zones;
  #ifdef CONFIG_FLAT_NODE_MEM_MAP
	struct page *node_mem_map;	  /* Array of page descriptors of the node */
  #endif
	struct bootmem_data *bdata;       /* Used in kernel initialization phase */

	unsigned long node_start_pfn;	  /* First pfn in node */
	unsigned long node_present_pages; /* total number of physical pages */
	unsigned long node_spanned_pages; /* total size of physical page
					     range, ``including holes'' */
	int node_id;

	/* Values used for the kswapd pageout daemon */
	wait_queue_head_t kswapd_wait;
	struct task_struct *kswapd;
	int kswapd_max_order;
  } pg_data_t;

* Our normal x86 PCs use the UMA model (Uniform access model). Thus, speaking
  in NUMA terms, it has only one node. The node name is contig_page_data:

  --> include/linux/mmzone.h:
  #ifndef CONFIG_NEED_MULTIPLE_NODES
  /*
   * Note: No use of the nid field, we only have a single node.
   */
  extern struct pglist_data	contig_page_data;
  #define NODE_DATA(nid)	(&contig_page_data)
  #define NODE_MEM_MAP(nid)	mem_map
  #define MAX_NODES_SHIFT		1

* Linux partitions the physical memory of everynode into three zones:

  --> include/linux/mmzone.h:
  enum zone_type {
  #ifdef CONFIG_ZONE_DMA
	/*
	 * ZONE_DMA is used when there are devices that are not able
	 * to do DMA to all of addressable memory (ZONE_NORMAL). Then we
	 * carve out the portion of memory that is needed for these devices.
	 * The range is arch specific.
	 *
	 * Some examples
	 *
	 * Architecture		Limit
	 * ---------------------------
	 * parisc, ia64, sparc	<4G
	 * s390			<2G
	 * arm			Various
	 * alpha		Unlimited or 0-16MB.
	 *
	 * i386, x86_64 and multiple other arches
	 * 			<16M.
	 */
	ZONE_DMA,
  #endif
  #ifdef CONFIG_ZONE_DMA32
	/*
	 * x86_64 needs two ZONE_DMAs because it supports devices that are
	 * only able to do DMA to the lower 16M but also 32 bit devices that
	 * can only do DMA areas below 4G.
	 */
	ZONE_DMA32,
  #endif
	/*
	 * Normal addressable memory is in ZONE_NORMAL. DMA operations can be
	 * performed on pages in ZONE_NORMAL if the DMA devices support
	 * transfers to all addressable memory.
	 *
	 * In x86: 16M < ZONE_NORMAL < 896M
	 */
	ZONE_NORMAL,
  #ifdef CONFIG_HIGHMEM
	/*
	 * A memory area that is only addressable by the kernel through
	 * mapping portions into its own address space. This is for example
	 * used by i386 to allow the kernel to address the memory beyond
	 * 900MB. The kernel will set up special mappings (page
	 * table entries on i386) for each page that the kernel needs to
	 * access.
	 *
	 * In x86: ZONE_HIGHMEM > 896M
	 */
	ZONE_HIGHMEM,
  #endif
	ZONE_MOVABLE,
	MAX_NR_ZONES
  };

* Each memory zone has its own descriptor, described below:

  --> include/linux/mmzone.h:
  struct zone {
	/* Fields commonly accessed by the page allocator
	 * page_min: number of reserved page frames inside the zones 
	 * 	     (see below example in comment)
	 */
	unsigned long		pages_min, pages_low, pages_high;

	/*
	 * We don't know if the memory that we're going to allocate will be freeable
	 * or/and it will be released eventually, so to avoid totally wasting several
	 * GB of ram we must reserve some of the lower zone memory (otherwise we risk
	 * to run OOM on the lower zones despite there's tons of freeable ram
	 * on the higher zones). This array is recalculated at runtime if the
	 * sysctl_lowmem_reserve_ratio sysctl changes.
	 *
	 * i.e. how many page frams must be preserved for critical low-on-memory
	 * situatins
	 *
	 * Example: An interrupt handler requesting meomry with the GFP_ATOMIC flag,
	 * 	    It must not block, and there's no enough memory in the system.
	 * 	    To avoid the bad event of returning no free pages atomically, 
	 *	    kernel reserves a pool of page frames for atomic allocation.
	 *	    i.e. lowmem_reserve
	 * 
	 */
	unsigned long		lowmem_reserve[MAX_NR_ZONES];

	/* Check the buddy system discussion that exists for each node */
	struct free_area	free_area[MAX_ORDER];

	/* for caches */
	struct per_cpu_pageset	pageset[NR_CPUS];

	/*
	 * free areas of different sizes
	 */
	spinlock_t		lock;

	/* Fields commonly accessed by the page reclaim scanner */
	spinlock_t		lru_lock;	   /* Spin lock for active and inactive lists */
	struct list_head	active_list;	   /* List of active pages in the zone */
	struct list_head	inactive_list;	   /* List of inactive pages in the zone */
	unsigned long		nr_scan_active;	   
	unsigned long		nr_scan_inactive;
	unsigned long		pages_scanned;	   /* counter for page frame reclaiming */
	unsigned long		flags;		   /* zone flags, see below */

	/*
	 * wait_table		-- the array holding the hash table
	 * wait_table_hash_nr_entries	-- the size of the hash table array
	 * wait_table_bits	-- wait_table_size == (1 << wait_table_bits)
	 *
	 * The purpose of all these is to keep track of the people
	 * waiting for a page to become available and make them
	 * runnable again when possible. The trouble is that this
	 * consumes a lot of space, especially when so few things
	 * wait on pages at a given time. So instead of using
	 * per-page waitqueues, we use a waitqueue hash table.
	 *
	 * The bucket discipline is to sleep on the same queue when
	 * colliding and wake all in that wait queue when removing.
	 * When something wakes, it must check to be sure its page is
	 * truly available, a la thundering herd. The cost of a
	 * collision is great, but given the expected load of the
	 * table, they should be so rare as to be outweighed by the
	 * benefits from the saved space.
	 *
	 * __wait_on_page_locked() and unlock_page() in mm/filemap.c, are the
	 * primary users of these fields, and in mm/page_alloc.c
	 * free_area_init_core() performs the initialization of them.
	 */
	wait_queue_head_t	* wait_table;
	unsigned long		wait_table_hash_nr_entries;
	unsigned long		wait_table_bits;

	/*
	 * Discontig memory support fields.(NUMA)
	 * (Pointer to the NUMA node)
	 */
	struct pglist_data	*zone_pgdat;
	/* zone_start_pfn == zone_start_paddr >> PAGE_SHIFT */
	unsigned long		zone_start_pfn;

	/*
	 * zone_start_pfn, spanned_pages and present_pages are all
	 * protected by span_seqlock.  It is a seqlock because it has
	 * to be read outside of zone->lock, and it is done in the main
	 * allocator path.  But, it is written quite infrequently.
	 *
	 * The lock is declared along with zone->lock because it is
	 * frequently read in proximity to zone->lock.  It's good to
	 * give them a chance of being in the same cacheline.
	 */
	unsigned long		spanned_pages;	/* total size, including holes (unit = page) */
	unsigned long		present_pages;	/* amount of memory (excluding holes) (unit = page) */

	/*
	 * rarely used fields:
	 */
	const char		*name;
  };

  typedef enum {
	ZONE_ALL_UNRECLAIMABLE,		/* all pages pinned */
	ZONE_RECLAIM_LOCKED,		/* prevents concurrent reclaim */
	ZONE_OOM_LOCKED,		/* zone is in OOM killer zonelist */
  } zone_flags_t;

  --> include/linux/mm.h:
  
  /*
   * Extract the node id from page->flags
   */
  #ifdef NODE_NOT_IN_PAGE_FLAGS
  extern int page_to_nid(struct page *page);
  #else
  static inline int page_to_nid(struct page *page)
  {
	return (page->flags >> NODES_PGSHIFT) & NODES_MASK;
  }
  #endif

  /*
   * In the node id extracted above, extract the needed zone id 
   * of this page.
   */
  static inline enum zone_type page_zonenum(struct page *page)
  {
	return (page->flags >> ZONES_PGSHIFT) & ZONES_MASK;
  }

  static inline struct zone *page_zone(struct page *page)
  {
	return &NODE_DATA(page_to_nid(page))->node_zones[page_zonenum(page)];
  }

  --> include/linux/mmzone.h:
  #ifndef CONFIG_NEED_MULTIPLE_NODES

  /*
   * contig_page_data is used for Unified Access (UMA) machines
   * to replace the list of Nodes `node_data[]' on NUMA machines 
   * Note: No use of the nid field, we only have a single node.
   */
  extern struct pglist_data	contig_page_data;
  #define NODE_DATA(nid)	(&contig_page_data)
  #define NODE_MEM_MAP(nid)	mem_map
  #define MAX_NODES_SHIFT		1

  #else /* CONFIG_NEED_MULTIPLE_NODES */

  /*
   * Here, we are most probably a NUMA, so extract the node data
   * in the architecture specific way (as u'll see in below header)
   */
  #include <asm/mmzone.h>

  #endif /* !CONFIG_NEED_MULTIPLE_NODES */

  --> include/asm-x86/mmzone_32.h:
  #ifdef CONFIG_NUMA
  extern struct pglist_data	*node_data[];
  #define NODE_DATA(nid)	(node_data[nid])
  
* Memory allocation requests are handled by the "Zoned Page Frame Allocator".
  It contains of:
  1- A zone allocator: component that searches the zone to find an address that
       	    	       can satisfy the request.
  2- Buddy System: Where page frames in the zone are handled
  3- Per-CPU page frame cache: small number of page frames kept in cache for
     	     	  	       performance purposes.

* Page allocation methods:
  
  ** alloc_pages(gfp_mask, order): Alloc 2^order pages, with mask gfp_mask

  --> include/linux/mmzone.h:
  #include <linux/topology.h>
  /* Returns the number of the current Node. */
  #ifndef numa_node_id
  #define numa_node_id()	(cpu_to_node(raw_smp_processor_id()))
  #endif

  --> include/linux/gfp.h:
  /*
   * If NUMA, let alloc_pages_current() specify the node first.
   * else (not NUMA), there's only one node, so call alloc_pages_node()
   */
  #ifdef CONFIG_NUMA
  extern struct page *alloc_pages_current(gfp_t gfp_mask, unsigned order);

  static inline struct page *
  alloc_pages(gfp_t gfp_mask, unsigned int order)
  {
	if (unlikely(order >= MAX_ORDER))
		return NULL;

	return alloc_pages_current(gfp_mask, order);
  }
  extern struct page *alloc_page_vma(gfp_t gfp_mask,
			struct vm_area_struct *vma, unsigned long addr);
  #else
  #define alloc_pages(gfp_mask, order) \
		alloc_pages_node(numa_node_id(), gfp_mask, order)
  #define alloc_page_vma(gfp_mask, vma, addr) alloc_pages(gfp_mask, 0)
  #endif

  ** alloc_page(gfp): Alloc one page

  #define alloc_page(gfp_mask) alloc_pages(gfp_mask, 0)

  ** __get_free_pages(gfp_maske, order): 
     Like alloc_pages, but return the linear address of the page instead of
     struct page* .

  extern unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order);
  extern unsigned long get_zeroed_page(gfp_t gfp_mask);

  #define __get_free_page(gfp_mask) \
		__get_free_pages((gfp_mask),0)

  #define __get_dma_pages(gfp_mask, order) \
		__get_free_pages((gfp_mask) | GFP_DMA,(order))

  --> mm/page_alloc.c:
  /*
   * Common helper functions.
   */
  unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order)
  {
	struct page * page;
	page = alloc_pages(gfp_mask, order);
	if (!page)
		return 0;

	/* 
	 * page_address(page) does some important work if the page
	 * exists in HIGMEME, as we'll see later
	 */
	return (unsigned long) page_address(page);
  }
  EXPORT_SYMBOL(__get_free_pages);

  unsigned long get_zeroed_page(gfp_t gfp_mask)
  {
	struct page * page;

	/*
	 * get_zeroed_page() returns a 32-bit address, which cannot represent
	 * a highmem page
	 */
	VM_BUG_ON((gfp_mask & __GFP_HIGHMEM) != 0);

	page = alloc_pages(gfp_mask | __GFP_ZERO, 0);
	if (page)
		return (unsigned long) page_address(page);
	return 0;
  }
  EXPORT_SYMBOL(get_zeroed_page);

* IMPORTANT: From above methods, as noted, the main allocation function is the 
  alloc_pages(gfp, order) macro which translates to alloc_pages_current() or
  alloc_pages_node() according to the NUMA configuration. which calls the
  heart of the allocator, i.e. __alloc_pages:

  [__get_free_pages() ->] alloc_pages() -> alloc_pages_{current,node}() ->
  __alloc_pages()
  

* Explanation of the GFP flags:

  --> include/linux/gfp.h:
  /*
   * GFP bitmasks..
   *
   * Zone modifiers (see linux/mmzone.h - low three bits)
   *
   * Do not put any conditional on these. If necessary modify the definitions
   * without the underscores and use the consistently. The definitions here may
   * be used in bit comparisons.
   */
  #define __GFP_DMA	((__force gfp_t)0x01u)
  #define __GFP_HIGHMEM	((__force gfp_t)0x02u)
  #define __GFP_DMA32	((__force gfp_t)0x04u)

  /*
   * Action modifiers - doesn't change the zoning
   *
   * __GFP_REPEAT: Try hard to allocate the memory, but the allocation attempt
   * _might_ fail.  This depends upon the particular VM implementation.
   *
   * __GFP_NOFAIL: The VM implementation _must_ retry infinitely: the caller
   * cannot handle allocation failures.
   *
   * __GFP_NORETRY: The VM implementation must not retry indefinitely.
   *
   * __GFP_MOVABLE: Flag that this page will be movable by the page migration
   * mechanism or reclaimed
   */
  #define __GFP_WAIT	((__force gfp_t)0x10u)	/* Can wait and reschedule? (i.e. caller can block ?) */
  #define __GFP_HIGH	((__force gfp_t)0x20u)	/* Should access emergency pools? (high priority allocation ?) */
  #define __GFP_IO	((__force gfp_t)0x40u)	/* Can start physical IO? */
  #define __GFP_FS	((__force gfp_t)0x80u)	/* Can call down to low-level FS? */
  #define __GFP_COLD	((__force gfp_t)0x100u)	/* Cache-cold page required */
  #define __GFP_NOWARN	((__force gfp_t)0x200u)	/* Suppress page allocation failure warning */
  #define __GFP_REPEAT	((__force gfp_t)0x400u)	/* Retry the allocation.  Might fail */
  #define __GFP_NOFAIL	((__force gfp_t)0x800u)	/* Retry for ever.  Cannot fail */
  #define __GFP_NORETRY	((__force gfp_t)0x1000u)/* Do not retry.  Might fail */
  #define __GFP_COMP	((__force gfp_t)0x4000u)/* Add compound page metadata (page frame belong to extended page) */
  #define __GFP_ZERO	((__force gfp_t)0x8000u)/* Return zeroed page on success */
  [...]

* In normal code, higher level flags than the above ones are used:

  --> include/linux/gfp.h:
  #define GFP_NOWAIT	(GFP_ATOMIC & ~__GFP_HIGH)
  /* GFP_ATOMIC means both !wait (__GFP_WAIT not set) and use emergency pool */
  #define GFP_ATOMIC	(__GFP_HIGH)
  #define GFP_NOIO	(__GFP_WAIT)
  #define GFP_NOFS	(__GFP_WAIT | __GFP_IO)
  #define GFP_KERNEL	(__GFP_WAIT | __GFP_IO | __GFP_FS)
  #define GFP_USER	(__GFP_WAIT | __GFP_IO | __GFP_FS | __GFP_HARDWALL)
  #define GFP_HIGHUSER	(__GFP_WAIT | __GFP_IO | __GFP_FS | __GFP_HARDWALL | \
			 __GFP_HIGHMEM)

* As mentioned, alloc_pages(gfp, order) is the main allocation method. But,
  in fact __alloc_pages() is the _raw_ one that really does allocation. alloc_pages
  just prepare the right parameters to the plain __alloc_pages by calling
  alloc_pages_node() as follows:

  --> include/linux/gfp.h:
  /* 
   * As mentioned, if we're not NUMA, alloc_pages() calls alloc_pages_node()
   * with node id = numa_node_id()
   */
  static inline struct page *alloc_pages_node(int nid, gfp_t gfp_mask,
						unsigned int order)
  {
	if (unlikely(order >= MAX_ORDER))
		return NULL;

	/* Unknown node is current node */
	if (nid < 0)
		nid = numa_node_id();

	return __alloc_pages(gfp_mask, order,
		NODE_DATA(nid)->node_zonelists + gfp_zone(gfp_mask));
  }
  
* As noticed in above method, gfp_zone(gfp_mask) deduce the appropriate
  zone of current allocation from sent flags:

  --> include/linux/gfp.h:
  /*
   * Determine the zones marked by the flags, with some order.
   * Return value is used as an index in a node's node_zonelist[] array.
   */
  static inline enum zone_type gfp_zone(gfp_t flags)
  {
	int base = 0;

  #ifdef CONFIG_NUMA
	if (flags & __GFP_THISNODE)
		base = MAX_NR_ZONES;
  #endif

  #ifdef CONFIG_ZONE_DMA
	if (flags & __GFP_DMA)
		return base + ZONE_DMA;
  #endif
  #ifdef CONFIG_ZONE_DMA32
	if (flags & __GFP_DMA32)
		return base + ZONE_DMA32;
  #endif
	if ((flags & (__GFP_HIGHMEM | __GFP_MOVABLE)) ==
			(__GFP_HIGHMEM | __GFP_MOVABLE))
		return base + ZONE_MOVABLE;
  #ifdef CONFIG_HIGHMEM
	if (flags & __GFP_HIGHMEM)
		return base + ZONE_HIGHMEM;
  #endif
	/* The default, in case nothing special is specified */
	return base + ZONE_NORMAL;
  }

* The High Memory story:
  Look at those 3 amazing LWN articles:
  Virtual Memory I: The Problem --> http://lwn.net/Articles/75174/
  64GB on 32-bit systems --> http://lwn.net/Articles/39925/
  Virtual Memory II: Return of objrmap --> http://lwn.net/Articles/75198/

  And this nice Microsoft Hardware Center article:
  Memory Support and Windows Operating Systems:
  http://www.microsoft.com/whdc/system/platform/server/PAE/PAEmem.mspx

* Long story short:

  Same info below is written more nicely in an article in my blog:
  http://darwish-07.blogspot.com/2008/02/long-story-short-old-i386-linux-1gb.html

  The usual programming practice is that every address points to a single byte,
  changing this means chaning lots of in-depth program code. --> (1)

  The 4GB address space needs to be shared between userspace and kernel space.
  If this wasn't shared, as in Ingo's 4/4 patch, a TLB flush would occur for
  EVERY transition from user-space to kernel space. --> (2)

  The usual split was 3GB for userspace and 1GB for kernelspace. This means
  the kernel can not map more than 1 GB of memory in its Page Table Entries. --> (3)

  Since each memory dereference must pass through the processor's MMU unit, to
  access a memory, it's must has a translation from va to pa. but as said, in
  (3), we only have 1GB of availble virtuals addresses. Thus, the kernel can 
  not access more than 1 GB. --> (4)

* High memory 'struct page's do not really have a linear (virtual)address, 
  it does not simply exist.

  page_address(struct_page) which used in __get_free_pages to return the virtual
  address of allocated page returns the address of the 'struct page' instead.
  
  This address always exists cause 'struct page's lives in low memory forever.

  --> include/linux/mm.h:
  /*
   * Return the virtual address of the page.
   */
  static __always_inline void *lowmem_page_address(struct page *page)
  {
	return __va(page_to_pfn(page) << PAGE_SHIFT);
  }

  /*
   * WANT_PAGE_VIRTUAL signals that the arch wants the virtual address 
   * of the page saved directly in page->virtual.
   * This is needed for HIGHMEM (where virtual addresses are temporarily
   * mapped) and for archs with poor multiplication performance (storage
   * vs. speed delima)
   *
   * See the comment above struct page -> virtual
   */
  #if defined(CONFIG_HIGHMEM) && !defined(WANT_PAGE_VIRTUAL)
  #define HASHED_PAGE_VIRTUAL
  #endif

  #if defined(WANT_PAGE_VIRTUAL)
  #define page_address(page) ((page)->virtual)
  #define set_page_address(page, address)		\
	do {						\
		(page)->virtual = (address);		\
	} while(0)
  #define page_address_init()  do { } while(0)
  #endif

  /*
   * Another method of getting the virtual address of a highmapped
   * page. Discussed below
   */
  #if defined(HASHED_PAGE_VIRTUAL)
  void *page_address(struct page *page);
  void set_page_address(struct page *page, void *virtual);
  void page_address_init(void);
  #endif

  /* 
   * The normal case where the virtual address got calculated
   * (not saved).
   * Note: This won't be valid in case of HIGHMEM enabled.
   */
  #if !defined(HASHED_PAGE_VIRTUAL) && !defined(WANT_PAGE_VIRTUAL)
  #define page_address(page) lowmem_page_address(page)
  #define set_page_address(page, address)  do { } while(0)
  #define page_address_init()  do { } while(0)
  #endif

* Naturally, high memory page frames that do not have a linear address
  can't be accessed by the kernel. The kernel dedicates 128MB of its
  address space to mapping high memory page frames.

  This means you can not access more than 128MB of high memory pages 
  concurrently. The 128MB space get periodically recycled.

* the page_address(page) _method_ used in case of HIGHMEM, 
  WANT_PAGE_VIRTUAL=false

  --> mm/highmem.c:
  /* 
   * lookup the hashtable for a specific page
   */

  /*
   * Hash table bucket
   */
  static struct page_address_slot {
	struct list_head lh;			/* List of page_address_maps */
	spinlock_t lock;			/* Protect this bucket's list */
  } ____cacheline_aligned_in_smp page_address_htable[1<<PA_HASH_ORDER];

  static struct page_address_slot *page_slot(struct page *page)
  {
	return &page_address_htable[hash_ptr(page, PA_HASH_ORDER)];
  }

  /*
   * Used in case of highmem systems with 
   * WANT_PAGE_VIRTUAL = NO (as stated by mm.h preprocessor conditionals)
   * HASHED_PAGE_VIRTUAL = YES ofcourse
   */
  void *page_address(struct page *page)
  {
	unsigned long flags;
	void *ret;
	struct page_address_slot *pas;

	if (!PageHighMem(page))
		return lowmem_page_address(page);

	pas = page_slot(page);
	ret = NULL;
	spin_lock_irqsave(&pas->lock, flags);
	if (!list_empty(&pas->lh)) {
		struct page_address_map *pam;

		list_for_each_entry(pam, &pas->lh, list) {
			if (pam->page == page) {
				ret = pam->virtual;
				goto done;
			}
		}
	}
  done:
	spin_unlock_irqrestore(&pas->lock, flags);
	return ret;
  }
  EXPORT_SYMBOL(page_address);

* Kernel uses three methods to map highmem pages:
  1- Permanent kernel mappings
  2- Temporary kernel mappings
  3- Noncontegous memory allocation

* Common variables for permanent kernel mapping:

  pkmap_page_table: page table for permanet kernel mapping
  LAST_PKMAP: number of entries in above PT. i.e. number of possible mappings
  pkmap_count[LAST_PKMAP]: counter for each mapping (discussed below)

  --> include/asm/pgtable_32.h:
  /*
   * Those are actually the maximum size of entries in a Page
   * Table according whether PAE is enabled or not.
   */
  #ifdef CONFIG_X86_PAE
  #define LAST_PKMAP 512
  #else
  #define LAST_PKMAP 1024
  #endif

  #define PKMAP_BASE ((FIXADDR_BOOT_START - PAGE_SIZE*(LAST_PKMAP + 1)) & PMD_MASK)

* kmap(), the core mapping method:

  --> include/linux/highmem.h:
  #ifndef ARCH_HAS_KMAP
  static inline void *kmap(struct page *page)
  {
	might_sleep();
	return page_address(page);
  }

  #define kunmap(page) do { (void) (page); } while (0)

  --> arch/x86/mm/highmem_32.c:
  void *kmap(struct page *page)
  {
	might_sleep();
	if (!PageHighMem(page))
		return page_address(page);
	return kmap_high(page);
  }

  --> include/asm-x86/highmem.h:
  #define LAST_PKMAP_MASK (LAST_PKMAP-1)
  #define PKMAP_NR(virt)  ((virt-PKMAP_BASE) >> PAGE_SHIFT)
  #define PKMAP_ADDR(nr)  (PKMAP_BASE + ((nr) << PAGE_SHIFT))

  --> mm/highmem.c:
  void *kmap_high(struct page *page)
  {
	unsigned long vaddr;

	/*
	 * For highmem pages, we can't trust "virtual" until
	 * after we have the lock.
	 *
	 * We cannot call this from interrupts, as it may block
	 * 
	 * IMPORTANT:
	 * spin_lock to avoid Page Table corruption from concurrent
	 * access. no need to disable interrupts since this function
	 * blocks and can't be called from an interrupt context.
	 */
	spin_lock(&kmap_lock);
	vaddr = (unsigned long)page_address(page);
	if (!vaddr)
		vaddr = map_new_virtual(page);
	
	/*
	 * Meaning of counter values:
	 * 0: usable pte which does not map any highmem frame
	 * 1: no highmem frame is mapped, but it can't be used 
	 *    (TLB stores the old mapping, and it's not flushed yet)
	 * +: pte maps a highmem frame, used by exactly n - 1 components
	 * 
	 * As said in the '+' section, each kmap_high() invocation increases
	 * the counter (below line). Thus we can know how many component
	 * actually use this important frame.
	 */
	pkmap_count[PKMAP_NR(vaddr)]++;
	BUG_ON(pkmap_count[PKMAP_NR(vaddr)] < 2);
	spin_unlock(&kmap_lock);
	return (void*) vaddr;
  }
  EXPORT_SYMBOL(kmap_high);

  * As found in above code, map_new_virtual map a new highmem frame to a virtual
    address as follows:

  --> mm/highmem.c:
  static int pkmap_count[LAST_PKMAP];
  /* number of mapped entries used till now */
  static unsigned int last_pkmap_nr;
  static  __cacheline_aligned_in_smp DEFINE_SPINLOCK(kmap_lock);

  static inline unsigned long map_new_virtual(struct page *page)
  {
	unsigned long vaddr;
	int count;

  start:
	count = LAST_PKMAP;
	/* Find an empty entry */
	for (;;) {
	    	/*
		 * Begin from where we left last time. 
		 *
		 * Thanks to anding with the mask, if we reached the end
		 * of pkmaps (PKMAP_LAST), last_pkmap_nr will be zero.
		 * In this case, flush_all_zero_pkmaps() will be called
		 */
		last_pkmap_nr = (last_pkmap_nr + 1) & LAST_PKMAP_MASK;
		if (!last_pkmap_nr) {
		   	/*
			 * Reset maps with count = 1 and flush their
			 * respective TLB entries
			 */
			flush_all_zero_pkmaps();
			count = LAST_PKMAP;
		}
		if (!pkmap_count[last_pkmap_nr])
			break;	/* Found a usable entry */
		if (--count)
			continue;

		/*
		 * We have searched all entries (PKMAP_LAST) with
		 * no luck.
		 * Sleep for somebody else to unmap their entries.
		 * We'll be awoken by another path calling kunmap()
		 */
		{
			DECLARE_WAITQUEUE(wait, current);

			__set_current_state(TASK_UNINTERRUPTIBLE);
			add_wait_queue(&pkmap_map_wait, &wait);
			spin_unlock(&kmap_lock);
			schedule();
			remove_wait_queue(&pkmap_map_wait, &wait);
			spin_lock(&kmap_lock);

			/* Somebody else might have mapped it while we slept */
			if (page_address(page))
				return (unsigned long)page_address(page);

			/* Re-start */
			goto start;
		}
	}
	vaddr = PKMAP_ADDR(last_pkmap_nr);
	set_pte_at(&init_mm, vaddr,
		   &(pkmap_page_table[last_pkmap_nr]), mk_pte(page, kmap_prot));

	pkmap_count[last_pkmap_nr] = 1;
	/* Discussed below */
	set_page_address(page, (void *)vaddr);

	return vaddr;
  }

* set_page_address in case of LOWMEM (empty), and WANT_PAGE_VIRTUAL (setting
  page->virtual) was discussed above. Third one with hash implementation is
  discussed below:

  --> mm/highmeme.c:
  void set_page_address(struct page *page, void *virtual)
  {
	unsigned long flags;
	struct page_address_slot *pas;
	struct page_address_map *pam;

	BUG_ON(!PageHighMem(page));

	/* Calculate the hash */
	pas = page_slot(page);
	if (virtual) {		/* Add */
		BUG_ON(list_empty(&page_address_pool));

		spin_lock_irqsave(&pool_lock, flags);
		pam = list_entry(page_address_pool.next,
				struct page_address_map, list);
		/* 
		 * We've acquired the pam, remove it from the free
		 * pool 
		 */
		list_del(&pam->list);
		spin_unlock_irqrestore(&pool_lock, flags);

		pam->page = page;
		pam->virtual = virtual;

		spin_lock_irqsave(&pas->lock, flags);

		/*
		 * Add the page to the page address slot (pas) list 
		 * this is how page_size() find us (searching the pas list)
		 */
		list_add_tail(&pam->list, &pas->lh);

		/* Done */
		spin_unlock_irqrestore(&pas->lock, flags);
	} else {		/* Remove */
		spin_lock_irqsave(&pas->lock, flags);
		list_for_each_entry(pam, &pas->lh, list) {
			if (pam->page == page) {
				list_del(&pam->list);
				spin_unlock_irqrestore(&pas->lock, flags);
				spin_lock_irqsave(&pool_lock, flags);

				/*
				 * The page is removed, restore it to the
				 * free pool
				 */
				list_add_tail(&pam->list, &page_address_pool);
				spin_unlock_irqrestore(&pool_lock, flags);
				goto done;
			}
		}
		spin_unlock_irqrestore(&pas->lock, flags);
	}
  done:
	return;
  }

  
* Temporary/Atomic kmapping:
  
  First, a small discussion of fix-mapped addresses:

* Fixmapped memory is a constant linear address whose virtual address does not 
  need to follow the LINEAR - PAGE_OFFSET rule, but rather a physical address
  set in an arbitary way.

* The difference between fix-mapped and linear addresses is that the former
  can point to any physical cell while the latter must follow the - PAGE_OFFSET
  rule. 
  Another difference is that dereferencing and checking the value of a fix-mapped
  address is quicker than normal pointers cause it's done at COMPILE time.

  i.e. Below tests occur at compile time:
  if (idx)
  if (idx >= 0xffffc000)
  return (0xffff0000UL - (idx << PAGE_SHIFT)


  --> incclude/asm-x86/fixmap_32.h:
  /*
   * Here we define all the compile-time 'special' virtual
   * addresses. The point is to have a constant address at
   * compile time, but to set the physical address only
   * in the boot process. We allocate these special addresses
   * from the end of virtual memory (0xfffff000) backwards.
   * (i.e. at the end of the 4GB virtual address area)
   *
   * Also this lets us do fail-safe vmalloc(), we
   * can guarantee that these special addresses and
   * vmalloc()-ed addresses never overlap.
   *
   * these 'compile-time allocated' memory buffers are
   * fixed-size 4k pages. (or larger if used with an increment
   * highger than 1) use fixmap_set(idx,phys) to associate
   * physical memory with fixmap indices.
   *
   * TLB entries of such buffers will not be flushed across
   * task switches.
   */
  enum fixed_addresses {
  	FIX_HOLE,
	FIX_VDSO,
	...

  /*
   * From below small calculation, there are KM_TYPE_NR (below) fix-mapped
   * linear addresses for each CPU in the system
   */
  #ifdef CONFIG_HIGHMEM
	FIX_KMAP_BEGIN,	/* reserved pte's for temporary kernel mappings */
	FIX_KMAP_END = FIX_KMAP_BEGIN + (KM_TYPE_NR * NR_CPUS) - 1,
  #endif

	...
  };
	
  --> include/asm-x86/fixmap_32.h:
  /*
   * 'index to address' translation. If anyone tries to use the idx
   * directly without tranlation, we catch the bug with a NULL-deference
   * kernel oops. Illegal ranges of incoming indices are caught too.
   */
  static __always_inline unsigned long fix_to_virt(const unsigned int idx)
  {
	/*
	 * this branch gets completely eliminated after inlining,
	 * except when someone tries to use fixaddr indices in an
	 * illegal way. (such as mixing up address types or using
	 * out-of-range indices).
	 *
	 * If it doesn't get removed, the linker will complain
	 * loudly with a reasonably clear error message..
	 *
	 * i.e. the __this*exist() method do not really exist, since
	 * this will be calculated at compile time, linker will search
	 * for this method if the below test was true.
	 */
	if (idx >= __end_of_fixed_addresses)
		__this_fixmap_does_not_exist();

        return __fix_to_virt(idx);
  }

  --> arch/x86/mm/pgtable_32.c:
  unsigned long __FIXADDR_TOP = 0xfffff000;
  EXPORT_SYMBOL(__FIXADDR_TOP);
  
  --> include/asm-x86/fixmap_32.h:
  #define FIXADDR_TOP	        ((unsigned long)__FIXADDR_TOP)

  #define __FIXADDR_SIZE	(__end_of_permanent_fixed_addresses << PAGE_SHIFT)
  #define __FIXADDR_BOOT_SIZE	(__end_of_fixed_addresses << PAGE_SHIFT)
  #define FIXADDR_START		(FIXADDR_TOP - __FIXADDR_SIZE)
  #define FIXADDR_BOOT_START	(FIXADDR_TOP - __FIXADDR_BOOT_SIZE)

  /* (x << PAGE_SHIFT) = x pages */
  #define __fix_to_virt(x)	(FIXADDR_TOP - ((x) << PAGE_SHIFT))
  #define __virt_to_fix(x)	((FIXADDR_TOP - ((x)&PAGE_MASK)) >> PAGE_SHIFT)

  /*
   * This functions has really no definition. The trick is to let
   * the compilation pass using the 'extern' keyword. If the compile-time
   * calculation at fix_to_virt() were true, the compiler will issue
   * a call to the function, where the linker will issue an error of
   * symbol-not-found
   */
  extern void __this_fixmap_does_not_exist(void);

* Now back to the kmap story:

  /* fixed-maps indices */
  enum fixed_addresses {
	...
  /*
   * From below small calculation, there are KM_TYPE_NR (below) fix-mapped
   * linear addresses for each CPU in the system
   */
  #ifdef CONFIG_HIGHMEM
	FIX_KMAP_BEGIN,	/* reserved pte's for temporary kernel mappings */
	FIX_KMAP_END = FIX_KMAP_BEGIN + (KM_TYPE_NR * NR_CPUS) - 1,
  #endif
	...
  };

  --> include/asm/kmap_types.h:
  /*
   * Each symbol in km_type, except the last one, is an _index_ of a 
   * fix-mapped linear address (as found above).
   *
   * NOTE: below indices are wrt the above FIX_KMAP_BEGIN to FIX_KMAP_END
   * 	   region indices.
   *
   * The kernel must ensure that the same window is never used by two 
   * kernel control paths at the same time. Thus, each symbol in the 
   * km_type structure is dedicated to one kernel COMPONENT and is 
   * named after the component
   */
  enum km_type {
	KM_BOUNCE_READ,
	KM_SKB_SUNRPC_DATA,
	KM_SKB_DATA_SOFTIRQ,
	KM_USER0,
	KM_USER1,
	KM_BIO_SRC_IRQ,
	KM_BIO_DST_IRQ,
	KM_PTE0,
	KM_PTE1,
	KM_IRQ0,
	KM_IRQ1,
	KM_SOFTIRQ0,
	KM_SOFTIRQ1,
	KM_TYPE_NR
  };

  --> arch/x86/mm/highmem_32.c:
  void *kmap_atomic(struct page *page, enum km_type type)
  {
	/* kmap_prot is the standtart kmap pte flags */
	return kmap_atomic_prot(page, type, kmap_prot);
  }

  /*
   * kmap_atomic/kunmap_atomic is significantly faster than kmap/kunmap because
   * no global lock is needed and because the kmap code must perform a global TLB
   * invalidation when the kmap pool wraps.
   *
   * However when holding an atomic kmap it is NOT __legal__ to sleep, so atomic
   * kmaps are appropriate for short, tight code paths only.
   */
  void *kmap_atomic_prot(struct page *page, enum km_type type, pgprot_t prot)
  {
	enum fixed_addresses idx;
	unsigned long vaddr;
	/* even !CONFIG_PREEMPT needs this, for in_atomic in do_page_fault */

	debug_kmap_atomic_prot(type);
	
	/*
	 * Disable pagefaults and kernel preemption
	 */
	pagefault_disable();

	if (!PageHighMem(page))
		return page_address(page);

	/*
	 * index in fixmapped_addresses list beginning from FIX_KMAP_BEGIN
	 */
	idx = type + KM_TYPE_NR * smp_processor_id();
	vaddr = __fix_to_virt(FIX_KMAP_BEGIN + idx);

	/*
	 * kmap_pte is initialized with fix_to_virt(FIX_KMAP_BEGIN)
	 * Bug if this window is already taken/mapped.
	 */
	BUG_ON(!pte_none(*(kmap_pte-idx)));

	set_pte(kmap_pte-idx, mk_pte(page, prot));
	arch_flush_lazy_mmu_mode();

	return (void *)vaddr;
  }

  --> arch/x86/mm/init_32.c:
  kmap_prot = PAGE_KERNEL

* The buddy algorithm:

* Check your OS book for the rigular discussion. Just remember that the physical
  address of a group is a multiple of its size. The mem_map is the main system's 
  repository of "struct page"s.

* There's a buddy system for each NUMA node zone. 

  --> include/linux/mmzone.h:

  /* Free memory management - zoned buddy allocator.  */
  #ifndef CONFIG_FORCE_MAX_ZONEORDER
  #define MAX_ORDER 11
  #else
  #define MAX_ORDER CONFIG_FORCE_MAX_ZONEORDER
  #endif
  #define MAX_ORDER_NR_PAGES (1 << (MAX_ORDER - 1))

  /* Darwish: In struct zone { */
  ...
	/* A free area structure for each buddy group 
	 * free_area[3] identifies all free blocks of size 2^3 */
  	struct free_area	free_area[MAX_ORDER];
  ...

  #define MIGRATE_UNMOVABLE     0
  #define MIGRATE_RECLAIMABLE   1
  #define MIGRATE_MOVABLE       2
  #define MIGRATE_RESERVE       3
  #define MIGRATE_ISOLATE       4 /* can't allocate from here */
  #define MIGRATE_TYPES         5

  struct free_area {
  	/* 
	 * This is the head of 5 linked lists of pages that represent
	 * the start of a free block in each Migrate type 
	 */
	struct list_head	free_list[MIGRATE_TYPES];

	/* Number of free 2^k blocks */
	unsigned long		nr_free;
  };

* Allocating a block from the buddy system (i.e. removing it from
  buddy blocks, thus the name rmqueue):

  --> mm/page_alloc.c:
  /*
   * Do the hard work of removing an element from the buddy allocator.
   * Call me with the zone->lock already held.
   */
  static struct page *__rmqueue(struct zone *zone, unsigned int order,
						int migratetype)
  {
	struct page *page;

	page = __rmqueue_smallest(zone, order, migratetype);

	if (unlikely(!page))
		/*
		 * This will be used if above block allocation with the
		 * needed migratetype failed which will search in other
		 * types. and if everything failed, it will search in
		 * the MIGRATE_RESERVE type instead of failing. It seems
		 * the system is in a bad state
		 */
		page = __rmqueue_fallback(zone, order, migratetype);

	return page;
  }

  /*
   * Go through the free lists for the given migratetype and remove
   * the smallest available page from the freelists
   */
  static struct page *__rmqueue_smallest(struct zone *zone, unsigned int order,
						int migratetype)
  {
	unsigned int current_order;
	struct free_area * area;
	struct page *page;

	/* Find a page of the appropriate size in the preferred list */
	for (current_order = order; current_order < MAX_ORDER; ++current_order) {
		area = &(zone->free_area[current_order]);
		if (list_empty(&area->free_list[migratetype]))
			continue;

		/* Remember, the 'lru' field is the list head that put a page
		 * as a head of a buddy block */
		page = list_entry(area->free_list[migratetype].next,
							struct page, lru);
		list_del(&page->lru);

		/* As expected, this clears the PG_Buddy flag and set the
		 * private flag to zero as it's meaningless now */
		rmv_page_order(page);

		area->nr_free--;
		__mod_zone_page_state(zone, NR_FREE_PAGES, - (1UL << order));

		/*
		 * This is very important. This handles the case where an exactly
		 * fit block was not found, and a bigger block is returned. This method
		 * subdivide the free space to new free_area lists.. 
		 * Ex: asking for 256-frame and returnning 1024-fram-block. remaining 750 
		 * frames will be divided to two groups a 256 one and another 512-frame one
		 */
		expand(zone, page, order, current_order, area, migratetype);

		return page;
	}

	return NULL;
  }

  /* explained above */
  static inline void expand(struct zone *zone, struct page *page,
	int low, int high, struct free_area *area,
	int migratetype)
  {
	unsigned long size = 1 << high;

	while (high > low) {
	        /* Decrease order by one */
		area--;
		high--;
		size >>= 1;

		VM_BUG_ON(bad_range(zone, &page[size]));
		/* Add it to the free_list */
		list_add(&page[size].lru, &area->free_list[migratetype]);
		area->nr_free++;

		/* set the PG_Buddy flag and save order in the 'private' field */
		set_page_order(&page[size], high);
	}
  }

* Freeing page blocks:

  --> mm/page_alloc.c:

  /* Remember that each zone has its own buddy system */
  static inline void __free_one_page(struct page *page,
		struct zone *zone, unsigned int order)
  {
	unsigned long page_idx;
	int order_size = 1 << order;
	int migratetype = get_pageblock_migratetype(page);

	if (unlikely(PageCompound(page)))
		destroy_compound_page(page, order);

	/* page index relative to biggest possible order block */
	page_idx = page_to_pfn(page) & ((1 << MAX_ORDER) - 1);

	VM_BUG_ON(page_idx & (order_size - 1));
	VM_BUG_ON(bad_range(zone, page));

	/* We have order_size free pages */
	__mod_zone_page_state(zone, NR_FREE_PAGES, order_size);

	while (order < MAX_ORDER-1) {
		unsigned long combined_idx;
		struct page *buddy;

		buddy = __page_find_buddy(page, page_idx, order);
		if (!page_is_buddy(page, buddy, order))
			break;		/* Move the buddy up one level. */

		/* Ok we've found a buddy, clear its order and remove
		 * it from the list of the same orders cause it will
		 * be merged with us (thus a new order and list) */
		list_del(&buddy->lru);
		zone->free_area[order].nr_free--;
		rmv_page_order(buddy);
		combined_idx = __find_combined_index(page_idx, order);
		page = page + (combined_idx - page_idx);
		page_idx = combined_idx;
		order++;
	}

	/* In case we've found a buddy or not, let the page
	 * be a head of a buddy block by setting its order and
	 * adding it to the appropriate list */
	set_page_order(page, order);
	list_add(&page->lru,
		&zone->free_area[order].free_list[migratetype]);
	zone->free_area[order].nr_free++;
  }

* Per-cpu page frame cache:

* To boost system performance, each memory zone defines a per-CPU page frame
  cache. Each per-CPU cache includes some pre-allocated page frames to be used
  for single memory requests issued by the local CPU. 

  Hot cache: page frames likely be included in hardware cache
  Cold cache: probably for DMA where no cache is modified

  --> include/linux/mmzone.h:
  struct per_cpu_pages {
	int count;		/* number of pages in the list */
	int high;		/* high watermark, emptying needed */
	int batch;		/* chunk size for buddy add/remove */
	struct list_head list;	/* the list of pages */
  };

  struct per_cpu_pageset {
	struct per_cpu_pages pcp;
  #ifdef CONFIG_NUMA
	s8 expire;
  #endif
  #ifdef CONFIG_SMP
	s8 stat_threshold;
	s8 vm_stat_diff[NR_VM_ZONE_STAT_ITEMS];
  #endif
  } ____cacheline_aligned_in_smp;

  --> mm/page_alloc.c:
  /*
   * Really, prep_compound_page() should be called from __rmqueue_bulk().  But
   * we cheat by calling it from here, in the order > 0 path.  Saves a branch
   * or two.
   */
  static struct page *buffered_rmqueue(struct zonelist *zonelist,
			struct zone *zone, int order, gfp_t gfp_flags)
  {
	unsigned long flags;
	struct page *page;
	int cold = !!(gfp_flags & __GFP_COLD);
	int cpu;
	int migratetype = allocflags_to_migratetype(gfp_flags);

  again:
	cpu  = get_cpu();

	/* Single page, allocate from the cache */
	if (likely(order == 0)) {
		struct per_cpu_pages *pcp;

		pcp = &zone_pcp(zone, cpu)->pcp;
		local_irq_save(flags);
		if (!pcp->count) {
			pcp->count = rmqueue_bulk(zone, 0,
					pcp->batch, &pcp->list, migratetype);
			if (unlikely(!pcp->count))
				goto failed;
		}

		/* Find a page of the appropriate migrate type
		 * It seems that cold stuff in the left and hot on the right */
		if (cold) {
			list_for_each_entry_reverse(page, &pcp->list, lru)
				if (page_private(page) == migratetype)
					break;
		} else {
			list_for_each_entry(page, &pcp->list, lru)
				if (page_private(page) == migratetype)
					break;
		}

		/* Allocate more to the pcp list if necessary */
		if (unlikely(&page->lru == &pcp->list)) {
			pcp->count += rmqueue_bulk(zone, 0,
					pcp->batch, &pcp->list, migratetype);
			page = list_entry(pcp->list.next, struct page, lru);
		}

		list_del(&page->lru);
		pcp->count--;
	} else {
		spin_lock_irqsave(&zone->lock, flags);
		/* Our normal buddy allocator */		
		page = __rmqueue(zone, order, migratetype);
		spin_unlock(&zone->lock);
		if (!page)
			goto failed;
	}

	__count_zone_vm_events(PGALLOC, zone, 1 << order);
	zone_statistics(zonelist, zone);
	local_irq_restore(flags);
	put_cpu();

	VM_BUG_ON(bad_range(zone, page));

	/* prepare the new page cause it's about to be returned
	 * from the allocator (see page_alloc.c) */
	if (prep_new_page(page, order, gfp_flags))
		goto again;
	return page;

  failed:
	local_irq_restore(flags);
	put_cpu();
	return NULL;
  }

* To free a single page:

  --> mm/page_alloc.c:
  /*
   * Free a 0-order page
   */
  static void free_hot_cold_page(struct page *page, int cold)
  {
	struct zone *zone = page_zone(page);
	struct per_cpu_pages *pcp;
	unsigned long flags;

	...

	arch_free_page(page, 0);
	kernel_map_pages(page, 1, 0);

	pcp = &zone_pcp(zone, get_cpu())->pcp;
	local_irq_save(flags);
	__count_vm_event(PGFREE);

	/* Page is free again, let it return to the per_cpu_pages
	 * list */
	if (cold)
		list_add_tail(&page->lru, &pcp->list);
	else
		list_add(&page->lru, &pcp->list);
	set_page_private(page, get_pageblock_migratetype(page));
	pcp->count++;

	/* More cached memory pages than needed */
	if (pcp->count >= pcp->high) {
		free_pages_bulk(zone, pcp->batch, &pcp->list, 0);
		pcp->count -= pcp->batch;
	}
	local_irq_restore(flags);
	put_cpu();
  }

* The zone allocator (__alloc_pages, the core, discussed above):
  
  --> mm/page_alloc.c:

  #define ALLOC_NO_WATERMARKS	0x01 /* don't check watermarks at all */
  #define ALLOC_WMARK_MIN	0x02 /* use pages_min watermark */
  #define ALLOC_WMARK_LOW	0x04 /* use pages_low watermark */
  #define ALLOC_WMARK_HIGH	0x08 /* use pages_high watermark */
  #define ALLOC_HARDER		0x10 /* try to alloc harder */
  #define ALLOC_HIGH		0x20 /* __GFP_HIGH set */
  #define ALLOC_CPUSET		0x40 /* check for correct cpuset */
  
  /*
   * get_page_from_freelist goes through the zonelist trying to allocate
   * a page.
   */
  static struct page *
  get_page_from_freelist(gfp_t gfp_mask, unsigned int order,
	        	struct zonelist *zonelist, int alloc_flags)
  {
	struct zone **z;
	struct page *page = NULL;
	int classzone_idx = zone_idx(zonelist->zones[0]);
	struct zone *zone;
	nodemask_t *allowednodes = NULL;/* zonelist_cache approximation */
	int zlc_active = 0;		/* set if using zonelist_cache */
	int did_zlc_setup = 0;		/* just call zlc_setup() one time */
	enum zone_type highest_zoneidx = -1; /* Gets set for policy zonelists */

  zonelist_scan:
	/*
	 * Scan zonelist, looking for a zone with enough free.
	 * See also cpuset_zone_allowed() comment in kernel/cpuset.c.
	 */
	z = zonelist->zones;

	do {
	        /* some NUMA_BUILD stuff */
	        ... 

		zone = *z;
		if ((alloc_flags & ALLOC_CPUSET) &&
			!cpuset_zone_allowed_softwall(zone, gfp_mask))
				goto try_next_zone;

		/* Check the watermarks if asked before allocating
		 * from thiz zone */
		if (!(alloc_flags & ALLOC_NO_WATERMARKS)) {
			unsigned long mark;

			/* in first try , __alloc_pages send 
			 * ALLOC_WMARK_LOW | ALLOC_CPUSET flags */
			if (alloc_flags & ALLOC_WMARK_MIN)
				mark = zone->pages_min;
			else if (alloc_flags & ALLOC_WMARK_LOW)
				mark = zone->pages_low;
			else
				mark = zone->pages_high;
			if (!zone_watermark_ok(zone, order, mark,
				    classzone_idx, alloc_flags)) {
				    
				/* mm, let's try to reclaim and see */
				if (!zone_reclaim_mode ||
				    !zone_reclaim(zone, gfp_mask, order))
					goto this_zone_full;
			}
		}
		
		/* The buddy allocation method */
		page = buffered_rmqueue(zonelist, zone, order, gfp_mask);
		if (page)
			break;

  this_zone_full:
		...
  try_next_zone:
		...
	} while (*(++z) != NULL);

	return page;
  }

  
  /*
   * Return 1 if free pages are above 'mark'. This takes into account the order
   * of the allocation.
   */
  int zone_watermark_ok(struct zone *z, int order, unsigned long mark,
		      int classzone_idx, int alloc_flags)
  {
	/* free_pages my go negative - that's OK */
	long min = mark;
	long free_pages = zone_page_state(z, NR_FREE_PAGES) - (1 << order) + 1;
	int o;

	/* I understand the ALLOC_HARDER case, but I can't understand the
	 * the ALLOC_HIGH one */
	if (alloc_flags & ALLOC_HIGH)
		min -= min / 2;
	if (alloc_flags & ALLOC_HARDER)
		min -= min / 4;

	/* First test */

	/* If free memory available in the zone is less than than the watermark
	 * + memory leaved for low-on-memory situations, fail */
	if (free_pages <= min + z->lowmem_reserve[classzone_idx])
		return 0;

	/* Second test */
	
	/* for orders from 0 to requested order 'order', assure that they have
	 * at least min/2^current_loop_order(i.e. o) */
	for (o = 0; o < order; o++) {
		/* At the next order, this order's pages become unavailable */
		free_pages -= z->free_area[o].nr_free << o;

		/* Require fewer higher order pages to be free */
		min >>= 1;

		if (free_pages <= min)
			return 0;
	}
	return 1;
  }

* without further ado, __alloc_pages:

  --> mm/page_alloc.c:
  /*
   * This is the 'heart' of the zoned buddy allocator.
   * @zonelist: Describe in order of pereference, the memory zones
   *	      suitable for memory allocation.
   */
  struct page *
  __alloc_pages(gfp_t gfp_mask, unsigned int order,
  		struct zonelist *zonelist)
  {
	const gfp_t wait = gfp_mask & __GFP_WAIT;
	struct zone **z;
	struct page *page;
	struct reclaim_state reclaim_state;
	struct task_struct *p = current;
	int do_retry;
	int alloc_flags;
	int did_some_progress;

	might_sleep_if(wait);

	if (should_fail_alloc_page(gfp_mask, order))
		return NULL;

  restart:
	z = zonelist->zones;  /* the list of zones suitable for gfp_mask */

	if (unlikely(*z == NULL)) {
		/*
		 * Happens if we have an empty zonelist as a result of
		 * GFP_THISNODE being used on a memoryless node
		 */
		return NULL;
	}

	page = get_page_from_freelist(gfp_mask|__GFP_HARDWALL, order,
				zonelist, ALLOC_WMARK_LOW|ALLOC_CPUSET);
	if (page)
		goto got_pg;

	/* --- If we're here then the system is low on memory --- */

	for (z = zonelist->zones; *z; z++)
		wakeup_kswapd(*z, order);

	/*
	 * OK, we're below the kswapd watermark and have kicked background
	 * reclaim. Now things get more complex, so set up alloc_flags according
	 * to how we want to proceed.
	 *
	 * The caller may dip into page reserves a bit more if the caller
	 * cannot run direct reclaim, or if the caller has realtime scheduling
	 * policy or is asking for __GFP_HIGH memory.  GFP_ATOMIC requests will
	 * set both ALLOC_HARDER (!wait) and ALLOC_HIGH (__GFP_HIGH).
	 * 
	 * Remember that this alloc flags is the one passed to the main
	 * get_pages_from_freelist discussed above.
	 */
	alloc_flags = ALLOC_WMARK_MIN;
	if ((unlikely(rt_task(p)) && !in_interrupt()) || !wait)
		alloc_flags |= ALLOC_HARDER;
	if (gfp_mask & __GFP_HIGH)
		alloc_flags |= ALLOC_HIGH;
	if (wait)
		alloc_flags |= ALLOC_CPUSET;

		/*
	 * Go through the zonelist again. Let __GFP_HIGH and allocations
	 * coming from realtime tasks go deeper into reserves.
	 *
	 * This is the last chance, in general, before the goto nopage.
	 * Ignore cpuset if GFP_ATOMIC (!wait) rather than fail alloc.
	 * See also cpuset_zone_allowed() comment in kernel/cpuset.c.
	 */
	page = get_page_from_freelist(gfp_mask, order, zonelist, alloc_flags);
	if (page)
		goto got_pg;

	/* --- If we're still here, then situation is bad --- */

rebalance:
	if (((p->flags & PF_MEMALLOC) || unlikely(test_thread_flag(TIF_MEMDIE)))
			&& !in_interrupt()) {
		if (!(gfp_mask & __GFP_NOMEMALLOC)) {
nofail_alloc:
			/* go through the zonelist yet again, ignoring mins */

			/* There's nothing more we can do, just get any
			 * page (INCLUDING the lowmem_reserve pages) */
			page = get_page_from_freelist(gfp_mask, order,
				zonelist, ALLOC_NO_WATERMARKS);
			if (page)
				goto got_pg;
			if (gfp_mask & __GFP_NOFAIL) {
				congestion_wait(WRITE, HZ/50);
				goto nofail_alloc;
			}
		}
		goto nopage;
	}

	/* Atomic allocations - we can't balance anything */
	if (!wait)
		goto nopage;

	/* We may block, check whether some other process need the CPU */
	cond_resched();

	/* Tell the world we're ready to do memory reclaiming */
	p->flags |= PF_MEMALLOC;

	/* Check below SLAB discussion */
	reclaim_state.reclaimed_slab = 0;
	p->reclaim_state = &reclaim_state;

	/* the main method used for Low on Memory reclaiming */
	did_some_progress = try_to_free_pages(zonelist->zones, order, gfp_mask);

	p->reclaim_state = NULL;
	p->flags &= ~PF_MEMALLOC;

	if (likely(did_some_progress)) {
		page = get_page_from_freelist(gfp_mask, order,
						zonelist, alloc_flags);
		if (page)
			goto got_pg;
	} else if ((gfp_mask & __GFP_FS) && !(gfp_mask & __GFP_NORETRY)) {

	        /*
		 * Darwish: We're in a deep trouble, we may invoke the Oom
		 * killer soon
		 */

		if (!try_set_zone_oom(zonelist)) {
			schedule_timeout_uninterruptible(1);
			goto restart;
		}

		/*
		 * Go through the zonelist yet one more time, keep
		 * very high watermark here, this is only to catch
		 * a parallel oom killing, we must fail if we're still
		 * under heavy pressure.
		 * 
		 * This succeeds if only another process has been killed
		 * by Oom. Kill ourselves too if memory is still tight.
		 */
		page = get_page_from_freelist(gfp_mask|__GFP_HARDWALL, order,
				zonelist, ALLOC_WMARK_HIGH|ALLOC_CPUSET);
		if (page) {
			clear_zonelist_oom(zonelist);
			goto got_pg;
		}

		/* The OOM killer will not help higher order allocs so fail */
		if (order > PAGE_ALLOC_COSTLY_ORDER) {
			clear_zonelist_oom(zonelist);
			goto nopage;
		}

		/* Sorry! */
		out_of_memory(zonelist, gfp_mask, order);
		clear_zonelist_oom(zonelist);
		goto restart;
	}

* The Slab Allocator:

* The cache descriptor:

  --> mm/slab.c:
  /*
   * struct kmem_cache
   *
   * manages a cache.
   */
  struct kmem_cache {
  /* 1) per-cpu data, touched during every alloc/free. per-cpu to avoid
      lock contention */
	struct array_cache *array[NR_CPUS];
  /* 2) Cache tunables. Protected by cache_chain_mutex */
	unsigned int batchcount;	/* Number of objects to be transfered to local caches */
	unsigned int limit;		/* Max number of free objects in the local cache */
	unsigned int shared;

	unsigned int buffer_size;
	u32 reciprocal_buffer_size;
  /* 3) touched by every alloc & free from the backend */

	unsigned int flags;		/* constant flags */
	unsigned int num;		/* # of objs per slab */

  /* 4) cache_grow/shrink */
	unsigned int gfporder;		/* order of pages per slab (2^n) */
	gfp_t gfpflags;	    	 	/* Flags sent to the buddy system when allocating pages */

	size_t colour;			/* cache colouring range (Number of colors for the slab) */
	unsigned int colour_off;	/* colour offset */
	struct kmem_cache *slabp_cache;
	unsigned int slab_size;		/* size of a signle slab */
	unsigned int dflags;		/* dynamic flags */

	/* constructor func */
	void (*ctor)(struct kmem_cache *, void *);

  /* 5) cache creation/removal */
	const char *name;		/* cache name, appears in /proc/slabinfo */
	struct list_head next;		/* list entry for the list of caches */

  #if DEBUG
	/*
	 * If debugging is enabled, then the allocator can add additional
	 * fields and/or padding to every object. buffer_size contains the total
	 * object size including these internal fields, the following two
	 * variables contain the offset to the user object and its size.
	 */
	int obj_offset;
	int obj_size;
  #endif
	/*
	 * We put nodelists[] at the end of kmem_cache, because we want to size
	 * this array to nr_node_ids slots instead of MAX_NUMNODES
	 * (see kmem_cache_init())
	 * We still use [MAX_NUMNODES] and not [1] or [0] because cache_cache
	 * is statically defined, so we reserve the max number of nodes.
	 */
	struct kmem_list3 *nodelists[MAX_NUMNODES];
	/*
	 * Do not add fields after nodelists[]
	 */
  };

  /*
   * The slab lists for all objects.
   */
  struct kmem_list3 {
  	/* partial list first, better asm code */	
	struct list_head slabs_partial;	/* slabs with both free and nonfree objects */
	struct list_head slabs_full;	/* slabs with only nonfree objects */
	struct list_head slabs_free;	/* slabs with only free objects */
	unsigned long free_objects;	/* number of free objects in cache */
	unsigned int free_limit;	
	unsigned int colour_next;	/* Per-node cache coloring */
	spinlock_t list_lock;
	/* 
	 * array_cache shared between all system CPUs, unlike kmem_cache->array[NR_CPUS]
	 * one. See discussion below for details
	 */
	struct array_cache *shared;	
	struct array_cache **alien;	/* on other nodes */
	unsigned long next_reap;	/* updated without locking */
	int free_touched;		/* updated without locking */
  };

* The slab descriptor:

  /*
   * struct slab
   *
   * Manages the objs in a slab. Placed either at the beginning of mem allocated
   * for a slab, or allocated from an general cache.
   * Slabs are chained into three list: fully used, partial, fully free slabs.
   */
  struct slab {
	struct list_head list;		/* partial, full or free ? */
	unsigned long colouroff;	/* offset of the first object in slab */
	void *s_mem;			/* including colour offset (Address of first slab object) */
	unsigned int inuse;		/* num of objs active in slab */

	/*
	 * kmem_bufctl_t is THE object descriptor. To my surprise, it's just
	 * an 'unsigned short int' representing the index of the next free
	 * object in the slab!
	 *
	 * Also check discussions below about this descriptor.
	 */
	kmem_bufctl_t free;		
	unsigned short nodeid;
  };

* Caches are specified to two types, general caches and specific caches.
  General caches are used by the slab allocator internally, while specific caches
  are used for the rest of the kernel.

* the named 'kmem_cache' kmem_cache is the core cache. Its objects are
  the cache descriptors for remaining caches used by kernel.

  --> mm/slab.c:
  /* internal cache of cache description objs */
  static struct kmem_cache cache_cache = {
	.batchcount = 1,
	.limit = BOOT_CPUCACHE_ENTRIES,
	.shared = 1,
	.buffer_size = sizeof(struct kmem_cache),
	.name = "kmem_cache",
  };

* Setting up the general (slab internal) caches:

  --> mm/slab.c:
  /*
   * Need this for bootstrapping a per node allocator.
   */
  #define NUM_INIT_LISTS (3 * MAX_NUMNODES)
  struct kmem_list3 __initdata initkmem_list3[NUM_INIT_LISTS];
  #define	CACHE_CACHE 0
  #define	SIZE_AC MAX_NUMNODES
  #define	SIZE_L3 (2 * MAX_NUMNODES)

  /*
   * Initialisation.  Called after the page allocator have been initialised and
   * before smp_init().
   */
  void __init kmem_cache_init(void)
  {
	/* Check the original file for remaining code, which is 
	 * commented very well */
	....

* A newly created cache does not contain slabs. New slabs are assigned to cache
  when a request for object exist and no more free object exists.

  --> mm/slab.c:
  /**
   * kmem_cache_alloc - Allocate an object
   * @cachep: The cache to allocate from.
   * @flags: See kmalloc().
   *
   * Allocate an object from this cache.  The flags are only relevant
   * if the cache has no available objects.
   */
  void *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags)
  {
	return __cache_alloc(cachep, flags, __builtin_return_address(0));
  }
  EXPORT_SYMBOL(kmem_cache_alloc);

  static __always_inline void *
  __cache_alloc(struct kmem_cache *cachep, gfp_t flags, void *caller)
  {
	unsigned long save_flags;
	void *objp;

	if (should_failslab(cachep, flags))
		return NULL;

	cache_alloc_debugcheck_before(cachep, flags);
	local_irq_save(save_flags);

	/* Main allocation method, which calls ____cache_alloc */
	objp = __do_cache_alloc(cachep, flags);

	local_irq_restore(save_flags);
	objp = cache_alloc_debugcheck_after(cachep, flags, objp, caller);
	prefetchw(objp);

	if (unlikely((flags & __GFP_ZERO) && objp))
		memset(objp, 0, obj_size(cachep));

	return objp;
  }

  ...

  #else /* !CONFIG_NUMA */

  /* 
   * This is more complicated if we're NUMA
   */
  static __always_inline void *
  __do_cache_alloc(struct kmem_cache *cachep, gfp_t flags)
  {
	return ____cache_alloc(cachep, flags);
  }

  #endif /* CONFIG_NUMA */

  /*
   * Grow (by 1) the number of slabs within a cache.  This is called by
   * kmem_cache_alloc() when there are no active objs left in a cache.
   */
  static int cache_grow(struct kmem_cache *cachep,
	        	gfp_t flags, int nodeid, void *objp)
  {
	struct slab *slabp;
	size_t offset;
	gfp_t local_flags;
	struct kmem_list3 *l3;

	/*
	 * Be lazy and only check for valid flags here,  keeping it out of the
	 * critical path in kmem_cache_alloc().
	 */
	BUG_ON(flags & GFP_SLAB_BUG_MASK);
	local_flags = flags & (GFP_CONSTRAINT_MASK|GFP_RECLAIM_MASK);

	/* Take the l3 list lock to change the colour_next on this node */
	check_irq_off();
	l3 = cachep->nodelists[nodeid];
	spin_lock(&l3->list_lock);

	/* Get colour for the slab, and cal the next value. */
	offset = l3->colour_next;
	l3->colour_next++;
	if (l3->colour_next >= cachep->colour)
		l3->colour_next = 0;
	spin_unlock(&l3->list_lock);

	offset *= cachep->colour_off;

	if (local_flags & __GFP_WAIT)
		local_irq_enable();

	/*
	 * The test for missing atomic flag is performed here, rather than
	 * the more obvious place, simply to reduce the critical path length
	 * in kmem_cache_alloc(). If a caller is seriously mis-behaving they
	 * will eventually be caught here (where it matters).
	 */
	kmem_flagcheck(cachep, flags);

	/*
	 * Get mem for the objs.  Attempt to allocate a physical page from
	 * 'nodeid'.
	 */
	if (!objp)
		objp = kmem_getpages(cachep, local_flags, nodeid);
	if (!objp)
		goto failed;

	/* Get slab management. */
	slabp = alloc_slabmgmt(cachep, objp, offset,
			local_flags & ~GFP_CONSTRAINT_MASK, nodeid);
	if (!slabp)
		goto opps1;

	slab_map_pages(cachep, slabp, objp);

	cache_init_objs(cachep, slabp);

	if (local_flags & __GFP_WAIT)
		local_irq_disable();
	check_irq_off();
	spin_lock(&l3->list_lock);

	/* Make slab active. */
	list_add_tail(&slabp->list, &(l3->slabs_free));
	STATS_INC_GROWN(cachep);
	l3->free_objects += cachep->num;
	spin_unlock(&l3->list_lock);
	return 1;
  opps1:
	kmem_freepages(cachep, objp);
  failed:
	if (local_flags & __GFP_WAIT)
		local_irq_disable();
	return 0;
  }

* The object descriptor:

  --> mm/slab.c:
  /*
   * kmem_bufctl_t:
   *
   * Bufctl's are used for linking objs within a slab
   * linked offsets.
   *
   * This implementation relies on "struct page" for locating the cache &
   * slab an object belongs to.
   * This allows the bufctl structure to be small (one int), but limits
   * the number of objects a slab (not a cache) can contain when off-slab
   * bufctls are used. The limit is the size of the largest general cache
   * that does not use off-slab slabs.
   * For 32bit archs with 4 kB pages, is this 56.
   * This is not serious, as it is only for large objects, when it is unwise
   * to have too many per slab.
   * Note: This limit can be raised by introducing a general cache whose size
   * is less than 512 (PAGE_SIZE<<3), but greater than 256.
   */

  typedef unsigned int kmem_bufctl_t;
  #define BUFCTL_END	(((kmem_bufctl_t)(~0U))-0)
  #define BUFCTL_FREE	(((kmem_bufctl_t)(~0U))-1)
  #define	BUFCTL_ACTIVE	(((kmem_bufctl_t)(~0U))-2)
  #define	SLAB_LIMIT	(((kmem_bufctl_t)(~0U))-3)

* Object alignment:

  --> include/linux/kernel.h:
  #define ALIGN(x, a)		__ALIGN_MASK(x,(typeof(x))(a)-1)
  #define __ALIGN_MASK(x, mask)	(((x)+(mask))&~(mask))

  --> mm/slab.c:
  struct kmem_cache *
  kmem_cache_create (const char *name, size_t size, size_t align,
		     unsigned long flags,
		     void (*ctor)(struct kmem_cache *, void *))
  {
     ....

	...
	/* 1) arch recommendation */
	if (flags & SLAB_HWCACHE_ALIGN) {
		/*
		 * In other words, If size is > (cache line size) / 2, store it
		 * in the beginning of the cache line. Otherwise round the
		 * object size to a submultiple of cache size, so an object
		 * can NOT span over two cache lines (i.e. pack different
		 * objects in the cache line WITHOUT letting an object span
		 * over multiple cache lines)
		 */
		ralign = cache_line_size();
		while (size <= ralign / 2)
			ralign /= 2;
	} else {
		ralign = BYTES_PER_WORD;
	}

	...

	size = ALIGN(size, align);
* 
