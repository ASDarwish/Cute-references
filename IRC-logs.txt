Useful discussions and help I got from the great people at #osdev

Special thanks to:
	- Brendan Trotter (bcos), the BCOS project, http://bcos.hopto.org/
	- Travis Geiselbrecht (geist), http://tkgeisel.com/

TOC:
----

(1)  On Testing the Page allocator
(2)  On APIC Timers and calibration
(3)  Geist great help on schedulers
(4)  On PIT idiosyncrasies

On Testing the Page allocator (December 2009):
----------------------------------------------

<geist>   i usually do something like create an array of pointers, say 512 of them
<geist>   then have a test app that randomly picks a slot, frees if something is
	  there and allocates a new random sized allocation
<geist>   that way you get a lot of sizes and their allocation/free order is random
<geist>   tests all of your free space list coherency
<geist>   you can also test for any corruption by maybe filling each allocation with
	  a random bit pattern, seeded by the slot number of the array
<geist>   and then double check before freeing that it hasn't been corrupted by
	  other allocations
<darwish> geist, this is going to be a good test indeed. I'll do it, it will make
	  me way more comfortable about the code
<geist>   i did this on my heap that i haul around for embedded projects
<geist>   been using it for a few years so as far as i know it's bug free
<darwish> which makes me feel even better :D
<geist>   btw, the heap is
	  http://git.newos.org/?p=lk.git;a=blob;f=lib/heap/heap.c;h=385d13a70d481614a625d015016bb6a8d8ee1767;hb=master
<geist>   super simple, not particularly fast
<geist>   but works
<darwish> just finished checking it
<darwish> will be of great help
<darwish> That's my first time to code such memory algos as you know, so my mind
	  was a bit uncomfortable on how to test those
<darwish> Things look much brighter now; thanks a lot geist ;)

On APIC Timers and calibration (April 2010):
--------------------------------------------

<darwish> Doh .. took me an hour to discover the hard way that APIC timer has
	  different frequency than the CPU's one ..
<webczat> darwish: you was calibrating using pit?
<darwish> Yeah, and it detects the CPU clock rightfully on different machines I
	  tested
<darwish> But seems CPU 'bus frequency' is different from CPU clock speed? I don't
	  know
<darwish> This is what the Linux log confirms: [   23.622558] Detected 10.390 MHz
	  APIC timer.
<darwish> while having a 1.8 GHz dual-CPU
<darwish> mm, after some googling, seems the 'bus frequency' is the 'external
	  clock frequency' ..
<darwish> which seems needs _another_ calibration beside the calibration already
	  done to detect CPU internal clock. Anyone confirming?
<darwish> I'll calibrate this external clock thing with the CPU TSC .. that will
	  be the easiest approach
<bcos>	  darwish: Use a stable time-base (e.g. PIT, RTC) to calibrate local APIC
	  timer
<bcos>    darwish: Then use the local APIC timer each time you need to recalibrate
	  the TSC
<webczat> bcos: why not to use pit for tsc, at first? maybe because tsc is per
	  cpu?
<bcos>    Maybe because TSC takes a lot of work to get right
<webczat> bcos: why it does?
<bcos>    (starting with figuring out what it counts, followed by when it changes
	  frequency)
<darwish> Yeah, I was worried about the stability thing. My 2 years old laptop has
	  a stable TSC, but I can't guarantee that on all x86-64 CPUs indeed
<bcos>    Hmm - if you only care about 64-bit CPUs, then it should be much easier
	  (there's a "TSC-invariable" flag returned by CPUID which should handle
	  half of them)
<geist>   yeah
<geist>   i think most of the big cpus in the last few years have had tht set
	  * bcos nods
<bcos>    If that bit isn't set though, you end up using CPU.vendor and CPU.family
<bcos>    webczat: For some CPUs (e.g. modern 64-bit) it measures time (which has
	  nothing to do with the current CPU speed). In other cases it measures
	  current CPU speed (which has nothing to do with time)
<geist>   yeah, it ended up failing for both things it could have been useful to
	  measure
<geist>   since it's not consistent
<geist>   it worked back in the day before cpus started changing speeds. i
	  remember speedstep start to show up in about 1999 or so
<geist>   and it was starting to screw up BeOS, which was using TSC as their time
	  base
<darwish> oh 
<bcos>    Intel introduced a new (architectural, and easy to use) performance
	  monitoring counter for measuring CPU speed
<geist>   then they went out of business and we never had to fix it :)
<geist>   right
<darwish> bcos: yeah, but only for core2duo+ if I remember :(
<bcos>    The other thing you need to watch out for is whether or not TSC stops
	  counting in varies sleep states..
<bcos>    "various"
<geist>   right, so ultimately what it means is you can only really use TSC for
	  counting small deltas of time/cycles
<geist>   for performance measuring purposes
<geist>   it's not useful for longer periods of time, or to generally keep the
	  system time bas
<bcos>    geist: I think the "TSC invariant" flag also implies the TSC keeps
	  ticking in most(?) sleep states
<darwish> bcos, I guess so, that's what I understood from the manual
<geist>   right
<bcos>    There are older CPUs that could have the "TSC invariant" flag set (if
	  the flag existed when the CPU was made)
<geist>   it's no longer measuring precisely cycles, but it became hard to do that
	  when the thing became superscalar and superpipelined anyway
<geist>   since it's sort of undefined exactly where instruction boundaries are
<bcos>    geist: A CPU cycles is still a CPU cycle (regardless of where
	  instruction boundaries are)
<geist>   darn interesting discussion but i gotta wander out for a few
<geist>   ta ta
<darwish> bcos, If I understand correctly, there also problems of TSCs being
	  stable, but unsynchronized between CPUs ..
<bcos>    darwish: Yes
<bcos>    darwish: I disable "RDTSC at CPL=3" so that doesn't worry me... :-)
<bcos>    darwish: For each thread I keep track of a virtual TSC, so RDTSC causes
	  a GPF and the GPF handler returns the thread's virtual TSC
<darwish> bcos, that's an interesting solution ..
<bcos>    darwish: For measuring time threads have to ask the kernel, and for
	  measuring "used CPU time" the TSC a thread gets doesn't include time
	  spent running other tasks
<bcos>    darwish: Also prevents some security exploits
<bcos>    (there's been research in the past about using the RDTSC on one CPU to
	  detect how much work another CPU is doing, and using that information to
	  help find encryption keys)
<bcos>    (mostly for hyper-threading)
<darwish> I heard about using the x86 CPU as an entropy for encryption ;)
<darwish> Though it's really weird how one can get keys from the cycles itself ..
<bcos>    darwish: Would take precise timing - e.g. detect if the other CPU is
	  doing "MUL" by timing "MUL" on the other logical CPU on the same core
<darwish> bcos, that's pretty hardcore
	  * bcos nods
<bcos>    darwish: Making RDTSC generate a GPF would be enough to make it useless
	  (not precise/fast enough) for that sort of thing though.. ;-)
<darwish> bcos, yeah, will mark this in my notes ..
<darwish> (RDTSC and userspace)
<bcos>	  It's funny, but Intel's programmers manual suggests that "RDTSC at
	  CPL=3" should be disabled for "a secure OS"
<bcos>	  (but no OS actually does it)
	  * bcos needs to sleep - 5:19 in the morning, and the clocks went back an
	  hour earlier (daylight savings)
<darwish> maybe they don't do it as a backward compatibility thing?
<webczat> bcos: but how to do detection like that?
<darwish> bcos, oh .. good night then :)
<bcos>	  darwish: Not sure - TSC can be useful
<bcos>    G'nights! :-)

Geist great help on schedulers (February 2010):
-----------------------------------------------

<darwish> After some thinking, I guess the best approach now to continue is:
	  writing a primitive initrd -> build the luser/kernel divide -> write
	  syscalls building blocks -> implement fork() and exit() -> think about
	  the scheduler
<darwish> went through some circles cause all above steps are inter-dependent,
	  but I hope that sequence is sane ..
<geist>   sounds fun
<geist>   though i dont know the context of your statement, but i almost always
	  recommend fleshing out your kernel before worrying about user space too
	  much
<geist>   you can write quite a complex system before user space comes along
<geist>   threading, smp, full vm, net stack, driver model, fs layer, etc
<geist>   all of those dont require user space to use
<darwish> mm, I wanted to build the scheduler first, thus the above steps. Maybe
	  I got concerned about the scheduler a bit early ..
<geist>   seems like the scheduler is one of the first steps
<geist>   how can you do all of that other stuff without any scheduling?
<darwish> geist, I thought I won't know what to schedule if there were no
	  binaries loaded (thus the initrd and syscalls steps), I guess my
	  assumptions are wrong?
<geist>   i did it almost completely first. 'kernel' as in something you can start
	  running code in ring 3, then simple memory management, then threads
	  (and hence scheduler), then smp, then more complicated paging vm
<geist>   darwish_: so what do you mean by 'schedule'?
<geist>   when ihear the word scheduler i usually take that to mean 'supports more
	  than one thread'
<geist>   and my question is how can you *not* support more than one thread almost
	  immediately
<darwish> geist, by schedule I meant interleaving two execution paths safely ..
<geist>   right
<geist>   so yeah, you want to do that almost immediately
<geist>   user space is nothing particularly special, it's just a sandboxed way to
          run code on your OS
<geist>   it doesn't fundamentally change the design of the kernel
<geist>   i think i hacked on my kernel for a year or so before even bothering with
	  user space
<geist>   and by then it was pretty full featured. i had just previopusly run test
          code as threads in kernel space
<darwish> geist, mm, so I can just start interleaving kernel code paths instead of
	  worrying about userspace ..
<geist>   darwish: sure. any modern preemptive kernel is not fundamentally different
	  from user space
<geist>   it's just ring 0 code that is multithreaded like user space
<darwish> mm ...
<geist>   think of the kernel as just a fancy library
<geist>   user space just makes calls into it via an abstract mechanism
<geist>   but on the other side it's still threaded and whatnot
<geist>   it's just a big fancy library loaded in every processe's address space
<darwish> so If I'm going to jump at the scheduler, what will the process
	  abstraction structure represent, a 'kernel thread'?
<geist>   if you design it right, there's no difference whatsoever
<geist>   there are just threads
<geist>   kernel threads are just threads that never leave kernel space
<darwish> geist, that's possibly a stupid question; you advised above that I cant
	  really have syscalls and user/kernel divide without the scheduler
	  first.
<geist>   well, it just doesn't make any sense
<darwish> geist, is this cause syscalls requires saving the user-process context
	  anyway before switching to the kernel?
<geist>   it's just dumb
<geist>   that's all,. you can do what you want, but there's no reason to delay
	  getting a scheduler working
<geist>   that's the meat of the kernel
<darwish> That's what I'll do now, I'm just making sure I understood your points
	  correctly ;)
<darwish> I guess I finished comprehending above advice. The goal now will be
	  firing several execution paths (threads) and interleaving them
	  SMP-safely, _without_ bogging myself with any other details ..
<geist>   yep. it's actually somewhat simpler than you think
<geist>   once you fully grokk it
<darwish> geist, as always, thanks a lot :-)
* darwish goes to schedulers land ..

On PIT idiosyncrasies (April 2010):
-----------------------------------

<darwish> The PIT is accurate enough for micro-second delays; isn't it?
<bcos>    Yes, but it depends on how it's used
<darwish> mm, can you elaborate a bit? I'm thinking of regular delays (like the
	  ones needed while firing secondary CPUs)
<darwish> long time bcos :) I've been busy last month at college; I hope things
	  are fine on your side
<bcos>    Always fine on this end :-)
<bcos>    If you poll the "current tick", then you'll miss ticks because of I/O
	  port delays, andend up with about 2 ms accuracy (at best) or worse
<darwish> Yes, I poll the OUT-2 pin to do the delay; didn't know port access is
	  very slow like that (in order of ms if I understood correctly)
<bcos>    For the legacy/ISA stuff, it typically takes about 1 us to access an IO
	  port
<darwish> "andend up with about 2 ms accuracy (at best) or worse" <-- I thought
	  this part is cause of the IO delay
<bcos>    Doh - sorry - meant "about 2 us accuracy"
<bcos>    For e.g. you poll an I/O port a tiny fraction before it's updated, then
	  1 us later (when you find out the delay has expired) you're 1 us late
<darwish> yeah; gonna add a comment on that. But thankfully the accuracy isn't too
	  bad for my purposes so far (usually > 50 us)
<bcos>    Best accuracy would be from using "one-shot" mode - in that case you're
	  always 1 us late (due to sending EOI) and you can just use a slightly
	  smaller count to adjust for it.. ;-)
<bcos>	  But, if you only need > 50 us accuracy then it should be fine..
<darwish> For such very sensitive cases, I'll use the APIC ;)
<darwish> I have the bootstrap one spare; but the rest are used for the scheduler
<darwish> 'sensitive cases' <-- where 1 us will make a difference
<darwish> Thanks a lot bcos!
<geist>   indeed
