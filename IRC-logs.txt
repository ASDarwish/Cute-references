Useful discussions and help I got from the great people at #osdev and
oftc.net #ck.

Special thanks to:
	- Brendan Trotter (bcos), the BCOS project, http://bcos.hopto.org/
	- Travis Geiselbrecht (geist), http://tkgeisel.com/

TOC:
----

(1)  On Testing the Page allocator
(2)  On APIC Timers and calibration
(3)  Geist great help on schedulers
(4)  On PIT idiosyncrasies
(5)  Virtual Memory in a long mode kernel
(6)  @@ [INTERESTING] On Spinlocks and kernel preemption @@
(7)  Con's advice on schedulers
(8)  On Implementing a File System
(9)  Geist on On the Unix File Descriptor Table, and passing pointers
(10) Advice on Userspace


On Testing the Page allocator (01/01/2010!)
----------------------------------------------

<geist>   i usually do something like create an array of pointers, say 512 of them
<geist>   then have a test app that randomly picks a slot, frees if something is
	  there and allocates a new random sized allocation
<geist>   that way you get a lot of sizes and their allocation/free order is random
<geist>   tests all of your free space list coherency
<geist>   you can also test for any corruption by maybe filling each allocation with
	  a random bit pattern, seeded by the slot number of the array
<geist>   and then double check before freeing that it hasn't been corrupted by
	  other allocations
<darwish> geist, this is going to be a good test indeed. I'll do it, it will make
	  me way more comfortable about the code
<geist>   i did this on my heap that i haul around for embedded projects
<geist>   been using it for a few years so as far as i know it's bug free
<darwish> which makes me feel even better :D
<geist>   btw, the heap is
	  http://git.newos.org/?p=lk.git;a=blob;f=lib/heap/heap.c;h=385d13a70d481614a625d015016bb6a8d8ee1767;hb=master
<geist>   super simple, not particularly fast
<geist>   but works
<darwish> just finished checking it
<darwish> will be of great help
<darwish> That's my first time to code such memory algos as you know, so my mind
	  was a bit uncomfortable on how to test those
<darwish> Things look much brighter now; thanks a lot geist ;)


On APIC Timers and calibration (April 2010):
--------------------------------------------

<darwish> Doh .. took me an hour to discover the hard way that APIC timer has
	  different frequency than the CPU's one ..
<webczat> darwish: you was calibrating using pit?
<darwish> Yeah, and it detects the CPU clock rightfully on different machines I
	  tested
<darwish> But seems CPU 'bus frequency' is different from CPU clock speed? I don't
	  know
<darwish> This is what the Linux log confirms: [   23.622558] Detected 10.390 MHz
	  APIC timer.
<darwish> while having a 1.8 GHz dual-CPU
<darwish> mm, after some googling, seems the 'bus frequency' is the 'external
	  clock frequency' ..
<darwish> which seems needs _another_ calibration beside the calibration already
	  done to detect CPU internal clock. Anyone confirming?
<darwish> I'll calibrate this external clock thing with the CPU TSC .. that will
	  be the easiest approach
<bcos>	  darwish: Use a stable time-base (e.g. PIT, RTC) to calibrate local APIC
	  timer
<bcos>    darwish: Then use the local APIC timer each time you need to recalibrate
	  the TSC
<webczat> bcos: why not to use pit for tsc, at first? maybe because tsc is per
	  cpu?
<bcos>    Maybe because TSC takes a lot of work to get right
<webczat> bcos: why it does?
<bcos>    (starting with figuring out what it counts, followed by when it changes
	  frequency)
<darwish> Yeah, I was worried about the stability thing. My 2 years old laptop has
	  a stable TSC, but I can't guarantee that on all x86-64 CPUs indeed
<bcos>    Hmm - if you only care about 64-bit CPUs, then it should be much easier
	  (there's a "TSC-invariable" flag returned by CPUID which should handle
	  half of them)
<geist>   yeah
<geist>   i think most of the big cpus in the last few years have had tht set
	  * bcos nods
<bcos>    If that bit isn't set though, you end up using CPU.vendor and CPU.family
<bcos>    webczat: For some CPUs (e.g. modern 64-bit) it measures time (which has
	  nothing to do with the current CPU speed). In other cases it measures
	  current CPU speed (which has nothing to do with time)
<geist>   yeah, it ended up failing for both things it could have been useful to
	  measure
<geist>   since it's not consistent
<geist>   it worked back in the day before cpus started changing speeds. i
	  remember speedstep start to show up in about 1999 or so
<geist>   and it was starting to screw up BeOS, which was using TSC as their time
	  base
<darwish> oh 
<bcos>    Intel introduced a new (architectural, and easy to use) performance
	  monitoring counter for measuring CPU speed
<geist>   then they went out of business and we never had to fix it :)
<geist>   right
<darwish> bcos: yeah, but only for core2duo+ if I remember :(
<bcos>    The other thing you need to watch out for is whether or not TSC stops
	  counting in varies sleep states..
<bcos>    "various"
<geist>   right, so ultimately what it means is you can only really use TSC for
	  counting small deltas of time/cycles
<geist>   for performance measuring purposes
<geist>   it's not useful for longer periods of time, or to generally keep the
	  system time bas
<bcos>    geist: I think the "TSC invariant" flag also implies the TSC keeps
	  ticking in most(?) sleep states
<darwish> bcos, I guess so, that's what I understood from the manual
<geist>   right
<bcos>    There are older CPUs that could have the "TSC invariant" flag set (if
	  the flag existed when the CPU was made)
<geist>   it's no longer measuring precisely cycles, but it became hard to do that
	  when the thing became superscalar and superpipelined anyway
<geist>   since it's sort of undefined exactly where instruction boundaries are
<bcos>    geist: A CPU cycles is still a CPU cycle (regardless of where
	  instruction boundaries are)
<geist>   darn interesting discussion but i gotta wander out for a few
<geist>   ta ta
<darwish> bcos, If I understand correctly, there also problems of TSCs being
	  stable, but unsynchronized between CPUs ..
<bcos>    darwish: Yes
<bcos>    darwish: I disable "RDTSC at CPL=3" so that doesn't worry me... :-)
<bcos>    darwish: For each thread I keep track of a virtual TSC, so RDTSC causes
	  a GPF and the GPF handler returns the thread's virtual TSC
<darwish> bcos, that's an interesting solution ..
<bcos>    darwish: For measuring time threads have to ask the kernel, and for
	  measuring "used CPU time" the TSC a thread gets doesn't include time
	  spent running other tasks
<bcos>    darwish: Also prevents some security exploits
<bcos>    (there's been research in the past about using the RDTSC on one CPU to
	  detect how much work another CPU is doing, and using that information to
	  help find encryption keys)
<bcos>    (mostly for hyper-threading)
<darwish> I heard about using the x86 CPU as an entropy for encryption ;)
<darwish> Though it's really weird how one can get keys from the cycles itself ..
<bcos>    darwish: Would take precise timing - e.g. detect if the other CPU is
	  doing "MUL" by timing "MUL" on the other logical CPU on the same core
<darwish> bcos, that's pretty hardcore
	  * bcos nods
<bcos>    darwish: Making RDTSC generate a GPF would be enough to make it useless
	  (not precise/fast enough) for that sort of thing though.. ;-)
<darwish> bcos, yeah, will mark this in my notes ..
<darwish> (RDTSC and userspace)
<bcos>	  It's funny, but Intel's programmers manual suggests that "RDTSC at
	  CPL=3" should be disabled for "a secure OS"
<bcos>	  (but no OS actually does it)
	  * bcos needs to sleep - 5:19 in the morning, and the clocks went back an
	  hour earlier (daylight savings)
<darwish> maybe they don't do it as a backward compatibility thing?
<webczat> bcos: but how to do detection like that?
<darwish> bcos, oh .. good night then :)
<bcos>	  darwish: Not sure - TSC can be useful
<bcos>    G'nights! :-)


Geist great help on schedulers (February 2010):
-----------------------------------------------

<darwish> After some thinking, I guess the best approach now to continue is:
	  writing a primitive initrd -> build the luser/kernel divide -> write
	  syscalls building blocks -> implement fork() and exit() -> think about
	  the scheduler
<darwish> went through some circles cause all above steps are inter-dependent,
	  but I hope that sequence is sane ..
<geist>   sounds fun
<geist>   though i dont know the context of your statement, but i almost always
	  recommend fleshing out your kernel before worrying about user space too
	  much
<geist>   you can write quite a complex system before user space comes along
<geist>   threading, smp, full vm, net stack, driver model, fs layer, etc
<geist>   all of those dont require user space to use
<darwish> mm, I wanted to build the scheduler first, thus the above steps. Maybe
	  I got concerned about the scheduler a bit early ..
<geist>   seems like the scheduler is one of the first steps
<geist>   how can you do all of that other stuff without any scheduling?
<darwish> geist, I thought I won't know what to schedule if there were no
	  binaries loaded (thus the initrd and syscalls steps), I guess my
	  assumptions are wrong?
<geist>   i did it almost completely first. 'kernel' as in something you can start
	  running code in ring 3, then simple memory management, then threads
	  (and hence scheduler), then smp, then more complicated paging vm
<geist>   darwish_: so what do you mean by 'schedule'?
<geist>   when ihear the word scheduler i usually take that to mean 'supports more
	  than one thread'
<geist>   and my question is how can you *not* support more than one thread almost
	  immediately
<darwish> geist, by schedule I meant interleaving two execution paths safely ..
<geist>   right
<geist>   so yeah, you want to do that almost immediately
<geist>   user space is nothing particularly special, it's just a sandboxed way to
          run code on your OS
<geist>   it doesn't fundamentally change the design of the kernel
<geist>   i think i hacked on my kernel for a year or so before even bothering with
	  user space
<geist>   and by then it was pretty full featured. i had just previopusly run test
          code as threads in kernel space
<darwish> geist, mm, so I can just start interleaving kernel code paths instead of
	  worrying about userspace ..
<geist>   darwish: sure. any modern preemptive kernel is not fundamentally different
	  from user space
<geist>   it's just ring 0 code that is multithreaded like user space
<darwish> mm ...
<geist>   think of the kernel as just a fancy library
<geist>   user space just makes calls into it via an abstract mechanism
<geist>   but on the other side it's still threaded and whatnot
<geist>   it's just a big fancy library loaded in every processe's address space
<darwish> so If I'm going to jump at the scheduler, what will the process
	  abstraction structure represent, a 'kernel thread'?
<geist>   if you design it right, there's no difference whatsoever
<geist>   there are just threads
<geist>   kernel threads are just threads that never leave kernel space
<darwish> geist, that's possibly a stupid question; you advised above that I cant
	  really have syscalls and user/kernel divide without the scheduler
	  first.
<geist>   well, it just doesn't make any sense
<darwish> geist, is this cause syscalls requires saving the user-process context
	  anyway before switching to the kernel?
<geist>   it's just dumb
<geist>   that's all,. you can do what you want, but there's no reason to delay
	  getting a scheduler working
<geist>   that's the meat of the kernel
<darwish> That's what I'll do now, I'm just making sure I understood your points
	  correctly ;)
<darwish> I guess I finished comprehending above advice. The goal now will be
	  firing several execution paths (threads) and interleaving them
	  SMP-safely, _without_ bogging myself with any other details ..
<geist>   yep. it's actually somewhat simpler than you think
<geist>   once you fully grokk it
<darwish> geist, as always, thanks a lot :-)
* darwish goes to schedulers land ..


On PIT idiosyncrasies (April 2010):
-----------------------------------

<darwish> The PIT is accurate enough for micro-second delays; isn't it?
<bcos>    Yes, but it depends on how it's used
<darwish> mm, can you elaborate a bit? I'm thinking of regular delays (like the
	  ones needed while firing secondary CPUs)
<darwish> long time bcos :) I've been busy last month at college; I hope things
	  are fine on your side
<bcos>    Always fine on this end :-)
<bcos>    If you poll the "current tick", then you'll miss ticks because of I/O
	  port delays, andend up with about 2 ms accuracy (at best) or worse
<darwish> Yes, I poll the OUT-2 pin to do the delay; didn't know port access is
	  very slow like that (in order of ms if I understood correctly)
<bcos>    For the legacy/ISA stuff, it typically takes about 1 us to access an IO
	  port
<darwish> "andend up with about 2 ms accuracy (at best) or worse" <-- I thought
	  this part is cause of the IO delay
<bcos>    Doh - sorry - meant "about 2 us accuracy"
<bcos>    For e.g. you poll an I/O port a tiny fraction before it's updated, then
	  1 us later (when you find out the delay has expired) you're 1 us late
<darwish> yeah; gonna add a comment on that. But thankfully the accuracy isn't too
	  bad for my purposes so far (usually > 50 us)
<bcos>    Best accuracy would be from using "one-shot" mode - in that case you're
	  always 1 us late (due to sending EOI) and you can just use a slightly
	  smaller count to adjust for it.. ;-)
<bcos>	  But, if you only need > 50 us accuracy then it should be fine..
<darwish> For such very sensitive cases, I'll use the APIC ;)
<darwish> I have the bootstrap one spare; but the rest are used for the scheduler
<darwish> 'sensitive cases' <-- where 1 us will make a difference
<darwish> Thanks a lot bcos!
<geist>   indeed


Virtual Memory in a long mode kernel (February 2010):
----------------------------------------------------

<darwish> OK, So I'm now building new page tables instead of the one setup at long
          mode boot. At this stage the kernel is already based on a virtual
          addresses base of 0xffffffff80000000
<darwish> I'm thinking of having a central kernel page table having the -2GB
          virtual space mapped to physical 0x0 upward. Any physical address > 2GB
          will need to be explicitly mapped (using a function like kmap()) ..
<darwish> Is this a valid strategy?
<darwish> I hope I won't face any surprises with this while doing the scheduler
<geist>   seems reasonable
<geist>   i think a general strategy is to map the kernel to 0xfffffff800000000
<geist>   and then 'below' that put a large physical mapping
<geist>   or just do what you ask and use < -2GB as your temporary mapping
<geist>   since you have essentially an infinite amount of address space I'd
          personally use -2GB -> 0  as your temporary mapping space
<geist>   and then size up physical address and map it in one large chunk below that
<geist>   say if you had 16GB of ram, map it to -20GB+ or something
<geist>   you can use 1GB pages if you have an amd and not have to burn a lot of
          memory on page tables
<darwish> mm ..
<darwish> geist, If I'm understanding correctly, you're talking about mapping the
          entire physical space and ditch this kmap() thing I proposed above
          completely
<geist>   pretty much
<geist>   it really depends on how you want to deal with the physical map in kernel
          space
<geist>   if you choose the linux strategy of having live data structures on 'top'
          of it then go with what you're thinking
<geist>   i'm not a huge fan of it, but i can see the appeal
<geist>   i'm peronally more of a fan of treating the kernel address space like
          any other, and have all the regions dynamically allocated
<geist>   but then also have this big mapping of chunks (or all of) physical space
          when you need it
* darwish is comprehending ..
<geist>   not too bad
<geist>   running some git maintenance on the server at work
<darwish> geist, Are 'live data structures' the ones allocated by the page
          allocator, kmalloc(), etcetera?
<geist>   kmalloc style styff, yeah
<geist>   and things like loadable modules
<geist>   linux runs its slab allocator on top of the physical mapping
<geist>   so that there's a 1:1 mapping of physical allocations to virtual space
<darwish> aah, that was my intention too (having the page allocator returning
          pages < physical 2GB for the kernel) ..
<darwish> Now parsing your second approach ..
<darwish> geist, Things began to be clear. So what are the advantages of the
          second non-Linux approach, address-space uniformity?
<geist>   for one you dont need any special zones in your physical allocator
<geist>   all of physical ram is perfectly available for everything
<geist>   except for maybe 32bit pci issues
<darwish> mm, this will let my page allocator and kmalloc stay simple as they are
          now ..
<geist>   i just think it's more uniform, but the linux strategy has some advantages
<darwish> beside the less page tables main advantage?
<geist>   if you can put up with the different zones for physical, and the need to
          ask for different types of physical ram, it already has everything
          mapped so the TLB pressure on kernel space is much lower
<geist>   and you dont have to modify the page tables much which means less TLB
          shootdowns on an smp system
<geist>   anyway, off to the store for a bit
<darwish> mm, I'll add above discussion to the wiki ..
<darwish> Thanks a lot geist!!


* [INTERESTING] On Spinlocks and kernel preemption (30/11/2010):
----------------------------------------------------------------

<darwish> If a thread is holding a spinlock, but finished using its quantum,
	  I should not preempt it till it releases such lock?
<darwish> talking about kernel spinlocks, no userspace yet
<bcos>	  In my opinion; if a thread is holding a spinlock, then any task
	  switches should be postponed
<bcos>	  (otherwise, beware of deadlocks)
<Candy>   imo, if a thread is holding a mutex, you should give it a tiny bit of
	  leeway to release it
<Candy>   and subtract that leeway from its next slice
<darwish> bcos, setting a time-limit on this wait should be better?
<darwish> bcos, say a 10 ticks limit
<bcos>    darwish: How long is the spinlock held???
<Candy>   no time limit
<Candy>   at least, not just a time limit
<darwish> bcos, I always hold it for very short times ofcourse; I was just afraid
	  of any bug here or there holding the lock more than usual
<darwish> Candy, the substraction idea is interesting
<Candy>   delay the switch until it releases all mutexes, or until a small amount
	  of time has passed
<bcos>    In that case, fix the bug (maybe consider a watchdog that uses NMI if
	  you're that worried)
<Candy>   ie, you wait until it releases the mutex to improve other threads'
	  work completed and you basically let it borrow from its next slice
<Candy>   for kernel space, let it keep the lock as long as it likes, and log
	  those that take too long
<darwish> yeah
<bcos>    I've considered doing some sort of contention counter - e.g. the first
	  time you attempt to acquire a spinlock and find it already locked,
	  increase a counter
<bcos>    so that I could find out which locks are causing contention
<Candy>   works for optimizing
<darwish> I guess I need something like that when lock usage increases in the
	  kernel.
<darwish> [19:36] bcos: (otherwise, beware of deadlocks)
<darwish> bcos, I guess deadlocks can occur anyway whether or not a thread got
	  preempted while holding a lock?
<darwish> (I wont preempt such threads, just making a sure I didn't lose an
	  important point in the middle)
<bcos>    ThreadA acquires lock and gets preempted by ThreadB. ThreadB kills
	  ThreadA. Now no-one can free the lock
<darwish> oh; kernel threads killing each other?
<darwish> bcos, you were talking about kernel threads for sure?
<bcos>    threads, that happen to be running in kernel space
<darwish> bcos, like a priviliged user-space program issuing a kill -9 signal?
<darwish> (using  a system call)
<bcos>    Multi-threaded process. One thread crashes all over the place. Do
	  you kill the entire process?
<bcos>    So, multi-threaded process. One of it's threads (running on a different
	  CPU) crashes, and the other CPU kills the entire process, while some of
	  the threads are holding kernel locks on other CPUs?
<darwish> yeah; I got the picture
<darwish> It's because I have no user-space support yet, that image of a
	  user-space process issuing a system call and getting killed while in
	  the system call path was not in my mind
<darwish> that makes a perfect sense; you should either log all the taken kernel
	  locks (impractical, inefficient), or just avoid preempting a thread
	  holding a lock. Thus, getting rid of the resource-tracking problem
<bcos>    When you acquire a lock, increment a "postpone task switches" counter.
	  When you do a task switch, check this counter, and if it's non-zero
	  you set a "task switch was postponed" flag. When you free the lock you
	  decrease the "postpone task switches" counter, and it it becomes zero,
	  check the "task switch was postponed flag", and it it's set, do the
	  task switch then
<bcos>    (which also means you leave interrupts enabled)
<darwish> yay .. the second part is interesting (switching after free_lock)
<bcos>    Need "per-CPU" counters/flags though
<darwish> I might also check the 'task switch was postponed' part at the end of
	  the ticks interrupt instead
<darwish> I'm already building per-CPU runqueues anyway ;)
<bcos>    "ticks interrupt"?
<darwish> the timer interrupt; I program it to 'tick' for each 10 ms
<bcos>    So, when a task switch is postponed, it may take an extra 10ms of
	  latency before it actually happens?
<darwish> I might change it period to tick each 1 ms instead .. but yes, it will
	  be unfair I guess
<darwish> s/it/its
<bcos>    darwish: If it takes 1 us to send an EOI to the PIC, and you do that
	  every 1 ms, how much CPU time is left to do anything else?
<darwish> bcos, yeah. I think a tick every 1 ms is too much. doing the check at
	  free_lock() sounds the best
<darwish> and with no latency whatsoever.
<darwish> bcos, can't believe how many times you've helped me throughout this
	  project .. THANKS


* Con's advice on schedulers:
_____________________________

<darwish> ah ofcourse. I have a small hobby kernel I've been developing for a
	  while .. I'm in the scheduling phase now
<conman>  mhm
<darwish> It's basically an MLFQ, with priorities boosting for starving threads
	  every second ..
<darwish> so I began testing it today with 200 100%-CPU-hogs ..
<darwish> the disparity between the most lucky and least lucky thread is full 1.5
	  seconds over a 2 minutes run
<darwish> (seconds of runtime)
<conman>  mhm
<darwish> After plotting the runtimes over the two minutes, it seems the priority
	  boosting messes up with fairness
<conman>  correct
<conman>  all heuristics have that effect
<darwish> I wonder if MLFQs can be made a little bit fairer?
<conman>  and what do you define as mlfq
<darwish> several queues representing priorities, when a thread uses its entire
	  slice, it's penalized by moving to a less priority, any higher-priority
	  thread available can preempt a running less-priority one
<conman>  problem with that design is always going to be batching of tasks at the
	  lowest priority
<conman>  it's intrinsically impossible to make it fair
<conman>  when comparing cpu bound to occasionally sleeping tasks
<darwish> indeed .. without the boosting, a good number of threads can starve up
	  there if the right number of high-priority tasks were available
<conman>  right
<conman>  you're describing what was wrong with staircase
<conman>  with enough frequently waking tasks, fully cpu bound tasks would starve
<darwish> yeah
<conman>  hence why staircase deadline evolved
<conman>  staircase that is deadline based
<conman>  and in essence that's what bfs does, but it's just terminology
<conman>  it's an earliest virtual deadline first round robin design
<conman>  so no real multi level queues, but same effect
<conman>  it's the simplest I could possibly make a design that remains fair and
	  low latency
<darwish> I don't know what are deadline-based schedulers yet .. but I'll
	  definitely check them soon: I'm not that happy about the currently
	  implemented scheduler
<conman>  it's only a virtual deadline
<conman>  just who has waited the longest
<conman>  that is all
<darwish> ah
<darwish> in the MLFQ currently implemented, the time-slices differ between
	  priorities
<darwish> i.e. the smallest priority ones have small time-slices, and the
	  time-slice goes up with priority
<darwish> you think that's good?
<conman>  no
<conman>  I use fixed time slices across all priorities
<conman>  and vary the deadline across priorities
<conman>  it means that the longest any two tasks of the *same* priority level
	  wait is always the same
<darwish> conman: the theory (which I've taken from a Solaris kernel book), was
	  that higher priorities are usually CPU-bound ones, so they need to keep
	  their hardware state as long as possible. That book argues that since
	  the low-priority tasks can interrupt the higher-priority ones, it's
	  not really a problem. Any experiences validating/invalidating this?
<bh>      wtf?
<conman>  by low they mean high
<conman>  and by high they mean low
<bh>      ok
<bh>      makes more sense
<conman>  it's neither here nor there in practice darwish
<conman>  because we don't have hard set out priorities for most things
<conman>  this goes fundamentally back to how we do or do not use nice
<conman>  if nice was intrinsically coded into every application with some
	  meaningful expectation of its behaviour
<conman>  then we'd code nice to work within those constraints
<conman>  but it's... not
<conman>  and the applications are not
<conman>  in fact that's precisely what I'm arguing with the implementation of
	  something like latnice
<darwish> In a sense, that scheduler assumes that it does its dynamic priority
	  modification _right_ :)
<conman>  (:
<darwish> I'll check handling starvation with virtual deadlines then .. seems
	  more theoretically consistent
<darwish> conman, (sorry, that was a huge number of questions)
<darwish> I was largely confused by the coded scheduler behavior .. but it seems
	  that's a flaw in the classical MLFQ design itself
<conman>  np
<conman>  gotta go


On Implementing a File System:
------------------------------

<darwish> hi
<darwish> anyone remember his first days of writing a file system?
<darwish> s/remember/remembering
<geist>	  yeah
<darwish> just a bit lost (as usual when starting a new sub-project in the
	  kernel)
<darwish> should I begin coding the low-level layer first? or code a
	  filesystem over a memory area first (ramdisk)?
<geist>	  i'd do the latter
<geist>	  so you can worry about one layer at a time
<geist>	  since a ramdisk is much easier to write and be pretty sure it works
	  properly
<darwish> mm...
<darwish> so I guess I'll modify the bootloader to load a filesystem image
	  (+ the kernel binary itself), write a small vfs interface, and then
	  fill it with the file-system code
<geist>	  sounds like a plan
<geist>	  similar to what i did
<darwish> and a simple file system as in BSD's legacy "fast file system"
	  won't be a bad start?
<geist>	  pretty decent
<geist>	  ffs/ext2/etc are a nice little family of fses
<geist>	  pretty easy to understand
<darwish> mm, the plan sounds interesting!


Geist on On the Unix File Descriptor Table, and passing pointers:
-----------------------------------------------------------------

<darwish> I wonder why Unix invented the idea of file descriptors in the range
	  of 0 -> N, and not just return the address of the file descriptor
	  (which holds current offset, etc) as the HANDLE?
<geist>	  because the address would be in kernel space
<darwish> mm, the user space will not dereference that address; it will just
	  use it in opaque form
<geist>	  sure, but then how to you verify it?
<geist>	  ie, you should *never* pass kernel pointers back and forth between
	  user and kernel space
<geist>	  that's a terrible security/stability issue
<darwish> mm, even if a signature was used, specifying that such a region is
	  indeed a FILE structure?
<geist>	  hmm?
<darwish> I mean as a form of verifying the opaque address
<geist>	  right, but now you have to start jumping through hoops to verify the
	  pointer
<geist>	  much easier to just have the fd be an index into a table
<geist>	  and anyway, fork() would mess it up
<darwish> geist, would it be too many hoops?
<geist>	  since you want the same fds on both sides of the fork to be different
<darwish> hmmm
<geist>	  anyway, handles to tables that point to things is almost always what
	  you want in a lot of situations
<geist>   direct pointers across interfaces/binaries/etc is almost never a good
	  idea
<darwish> mm, yes. It seems like userspace meddling into an internal kernel
	  representation; somethin it shouldn't deal with ..
<geist>	  you could do all sorts of fun security hacks with that. start a
	  write() with a large buffer with a fake FILE structure, and then in
	  another thread try to use it by guessing the kernel's address, etc
<darwish> ooh yes, quite horrible indeed
<darwish> seems the 0 -> N numbering provide kind of "name space" abstraction.
	  i.e. process A's file descriptor 8 is not equivalent to process B's
	  file descriptor 8
<darwish> aah, now things began to make sense
<geist>	  ight
<geist>	  each process has it's own fd table


Advice on userspace:
--------------------

<_eXeCuTeR>  geist, when do you suggest on moving to usermode? first
	     ELF/process ever executed?
<geist>	     _eXeCuTeR: well, my experience may be different, but i actually
	     waited quite a while
<geist>      i hacked on my kernel for about a year or so before getting into
	     user space. had a driver model, VMM, fs layer, etc
<geist>	     so that when i finally did user space it was just a matter of
	     plugging stuff together
<geist>	     it was actually quite easy. think it took a day or so to get
	     syscalls and hello world working
<_eXeCuTeR>  i've got a VMM and vfs layer (also got fstab to be more generic)
<azonenberg> right now my vfs has a single hard coded rootfs mount at /
<Candy>      I want the polar opposite of an fstab
<geist>      well, then aside from the loading point of view, running user
	     space (ie, seperated from kernel space and each other) processes
	     is not much more complicated than creating a syscall layer and
	     mechanism to existing kernel apis
<azonenberg> I have a syscall layer already
<geist>	     if you designed a lot of high level kernel apis to be pretty
	     agnostic to how they're called, it's pretty easy
<azonenberg> and my apps are using it
<geist>	     right
<azonenberg> The only difference is that they're still in ring 0
<azonenberg> and calling from kernel to kernel mode
<geist>	     right. that's basically what i did for a while. had a bunch of
	     kernel modules that would just create threads and run against
	     the kernel api
<geist>	     which i designed to be basically the same style api as user space
<azonenberg> XgF: Oh, so your list of filesystems to mount is in the registry?
<geist>	     note: this is not something that a lot of 'big' kernel seem to do.
	     ie, linux's kernel api is pretty much different from user space
<geist>	     can't easily open files in linux kernel, for example
<azonenberg> I have a pretty well defined interface for that
<Ready4Dis>  Why should it, lol
<azonenberg> "user" applications call for example read()
<Candy>      Ready4Dis: try starting "/sbin/init" without reading a file
<geist>      seems to be pretty logical to just have user space syscalls be
	     thin wrappers around existing kernel apis, so that the kernel
	     apis are basically the same
<azonenberg> which calls syscall(SYS_READ)
<azonenberg> the interrupt vector then calls sys_read()
<azonenberg> Drivers etc can call sys_read() directly
<geist>	     it's how beos did it, which was my model for kernels at the time
<azonenberg> with the assumption they're already in kernel mode
<geist>	     right, exactly
<Ready4Dis>  Candy, depend on the os, most micro kernels can't do many things
<azonenberg> for example my 24C eeprom driver
<azonenberg> does a sys_open("/dev/i2c1") in the open() handler
<geist>	     if you design it that way, the syscall layer is pretty easy. you
	     can easily have it be an autogenerated blob of code if you want
<Candy>	     Ready4Dis: still, no matter what design you have, you still need
	     some way of getting init in the air if it's at least goin gto work
<XgF>	     My kernel is 50% syscall implementation and 50% glue to make that
	     syscall implementation work. As befits a microkernel
<geist>	     and if the VMM has a nice consistent api, it's easy to punch those
	     through to user space and have the user space loader work with it
<XgF>	     Candy: For me, thats the bootloaders job :p
<geist>	     ie, 'give me a region of memory here' or'memory map this file here'
<azonenberg> and a read consists of a sys_write and sys_ioctl to set the address
<azonenberg> then a sys_read on the bus handle
<Ready4Dis>  yes, most of my init comes from initrd
<geist>	     that, alongside the usual vfs calls, is pretty much all you need
	     for a basic user space
<Ready4Dis>  Not saying I can't di file reads from my kernel, but its not its
	     job (for the most part)
<Ready4Dis>  Yes, and my second stage bootloader
