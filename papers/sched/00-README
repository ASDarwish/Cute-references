
				Schedulers
				----------


Study notes for a number of Scheduling-related papers; this serves as
a preparation for building an SMP scheduler for our 'Cute' kernel.

Ahmed S. Darwish <darwish.07@gmail.com>


* TOC:
  ----

  - Arpaci00, Operating Systems Notes, 'Scheduling: Introduction'
  - Corbato62, 'An Experimental Time-Sharing System', MIT
  - Corbato63, 'The Compatible Time-Sharing System - A Programmer's Guide'
  - Dingledine09, 'Performance Improvements on Tor'
  - Arpaci00, OS Notes, 'Scheduling: The Multi-Level Feedback Queue'
  - Arpaci98, 'Multilevel Feedback Queue Scheduling in Solaris'
  - Oracle02, Man Pages Section 4: File Formats, ts_dptbl(4)
  - Sliberchatz08, Bibliography from the 'Operating System Concepts' book


* Remzi Arpaci-Dusseau, Operating Systems Notes, Introduction, 2000:
  ------------------------------------------------------------------

  The included portion of Remzi's notes (ch. 6, 7, and 8) build a light-
  weight (undergrad-level) introduction to general-purpose OS scheduling.
  These documents also include a number of good references.

  Here are the important points in these notes + analysis of mine:

  - Some of the metrics in measuring schedulers include 'Turnaround time',
    which is the time taken between registering a job for execution and
    its completion/return/finish. Mainly:

    T_{turnaround} = T_{finish} - T_{arrival}

    Another metric, caring more about latency, is the 'response time'
    which is the time the job waits in the ready queue till some CPU time
    is provided:

    T_{response} = T_{firstrun} - T_{arrival}

  - The cost of context switching does not arise from the OS actions of
    saving and restoring a few registers. For example, we do context
    switching in our kernel using only 37 x86 opcodes: they are mainly
    register moves and stack pops and pushes.

    The real cost is that 'when programs run, they build up a great deal
    of state in CPU caches, TLBs, branch predictors, and other on-chip
    hardware. Switching to another job causes this state to be flushed
    and new state relevant to the currently-running job to be brought in,
    which may exact a noticeable performance cost.'

  - Round-robin, the simplest practical scheduler possible, involves the
    act of __cycling__ on the runqueue's list of processes, running each
    job for a fixed time slice (unit). Assuming processes A, B, C, and D,
    the scheduler run the jobs as follows

    | A | B | C | D | A | B | C | D | A | B | C | D | A | B | C | ...

    each for an equal amount of time. Advantages include implementation
    simplicity, good average response times (see disadvantages though),
    and being starvation-free by design.

    Disadvantages include very bad behaviour on system saturation, cause
    the system will swap-in and out programs for each (supposedly small)
    time slice [*], and response-time rising linearly with the number of
    active jobs in the system, even for highly-interactive processes.

    [*] this is a point handled by the Corbato paper below

  - In practice, most of processes will issue a number of blocking I/O
    requests during their allocated time quanta, which leads to moving
    them out of the runqueue altogether, and returning back to such queue
    after I/O completion.

    HOW are these jobs treated after returning back can be as simple as
    considering them new processes (classical round-robin), or to perform
    timing calculations and modify their priority (assuming 'priorities'
    support in the scheduler, or a similar concept).

  - Finally, there's a nice paragraph on the separation of mechanism and
    policy in OS Design. For example, stuff like programming the system
    ticks handler and context-switching code are scheduling mechanisms
    (mechanically how the jobs get interleaved). Scheduling policies
    (or disciplines) is what you're currently reading is about.

    In real-time POSIX for example, there are several programmable
    scheduling 'policies' that can be setup through sched_setscheduler(2).
    Such policies include SCHED_FIFO, SCHED_RR, and SCHED_OTHER.


* F.J. Corbato et al., 'An Experimental Time-Sharing System', 1962:
  -----------------------------------------------------------------

  This is one of the earliest papers on time-sharing schedulers. The core
  of the paper (beside its overall historical importance) is the section
  'A Multi-Level Scheduling Algorithm', which is in modern terminology now
  called the 'Multi-level Feedback Queue' (MLFQ). Important points:

  - The core problem the algorithm tries to solve is system saturation:
    a) total size of active user programs exceed high-speed memory
    b) too  many active user programs to maintain an adequate response at
    each user console.

    The paper assumes that a good design of the system to solve the
    saturation problem is to have a 'saturation procedure' which gives
    __graceful__ degradation of the response time and effective real-time
    computation speed of the large and long-running users.

  - The paper argues that a simple round-robin scheduler will not work in
    case of saturation cause it will cause an 'abrupt collapse of service'
    due to the sudden burden of time required to swap programs in-and-out
    of secondary memory such as a disk.

    The multi-level scheduling algorithm is presented, which is claimed to
    improve the __saturation performance__ of a time-sharing system.

  - We now delve into the scheduler algorithm details: from the very first,
    the scheduler partitions its runqueue to several priority queues
    l_0, l_1, ..., l_L. ('L' is to be calculated later)

  - In general, the bigger the program memory footprint (size), the less
    queue priority it's assigned to. But, the less the queue priority, the
    bigger the runtime for each of its tasks in number of quantum.

    Mathematically: l = [ log_2( [w_p/w_q] + 1 ) ]

    where each task at queue 'l' runs for 2^l quanta, and that if
    l_a < l_b, then l_a has a higher priority than l_b. We can deduce
    several things from above equation:

    a) if      0 < w_p <    w_q, [w_p/w_q] =  0, runtime =  1 quantum |-- 1
    b) if    w_q < w_p <  2*w_q, [w_p/w_q] =  1, runtime =  2 quanta  -|
    c) if  2*w_q < w_p <  3*w_q, [w_p/w_q] =  2, runtime =  2 quanta  -|-- 2
    d) if  3*w_q < w_p <  4*w_q, [w_p/w_q] =  3, runtime =  4 quanta  ..|
    e) if  4*w_q < w_p <  5*w_q, [w_p/w_q] =  4, runtime =  4 quanta  ..|
    f) if  5*w_q < w_p <  6*w_q: [w_p/w_q] =  5, runtime =  4 quanta  ..|-- 4
    g) if  6*w_q < w_p <  7*w_q: [w_p/w_q] =  6, runtime =  4 quanta  ..|
    h) if  7*w_q < w_p <  8*w_q: [w_p/w_q] =  7, runtime =  8 quanta  +++
    ...								      +++|-- 8
    p) if 15*w_q < w_p < 16*w_q: [w_p/w_q] = 15, runtime = 16 quanta  xxxx
    ...								      xxxx|-- 16
    _) if 31*w_q < w_p < 32*w_q: [w_p/w_q] = 31, runtime = 32 quanta  @@@@@
    ...								      @@@@@|-- 32
    _) if 63*w_q < w_p < 64*w_q: [w_p/w_q] = 63, runtime = 64 quanta

    Note: at point 'a)', queue 'l' is equal to 0: the __highest__ possible
    queue in priority.

    By above design, the bigger the program, the bigger its allocated
    number of quanta, making the time-overhead of swapping negligible.
    Thus, optimizing the turnaround time of CPU-bound processes.

  - The above point in scheduler design _tries_ to solve the first part of
    the problem the algorithm tackles: the stated point 'a) total size of
    active user programs exceed high-speed memory' above.

    From now on, the paper tackles second part of the problem, 'b) too many
    active user programs to maintain an adequate response'.

  - The scheduler __always__ begin with the tasks at the head of the
    highest priority queue (l_0). After that queue gets exhausted, it moves
    to the direct following queue l_1, and so on  <== (I)

    Tasks in a single queue get run in a round-robin fashion.

  - If while executing a task at queue 'l' (during the 2**l quantum), a
    task got assigned to a higher priority queue 'l*', the current
    executing task get put at the __head__ of the 'l'th queue, and the
    process at (I) is restarted from queue 'l*', and so on.

    In other words, a task cannot get executed while another task is
    waiting at a higher priority queue.

  - If a program in queue 'l' has not 'completed' (did not issue any I/O)
    within its 2**l-quantum runtime, it's placed at the __tail__ of the
    less-priority queue 'l+1'

  - A powerful point in the algorithm above (beside broad bounds on system
    performance) is that the classification procedure for tasks is entirely
    automatic: it depends on performance and program size rather than
    static declarations or (the possibly malicious) users hopes.

  - As once stated above, the main theme around this scheduler is improving
    the __saturation performance__ of the system.

    This performance improvement on saturation exists cause as a user taxes
    the system, the degradation of service occurs progressively starting
    with users with large or long-running programs (least-priority queues)
    first.

  - Starvation: there's a probable case of a big number of active high
    priority tasks starving less-priority CPU-bound tasks out of execution.

    The paper __models__ a worst cast of having N tasks, each running at
    queue 'l', and each relinquishing the CPU __directly__ before their
    2**l quantum runtime, and thus each also returning to the the same-
    priority queue, with the condition of returning to the runqueue only
    't_u'-seconds later (time for finishing I/O and, or, user response).

    If the system will always be busy at queue 'l' above, starving all less
    priority queues 'l + x' for x > 0, then the queue 'l' must never be
    empty; we must have:

	N * 2^l * q >= t_u
	2^l         >= (t_u / N*q)
	l           >= log_2 (t_u / N*q)

    where 'N' is the number of tasks at queue 'l'.

    From above, we've found an upper-bound on the priority queue that can
    be serviced: queues > 'l' above can possibly starve if number of tasks
    reach N. Thus, we have a guarantee that all

	l_a        <= [ log_2 (t_u / N * q) ]

    will be serviced and will __not__ get starved (given the estimated
    response time t_u), where '[]' denotes the integral part of enclosed
    equation. The author calls this queue the 'highest serviced level'
    queue.

  - The equation above has set a bound on the starvation problem. So if we
    set the max possible 'highest serviced queue' to the max possible queue
    on the system, we have a __theoretical guarantee__ of no starvation in
    the system.

    (Providing this guarantee is very hard for modern-day general purpose
    operating systems: we cannot easily put a limit on the number of tasks
    'N', and we cannot even have a valid estimation of 't_u', the I/O
    or user response finish time.

    These values can be calculated easily if the target hardware and work
    load is previously known; this is the case in the paper [IBM 7090],
    and this can also be the case for small and predictable devices like a
    small network router.)

  - By now, we've analyzed most of the paper sans the response time
    analysis.

    While these response time equations are intellectually simulating, they
    offer very broad bounds that are not entirely practical for a general
    purpose scheduler, similar to the paper's way of handling starvation in
    the above point.


* F.J. Corbato et al., 'The CTSS: A Programmer's Guide', 1963:
  -------------------------------------------------------------

  This manual gives a very nice overview of the historical time-sharing
  hands-on experience. It's a worthwhile read while studying the CTSS
  scheduler paper above.

  A useful demonstration of a programmer's session on CTSS is also
  presented in the Appendix. Interesting text editor!! it makes me
  remember the original UNIX editor 'ed'.


* Roger Dingledine, 'Performance Improvements on Tor', 2009:
  ----------------------------------------------------------

  Now why am I including a paper on Tor while we're dealing with
  schedulers? Because the Tor network tries to solve a problem very
  similar to the one Corbato was designing his scheduler model against:

  ``Section 1 described mechanisms to let low-volume streams have a
  chance at competing with high-volume streams. Without those mechanisms,
  normal web browsing users will always get squeezed out by people
  pulling down larger content and tolerating high latency.

  But the next problem is that some users simply add more load than the
  network can handle. Just making sure that all the load gets handled
  __fairly__ isn't enough if there’s too much load in the first place.(+)

  When we originally designed Tor, we aimed for high throughput. We
  figured that providing high throughput would mean we get good latency
  properties for free. However, now that it’s clear we have several user
  profiles trying to use the Tor network at once, we need to consider
  changing some of those design choices. Some of these changes would
  aim for better latency and worse throughput. (++) ''

  The statement marked with (+) matches word-by-word the problem
  statement of the Multi-Level Feedback Queue paper: to gracefuly handle
  'system saturation' while maintaining an acceptable level of system
  responsiveness. Thus, avoiding the 'abrupt collapse of service' caused
  by OS scheduler's round-robin algorithm or Tor's full fairness in high
  overlad.

  A main theme of the MLFQ solution, and Tor's one proposed at (++), is
  not to let throughput-driven jobs (compilers in schedulers case, an
  700-MB ISO download in the Tor case) deprive latency-senitive jobs
  out of quickly-needed resources. Examples of such latency-sensive
  apps include an editor or web-browser GUI scrolling in OS schedulers,
  and servicing HTML websites in Tor.


* Remzi Arpaci-Dusseau, Operating Systems Notes, The MLFQ, 2000:
  --------------------------------------------------------------

  Here, the MLFQ algorithm we studied from __primary sources__ above is
  re-discussed in modern terms. Important points:

  - The hardest point in designing schedulers is to minimize response time
    for interactive jobs, while also minimizing overall system turnround
    time.

  - As we know by now, MLFQ varies the priority of a job based on its
    observational behaviour: learning from jobs as they run, and using
    history to predict their future behaviour. Historical state is kept in
    the scheduler priority queues.

    This is an important systems design pattern of learning from history:
    observing current trends and learning from the past to predict the
    future. Such pattern is common in CPUs branch prediction, working set
    rules, and in networking protocols congestion control mechanisms.

    Remzi advises us though that 'such approaches work when jobs have
    __phases of behavior__ and are thus predictable; of course, one must
    be careful with such techniques, as they can easily be wrong and drive
    a system to make worse decisions than they would have with no knowledge
    at all.'

  - There are two important problems in MLFQs:

    a) Starvation: One of the possible ways to handle it is the well-known
    'aging principle': increasing the priority as the process waits. Check
    below parenthesized notes though; it's not that simple.

    (Note1: The original paper theoretically 'solved' this issue by giving
    bounds on starvation. These bounds can only work if we have a true
    estimate of I/O response times _and_ of the jobs number in the system.
    This is practically impossible for general-purpose scheduling.)

    (Note2: There are several ways to __practically__ apply aging methods.
    Some of these ways are discussed in David Black's paper 'Scheduling
    support for concurrency and parallelism in the Mach Operating System.')

    b) Gaming the system: A possible solution is to improve the algorithm
    accounting of CPU time, making it less viable to gaming. Check the
    notes for further details.


* Remzi Arpaci-Dusseau, 'Multilevel Feedback Queue Scheduling in Solaris',
  1998  &&  Solaris, Man Pages Section 4: File Formats, ts_dptbl(4), 2002:
  ------------------------------------------------------------------------

  From now on, we check how various real-world kernels implement MLFQ
  scheduling. Beginning with Solaris, here are the important points:

  - For regular jobs, the kernel has 60 priority queues. A job begins in
    the middle, at priority 29. As in classical MLFQ, CPU-bound jobs get
    assigned smaller priorities over time (with bigger slices), and I/O
    tasks move up, getting scheduled whenever they have jobs to do.

    Now here is Solaris 'dispatcher parameter table' defaults:

    Priority	q (ms)	tqexp	slpret	maxwait lwait
    0		200	0	50	0	50
    ...		vvv	v	vv	v	vv
    9		vvv	v	vv	v	vv
    10		160	v	51	v	51
    11		vvv     1	vv	v	vv
    12		vvv	2	vv	v	vv
    13		vvv	3	vv	v	vv
    14		vvv	4	vv	v	vv
    15		vvv	5	vv	v	vv
    16		vvv	6	vv	v	vv
    17		vvv	7	vv	v	vv
    18		vvv	8	vv	v	vv
    19		vvv	9	vv	v	vv
    20		120	10	52	v	52
    21		vvv	11	vv	v	vv
    22		vvv	12	vv	v	vv
    23		vvv	13	vv	v	vv
    24		vvv	14	vv	v	vv
    25		vvv	15	vv	v	vv
    26		vvv	16	vv	v	vv
    27		vvv	17	vv	v	vv
    28		vvv	18	vv	v	vv
    29		vvv	19	vv	v	vv
    30		80	20	53	v	53
    31		vv	21	vv	v	vv
    33		vv	22	vv	v	vv
    33		vv	23	vv	v	vv
    34		vv	24	vv	v	vv
    35		vv	25	54	v	54
    36		vv	26	vv	v	vv
    37		vv	27	vv	v	vv
    38		vv	28	vv	v	vv
    39		vv	29	vv	v	vv
    40		40	30	55	v	55
    41		vv	31	vv	v	vv
    42		vv	32	vv	v	vv
    43		vv	33	vv	v	vv
    44		vv	34	vv	v	vv
    45		vv	35	56	v	56
    46		vv	36	57	v	57
    47		vv	37	58	v	58
    48		vv	38	vv	v	vv
    49		vv	39	vv	v	59
    50		vv	40	vv	v	vv
    51		vv	41	vv	v	vv
    52		vv	42	vv	v	vv
    53		vv	43	vv	v	vv
    54		vv	44	vv	v	vv
    55		vv	45	vv	v	vv
    56		vv	46	vv	v	vv
    57		vv	47	vv	v	vv
    58		vv	48	vv	v	vv
    59		20	49	59	32000	vv

    tqexp: time quantum expired priority, the new priority the thread is set
    to when it uses its entire time quantum. As you can see, any process
    using its entire slice get its priority decreased by __10 levels__.

    slpret: sleep return priority value, the new priority the thread is set
    to after waking up (from a sleep event like network or disk I/O.)
    The thread priority is increased to 50 or above after an I/O operation!

    maxwait: a starvation avoidance mechanism to compensate threads waiting
    for a relatively long time in the dispatcher queue. 'maxwait' is such
    waiting time threshold. If a thread waited more than maxwait in the
    queues, its priority is boosted up to the respective 'lwait' priority.

    Each thread has a dispatcher_wait ('dispwait') counter, which is reset
    when the thread is inserted into the dispatch queue (following a time-
    quantum expiration, or cause of wakeup from I/O or locking). This
    field is incremented once per second for all processes waiting in the
    dispatcher queues. If such counter exceeded 'maxwait', then the thread
    priority is incremented to 'lwait' -- a priority > 50 by default.

    Note that the default 'maxwait' for Solaris is 0: just 1 second and
    all the system processes priority will be boosted up above 50. This
    avoids starving CPU-bound too much.


* Bibliography from 'Operating System Concepts', Sliberchatz et al., 2007:
  ------------------------------------------------------------------------

  Feedback queues were originally implemented on the CTSS system described
  in Corbato et al.[1]. This feedback queue scheduling system was analyzed
  by Scharge[2]. The preemptive priority scheduling algorithm was suggested
  by Kleinrock[3].

  [1] Fernando J. Corbato et al., 'An Experimental Time-Sharing System', 
      Proceedings of the AFIPS Fall Joint Computer Conference (1962)

  [2] L. E. Scharge, 'The Queue M/G/I with Feedback to Lower Priority
      Queues', Management Science, Volume 13 (1967)

  [3] L. Kleinrock, 'Queuing Systems, Volume II: Computer Applications,
      Wiley Interscience (1975)

  Anderson et al.[4], Lewis and Berg[5], and Philbin et al[6] discuss
  thread scheduling.

  [4] T. E. Anderson et al., 'The Performance Implications of Thread
      Management Alternatives for Shared-Memory Multiprocessors', IEEE
      Transactions on Computers, Volume 38, Number 12 (1989)

  [5] B. Lewis and D. Berg, 'Multithreaded Programming with Pthreads,
      Sun Microsystems Press (1998)

  [6] J. Philbin et al., 'Thread Scheduling for Cache Locality',
      Architectural Support for Programming Language and Operating Systems,
      (1996)

  Scheduling techniques that take into account information regarding
  process execution times from previous runs are described in Fisher[7],
  Hall et al.[8], and Lowney et al.[9]

  [7] J. A. Fisher, 'Trace Scheduling: A Technique for Global Microcode
      Dispatching', IEEE Transactions on Computers, Volume 30, Number 7
      (1981)

  [8] L. Hall et al., 'Scheduling to Minimize Average Completion Time:
      Off-line and On-line Algorithms', SODA: ACM-SIAM Symposium on
      Discrete Algorithms (1996)

  [9] P. G. Lowney et al., 'The Multiflow Trace Scheduling Compiler',
      Journal of Supercomputing, Volume 7, Number 1-2 (1993)

  Fair-share schedulers are covered by Henry[10], Woodside[11], and Kay
  Kay and Lauder[12]

  [10] G. Henry, 'The Fair Share Scheduler', AT&T Bell Laboratories
       Technical Journal (1984)

  [11] C. Woodside, 'Controllability of Computer Performance Tradeoffs
       Obtained Using Controlled-Share Queue Schedulers', IEEE
       Transactions on Software Engineering, Volume SE-12, Number 10
       (1986)

  [12] J. Kay and P. Lauder, 'Fair Share Scheduler', Communications of the
       ACM, Volume 31, Number 1 (1988)

  Siddha et al[13] discusses __MODERN__ challenges on multicore systems.

  [13] S. Siddha et al, 'Process Scheduling Challenges in the Era of
       Multi-Core Processors', Intel Technology Journal, Volume 11 (2007)

  Multiprocessor scheduling was discussed by Tucker and Gupta[14], Zahorjan
  and McCann[15], Fietelson and Rudolph[16], Leutenegger and Vernnon[17],
  Blumofe and Leisersonn[18], Polychronopoulos and Kuck[19], and Lucco[20].

  [14] A. Tucker and A. Gupta, 'Process Control and Scheduling Issues for
       Multiprogrammed Shared-Memory Multiprocessors', Proceedings of the
       ACM Symposium on Operating Systems Principles (1989)

  [15] J. Zahorjan and C. McCann, 'Processor Scheduling in Shared-Memory
       Multiprocessors', Proceedings of the Conference on Measurement and
       Modeling of Computer Systems (1990)

  [16] D. Fietelson and L. Rudolph, 'Mapping and Scheduling in a Shared
       Parallel Environment Using Distributed Hierarchial Control', 
       Proceedings of the International Conference on Parallel Processing,
       (1990)

  [17] S. Leutenegger and M. Vernnon, 'The Performance of Multiprogrammed
       Multiprocessor Scheduling Policies', Proceedings of the Conference
       on Measurement and Modeling of Computer Systems (1990)

  [18] R. Blumofe and C. Leisersonn, 'Scheduling Multi-threaded
       Computations by Word Stealing', Proceedings of the Annual Symposium
       on Foundations of Computer Science (1994)

  [19] C. D. Polychronopoulos and D. J. Kuck, 'Guided Self-Scheduling:
       'A Practical Scheduling Scheme for Parallel Supercomputers', IEEE
       Transactions on Computers, Volume 36, Number 12, (1987)

  [20] S. Lucco, 'A Dynamic Scheduling Method for Irregular Parallel
       Programs', Proceedings of the conference on Programming Languages
       Design and Implementation, (1992)

  The authors then list the standard books covering well-known general
  purpose operating systems kernels like SVR2, the BSDs, Linux, NT, and
  Solaris by Bach, Kirk McKusick, Bovet and Cesati, Solomon and
  Russionovich, and Mauro and McDougal respectively.
